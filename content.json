{"meta":{"title":"LP's Notes","subtitle":"侠客行舟 不进则退","description":"侠客行舟|技术博客|Java|分布式|架构|大数据","author":"LPNOTE","url":"http://www.lpnote.com"},"pages":[{"title":"","date":"2019-02-13T08:59:06.516Z","updated":"2019-02-13T08:59:06.516Z","comments":true,"path":"404.html","permalink":"http://www.lpnote.com/404.html","excerpt":"","text":"超人，你要找的页面不存在 function goback(){ window.location.href=\"http://www.lpnote.com/\"; } 返回"},{"title":"关于","date":"2016-01-31T14:10:28.000Z","updated":"2019-02-13T08:59:06.552Z","comments":true,"path":"about/index.html","permalink":"http://www.lpnote.com/about/index.html","excerpt":"","text":"个人概况2009年毕业于重庆大学软件工程 主要历职 重庆国虹科技 阿里巴巴 重庆易极付 重庆猪八戒网络 现任职于：重庆宜迅联供应链 关注领域Java/大数据/分布式/架构/中间件 联系我email: lpwork@foxmail.comgithub: http://github.com/icanfly"},{"title":"categories","date":"2017-01-12T01:52:54.000Z","updated":"2019-02-13T08:59:06.552Z","comments":true,"path":"categories/index.html","permalink":"http://www.lpnote.com/categories/index.html","excerpt":"","text":""},{"title":"留言","date":"2016-02-01T12:29:57.000Z","updated":"2019-02-13T08:59:06.552Z","comments":true,"path":"comment/index.html","permalink":"http://www.lpnote.com/comment/index.html","excerpt":"","text":""},{"title":"参与开源","date":"1969-12-31T16:00:02.013Z","updated":"2019-02-13T08:59:06.568Z","comments":true,"path":"participate/index.html","permalink":"http://www.lpnote.com/participate/index.html","excerpt":"","text":"参与开源的一些记录： Presto: Fix ‘restart presto failed in Ambari 2.5’ and Upgrade version of presto component Durid: 复杂sql解析不正确的问题 SpringBoot: document of spring.http.multipart.file-size-threshold is confused or mistake JavaNSQClient: add heartbeat interval exceeded check"},{"title":"参考手册","date":"1969-12-31T16:00:02.013Z","updated":"2019-02-13T08:59:06.568Z","comments":true,"path":"reference/index.html","permalink":"http://www.lpnote.com/reference/index.html","excerpt":"","text":"Git参考手册 Dubbo参考手册 CORS协议解读"},{"title":"Tags","date":"2016-08-11T04:12:45.000Z","updated":"2019-02-13T08:59:06.568Z","comments":false,"path":"tags/index.html","permalink":"http://www.lpnote.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Elasticsearch中ignore_above的作用","slug":"ignore_above-in-elasticsearch","date":"2019-02-13T08:57:50.000Z","updated":"2019-02-13T08:59:06.532Z","comments":true,"path":"2019/02/13/ignore_above-in-elasticsearch/","link":"","permalink":"http://www.lpnote.com/2019/02/13/ignore_above-in-elasticsearch/","excerpt":"","text":"ignore_above一般配合keyword类型使用，指示该字段的最大索引长度（即超过该长度的内容将不会被索引），对于超过ignore_above长度的字符串，analyzer不会进行索引分析，所以超过该长度的内容将不会被搜索到。这个选项主要对not_analyzed字段有用，这些字段通常用来进行过滤、聚合和排序。而且这些字段都是整体存在的，不需要进行索引分析处理，所以一般不会允许在这些字段中索引过长的项。 当在设置索引的mapping设置后，如果keyword字段没有显式设置ignore_above的值，则ES会默认设置该长度为256，当然你可以在后续的操作中修改这个值，但是修改后需要重建索引才能让以前不满足的值重新变得满足而被索引。 不满足该设置的文档会被保存，但是该字段值不会被索引 通过查询该字段的值时该文档不会被索引到，并被输出 通过其它字段的查询时，如果该文档满足条件会被索引到，并被输出 该设置选项并不影响文档的保存，只影响文档的字段是否被索引和搜索 注：keyword类型的字段的最大长度限制为32766个UTF-8字符，text类型的字段对字符长度没有限制 所以在设置keyword类型的ignore_above值时应该先遵守keyword本身的值最大长度限制。 ignore_above 值表示字符个数，但是 Lucene 计算的是字节数。如果你使用包含很多非 ASCII 字符的 UTF-8 文本，你应该将这个限制设置成 32766 / 3 = 10922 因为 UTF-8 字符可能最多占用 3 个字节。","categories":[{"name":"原创文章","slug":"原创文章","permalink":"http://www.lpnote.com/categories/原创文章/"}],"tags":[{"name":"搜索引擎","slug":"搜索引擎","permalink":"http://www.lpnote.com/tags/搜索引擎/"},{"name":"lucene","slug":"lucene","permalink":"http://www.lpnote.com/tags/lucene/"},{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://www.lpnote.com/tags/elasticsearch/"}]},{"title":"java并发之Striped64解析","slug":"java-concurrent-striped64","date":"2019-02-02T08:42:44.000Z","updated":"2019-02-13T08:59:06.532Z","comments":true,"path":"2019/02/02/java-concurrent-striped64/","link":"","permalink":"http://www.lpnote.com/2019/02/02/java-concurrent-striped64/","excerpt":"","text":"注：本文基于JDK1.8进行解析，其它JDK版本可能有所不同。 早在JDK1.5的时候就已经引入了大神Doug Lea的并发包体系，其中包括各种显式锁及实现，原子类，原子引用等，极大的丰富了JDK的并发生态。让我们实现数据同步从“原始社会”的synchroinzed阶段一下子过度到了基于CAS的“现代社会”，JDK1.5的AQS堪称当代并发的一个神器级的工具，然而追求永远是无穷尽的，当人们在享受到原子类带来的性能提升的时候，大神Doug Lea又一次为原子操作的Long和Double带来新的成员：Striped64及它的子类。它的原理相对来说比较简单，也是JDK常用的方式，就是通过CAS以及“分段技术”努力地减少争用，尽最大可能提高并发度。 Striped64该类维护了一个惰性初始化的列表和一个基础(base)的数值，列表的大小是2的次方，索引这个列表是通过基于每个线程的内部Probe算出一个Hashcode来确定。这个类的几乎所有的方法都是protected的，所以只有它的子类可以使用。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465abstract class Striped64 extends Number &#123; @sun.misc.Contended static final class Cell &#123; volatile long value; Cell(long x) &#123; value = x; &#125; final boolean cas(long cmp, long val) &#123; return UNSAFE.compareAndSwapLong(this, valueOffset, cmp, val); &#125; // Unsafe mechanics private static final sun.misc.Unsafe UNSAFE; private static final long valueOffset; static &#123; try &#123; UNSAFE = sun.misc.Unsafe.getUnsafe(); Class&lt;?&gt; ak = Cell.class; valueOffset = UNSAFE.objectFieldOffset (ak.getDeclaredField(\"value\")); &#125; catch (Exception e) &#123; throw new Error(e); &#125; &#125; &#125; /** Number of CPUS, to place bound on table size */ static final int NCPU = Runtime.getRuntime().availableProcessors(); /** * Table of cells. When non-null, size is a power of 2. */ transient volatile Cell[] cells; /** * Base value, used mainly when there is no contention, but also as * a fallback during table initialization races. Updated via CAS. */ transient volatile long base; /** * Spinlock (locked via CAS) used when resizing and/or creating Cells. */ transient volatile int cellsBusy; .... 相关方法省略 // Unsafe mechanics private static final sun.misc.Unsafe UNSAFE; private static final long BASE; private static final long CELLSBUSY; private static final long PROBE; static &#123; try &#123; UNSAFE = sun.misc.Unsafe.getUnsafe(); Class&lt;?&gt; sk = Striped64.class; BASE = UNSAFE.objectFieldOffset (sk.getDeclaredField(\"base\")); CELLSBUSY = UNSAFE.objectFieldOffset (sk.getDeclaredField(\"cellsBusy\")); Class&lt;?&gt; tk = Thread.class; PROBE = UNSAFE.objectFieldOffset (tk.getDeclaredField(\"threadLocalRandomProbe\")); &#125; catch (Exception e) &#123; throw new Error(e); &#125; &#125;&#125; 该类一个包本地类，只能在包范围内引用，包含支持64位值动态分段的类的通用表示和机制。该类同时继承至Number，因此具体的子类必须实现其接口方法。 在Striped64内部，持有数据的是一个由叫做Cell的数据结构的一个列表实现，这个Cell数据结构通过使用@sun.misc.Contented这个注解来减少缓存行冲突，关于缓存冲突，缓存行，伪共享的描述可以参看相关资料。通常情况下，缓存行填充(Padding)对于大多数原子操作来说都是不必要的，因为它们散落在不规则的内存中。但是对于存在于一个数组内的原子对象来说，这样的情况会发生变化，它们会产生相互影响，原因是因为它们在内存中的布局会相互紧挨着，并存在大量的共享相同的缓存行，而共享缓存行对于性能的影响将是非常巨大的。 相对来说Cell这个结构还是比较大的，所以我们尽量避免提前创建它们，除非在真正用到它们的时候。当没有竞争时，所有的更新操作都会应用到base字段上。当第一次产生争用时（在base字段上发生CAS失败），这个列表会被初始化，初始化大小为2。当后续仍然产生争用时，这个列表会被进一步扩展（除非到达了它的终极大小限制：列表大小的扩展到和CPU数量相当），列表中的Slot是空的，只有在使用它的时候才进行初始化。 一个自旋锁cellsBusy被用于列表的初始化和扩容，以及Slot的填充。在这里没有必要使用阻塞，当锁不可用时，线程会尝试获取其它Slot的锁（或者尝试base字段）。在这些重试期间，争用是增加了但局部性争用是降低了，这仍然比替代方案更好。 通过ThreadLocalRandom维护的Thread probe字段用作每线程哈希码。在未产生争用时，我们让它保持未初始化的值为0。当初始化时尽量保证这个值不与其它线程的值相冲突。执行更新操作时，失败的CAS会指示争用或列表冲突。当发生冲突时，如果此时列表的大小还没有达到极限大小限制，列表会进行扩容除非有其它的线程持有这把锁。如果被hashcode指定索引到的slot为空，并且锁是可用的，那么这个slot会被初始化为一个新的Cell。其它情况下，如果slot中存在Cell，那么就执行一次CAS操作来更新Cell中的值。重试通过“双重散列”进行，使用辅助散列（Marsaglia XorShift）尝试查找空闲插槽。 列表大小是有限的，因为当线程多于CPU时，假设每个线程都绑定到CPU，就会存在一个完美的哈希函数，将线程映射到槽以消除冲突。 当我们达到容量时，我们通过随机改变冲突线程的哈希码来搜索此映射。 因为搜索是随机的，并且冲突仅通过CAS失败而变得已知，所以收敛可能很慢，并且因为线程通常不会永远地绑定到CPUS，所以可能根本不会发生。 然而，尽管存在这些限制，但在这些情况下观察到的争用率通常较低。 当曾经散列到它的线程终止时，以及在列表扩容导致没有线程在扩展掩码下散列到它的情况下，Cell可能会被释放。我们不会尝试检测或删除此类Cell，假设对于长期运行的实例，观察到的争用情况可能会再次出现，因此最终将再次需要Cell; 对于短命的实例来说，没关系，GC帮我们清理这整个实例。 在整个实现过程中大量使用CAS无锁操作，并运用Padding技术（缓存行填充）将一个原子化的Long操作性能发挥到极致，在普通无争用或者争用较少的情况下，可以用base以及少量的Cell就可以动态减少争用，并在争用激烈时通过扩容Cell列表的方式来分散争用。这种模式有点类似分段锁的方式，不同的是这种实现更高效，全程无锁无阻塞。 Striped64类使用一个base和一个分散的Cell列表来实现对于Long型数值的操作，其核心的方法为longAccumulate和doubleAccumulate，其中这两个方法思路和模式均相同，只是一个针对于long类型，一个针对double类型。 关于对longAccumulate方法的解析如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980final void longAccumulate(long x, LongBinaryOperator fn, boolean wasUncontended) &#123; int h; if ((h = getProbe()) == 0) &#123; ThreadLocalRandom.current(); // force initialization h = getProbe(); wasUncontended = true; &#125; boolean collide = false; // True if last slot nonempty for (;;) &#123; Cell[] as; Cell a; int n; long v; if ((as = cells) != null &amp;&amp; (n = as.length) &gt; 0) &#123; if ((a = as[(n - 1) &amp; h]) == null) &#123; if (cellsBusy == 0) &#123; Cell r = new Cell(x); // Optimistically create if (cellsBusy == 0 &amp;&amp; casCellsBusy()) &#123; boolean created = false; try &#123; // Recheck under lock Cell[] rs; int m, j; if ((rs = cells) != null &amp;&amp; (m = rs.length) &gt; 0 &amp;&amp; rs[j = (m - 1) &amp; h] == null) &#123; rs[j] = r; created = true; &#125; &#125; finally &#123; cellsBusy = 0; &#125; if (created) break; continue; // Slot is now non-empty &#125; &#125; collide = false; &#125; else if (!wasUncontended) // CAS already known to fail wasUncontended = true; // Continue after rehash else if (a.cas(v = a.value, ((fn == null) ? v + x : fn.applyAsLong(v, x)))) break; else if (n &gt;= NCPU || cells != as) collide = false; // At max size or stale else if (!collide) collide = true; else if (cellsBusy == 0 &amp;&amp; casCellsBusy()) &#123; try &#123; if (cells == as) &#123; // Expand table unless stale Cell[] rs = new Cell[n &lt;&lt; 1]; for (int i = 0; i &lt; n; ++i) rs[i] = as[i]; cells = rs; &#125; &#125; finally &#123; cellsBusy = 0; &#125; collide = false; continue; // Retry with expanded table &#125; h = advanceProbe(h); &#125; else if (cellsBusy == 0 &amp;&amp; cells == as &amp;&amp; casCellsBusy()) &#123; boolean init = false; try &#123; // Initialize table if (cells == as) &#123; Cell[] rs = new Cell[2]; rs[h &amp; 1] = new Cell(x); cells = rs; init = true; &#125; &#125; finally &#123; cellsBusy = 0; &#125; if (init) break; &#125; else if (casBase(v = base, ((fn == null) ? v + x : fn.applyAsLong(v, x)))) break; // Fall back on using base &#125; &#125; 该类还有几个子类，通常我们在使用的时候一般会使用到的就是它的子类，包括：LongAdder，LongAccumulator，DoubleAdder，DoubleAccumulator，其中LongAdder和LongAccumulator只存在细微差异，Adder故名思意是求和的意思，LongAdder是指多次调用累加求和。而LongAccumulator是累积计算的意思，累积计算就不一定是求和了，也有可能是其它操作，这里它提供了一个二元操作接口了：123456789101112@FunctionalInterfacepublic interface LongBinaryOperator &#123; /** * Applies this operator to the given operands. * * @param left the first operand * @param right the second operand * @return the operator result */ long applyAsLong(long left, long right);&#125; 用于控制在这个累积器中应该如何对long类数据进行操作。在Striped64的longAccumulate方法中我们也看到了LongBinaryOperator作为了参数传入，并在更新值时进行了计算，只是默认在传null的情况下，默认为累加，这也是LongAdder实现累加的原理： LongAdder类累加方法123456789101112131415/** * Adds the given value. * * @param x the value to add */ public void add(long x) &#123; Cell[] as; long b, v; int m; Cell a; if ((as = cells) != null || !casBase(b = base, b + x)) &#123; boolean uncontended = true; if (as == null || (m = as.length - 1) &lt; 0 || (a = as[getProbe() &amp; m]) == null || !(uncontended = a.cas(v = a.value, v + x))) longAccumulate(x, null, uncontended); &#125; &#125; DoubleAdder和DoubleAccumuator同LongAdder和LongAccumulator，这里不再累述。","categories":[{"name":"原创文章","slug":"原创文章","permalink":"http://www.lpnote.com/categories/原创文章/"}],"tags":[{"name":"java","slug":"java","permalink":"http://www.lpnote.com/tags/java/"},{"name":"多线程","slug":"多线程","permalink":"http://www.lpnote.com/tags/多线程/"},{"name":"并发","slug":"并发","permalink":"http://www.lpnote.com/tags/并发/"}]},{"title":"一次Skywalking内存泄露的原因分析","slug":"memory-leak-analysis-for-skywalking","date":"2019-01-31T03:21:57.000Z","updated":"2019-02-13T08:59:06.536Z","comments":true,"path":"2019/01/31/memory-leak-analysis-for-skywalking/","link":"","permalink":"http://www.lpnote.com/2019/01/31/memory-leak-analysis-for-skywalking/","excerpt":"什么是skywalkingSkywalking 是一款分布式系统的应用程序性能监视工具(APM)，专为微服务、云本机架构和基于容器（Docker、K8s、Mesos）架构而设计。 详细的Skywalking介绍见：Skywalking官网 遇到的问题场景 公司Dev/Test环境 Collector因故宕机很长时间，约两周（无人维护监控） 应用接入端agent内存暴涨导致大量应用内存溢出或告警 Skywalking版本：5.0.0-GA","text":"什么是skywalkingSkywalking 是一款分布式系统的应用程序性能监视工具(APM)，专为微服务、云本机架构和基于容器（Docker、K8s、Mesos）架构而设计。 详细的Skywalking介绍见：Skywalking官网 遇到的问题场景 公司Dev/Test环境 Collector因故宕机很长时间，约两周（无人维护监控） 应用接入端agent内存暴涨导致大量应用内存溢出或告警 Skywalking版本：5.0.0-GA 问题现象123ERROR 2018-12-03 09:50:47:931 AppAndServiceRegisterClient : AppAndServiceRegisterClient execute fail.org.apache.skywalking.apm.dependencies.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception at org.apache.skywalking.apm.dependencies.io.grpc.stub.ClientCalls.toStatusRuntimeException(ClientCalls.java:222) 另一些应用报错：1java.lang.OutOfMemoryError: GC overhead limit exceeded 1234567891011121314151617181920212223242526272829ERROR 2019-01-20 20:44:21:024 JVMService : send JVM metrics to Collector fail. org.apache.skywalking.apm.dependencies.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception at org.apache.skywalking.apm.dependencies.io.grpc.stub.ClientCalls.toStatusRuntimeException(ClientCalls.java:222) at org.apache.skywalking.apm.dependencies.io.grpc.stub.ClientCalls.getUnchecked(ClientCalls.java:203) at org.apache.skywalking.apm.dependencies.io.grpc.stub.ClientCalls.blockingUnaryCall(ClientCalls.java:132) at org.apache.skywalking.apm.network.proto.JVMMetricsServiceGrpc$JVMMetricsServiceBlockingStub.collect(JVMMetricsServiceGrpc.java:158) at org.apache.skywalking.apm.agent.core.jvm.JVMService$Sender.run(JVMService.java:143) at org.apache.skywalking.apm.util.RunnableWithExceptionProtection.run(RunnableWithExceptionProtection.java:36) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)Caused by: org.apache.skywalking.apm.dependencies.io.netty.channel.AbstractChannel$AnnotatedConnectException: 拒绝连接: /172.21.16.175:11800 at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717) at org.apache.skywalking.apm.dependencies.io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:325) at org.apache.skywalking.apm.dependencies.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340) at org.apache.skywalking.apm.dependencies.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:634) at org.apache.skywalking.apm.dependencies.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:581) at org.apache.skywalking.apm.dependencies.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:498) at org.apache.skywalking.apm.dependencies.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:460) at org.apache.skywalking.apm.dependencies.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:884) at org.apache.skywalking.apm.dependencies.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) ... 1 moreCaused by: java.net.ConnectException: 拒绝连接 ... 11 more 这些日志大量出现在应用日志中，从发生内存溢出以前很久时间一直持续。 通过jmap将java内存对象状态dump出来分析： 发现存在大量skywalking对象占据内存，具体对象为：HpackHeaderField和ManagedChannelImpl。 问题分析HpackHeaderField和ManagedChannelImpl均为处理gRPC的处理类，并根据内存泄露时报的错误来看，Collector挂掉了很久，一直在重试连接。查看skywalking的源码分析重连过程： GRPCChannelManager 1234567891011121314151617181920212223242526272829303132@Override public void run() &#123; logger.debug(\"Selected collector grpc service running, reconnect:&#123;&#125;.\", reconnect); if (reconnect) &#123; if (RemoteDownstreamConfig.Collector.GRPC_SERVERS.size() &gt; 0) &#123; String server = \"\"; try &#123; int index = Math.abs(random.nextInt()) % RemoteDownstreamConfig.Collector.GRPC_SERVERS.size(); server = RemoteDownstreamConfig.Collector.GRPC_SERVERS.get(index); String[] ipAndPort = server.split(\":\"); managedChannel = GRPCChannel.newBuilder(ipAndPort[0], Integer.parseInt(ipAndPort[1])) .addManagedChannelBuilder(new StandardChannelBuilder()) .addManagedChannelBuilder(new TLSChannelBuilder()) .addChannelDecorator(new AuthenticationDecorator()) .build(); if (!managedChannel.isShutdown() &amp;&amp; !managedChannel.isTerminated()) &#123; reconnect = false; notify(GRPCChannelStatus.CONNECTED); &#125; else &#123; notify(GRPCChannelStatus.DISCONNECT); &#125; return; &#125; catch (Throwable t) &#123; logger.error(t, \"Create channel to &#123;&#125; fail.\", server); notify(GRPCChannelStatus.DISCONNECT); &#125; &#125; logger.debug(\"Selected collector grpc service is not available. Wait &#123;&#125; seconds to retry\", Config.Collector.GRPC_CHANNEL_CHECK_INTERVAL); &#125; &#125; 从上面的代码可知，当collector挂掉后，agent在尝试重连而一直连接不上时，会不断的创建ManagedChannel对象，查看gRPC的ManagedChannelOrphanWrapper源码： 12345678910111213141516final class ManagedChannelOrphanWrapper extends ForwardingManagedChannel &#123; private static final ReferenceQueue&lt;ManagedChannelOrphanWrapper&gt; refqueue = new ReferenceQueue&lt;ManagedChannelOrphanWrapper&gt;(); // Retain the References so they don't get GC'd private static final ConcurrentMap&lt;ManagedChannelReference, ManagedChannelReference&gt; refs = new ConcurrentHashMap&lt;ManagedChannelReference, ManagedChannelReference&gt;(); private static final Logger logger = Logger.getLogger(ManagedChannelOrphanWrapper.class.getName()); private final ManagedChannelReference phantom; ManagedChannelOrphanWrapper(ManagedChannel delegate) &#123; this(delegate, refqueue, refs); &#125;... 此后代码省略 上面有一句话明确提示：Retain the References so they don’t get GC’d同时也初始化了一些Netty相关的处理类，并且没有释放。 翻看了一些gRPC的文档，也是建议一定要显式的关闭channel。 修改了agent部分的源码，处理为在重连时关闭旧的Channel对象： 12345678910111213141516171819202122232425262728293031323334353637@Override public void run() &#123; logger.debug(\"Selected collector grpc service running, reconnect:&#123;&#125;.\", reconnect); if (reconnect) &#123; if (RemoteDownstreamConfig.Collector.GRPC_SERVERS.size() &gt; 0) &#123; String server = \"\"; GRPCChannel oldChannel = managedChannel; try &#123; int index = Math.abs(random.nextInt()) % RemoteDownstreamConfig.Collector.GRPC_SERVERS.size(); server = RemoteDownstreamConfig.Collector.GRPC_SERVERS.get(index); String[] ipAndPort = server.split(\":\"); managedChannel = GRPCChannel.newBuilder(ipAndPort[0], Integer.parseInt(ipAndPort[1])) .addManagedChannelBuilder(new StandardChannelBuilder()) .addManagedChannelBuilder(new TLSChannelBuilder()) .addChannelDecorator(new AuthenticationDecorator()) .build(); if (!managedChannel.isShutdown() &amp;&amp; !managedChannel.isTerminated()) &#123; reconnect = false; notify(GRPCChannelStatus.CONNECTED); &#125; else &#123; notify(GRPCChannelStatus.DISCONNECT); &#125; return; &#125; catch (Throwable t) &#123; logger.error(t, \"Create channel to &#123;&#125; fail.\", server); notify(GRPCChannelStatus.DISCONNECT); &#125; finally &#123; if (oldChannel != null) &#123; oldChannel.shutdownNow(); &#125; &#125; &#125; logger.debug(\"Selected collector grpc service is not available. Wait &#123;&#125; seconds to retry\", Config.Collector.GRPC_CHANNEL_CHECK_INTERVAL); &#125; &#125; 打包推送至Dev环境进行观察，同时也模拟Collector的情况将Collector杀死。经过一段时间的观察，内存平稳无异常。 到此skywalking的特定情况下的内存泄漏问题得到解决，相关issue已由同事提交到skywalking官方。","categories":[{"name":"原创文章","slug":"原创文章","permalink":"http://www.lpnote.com/categories/原创文章/"}],"tags":[{"name":"问题解析","slug":"问题解析","permalink":"http://www.lpnote.com/tags/问题解析/"},{"name":"参与开源","slug":"参与开源","permalink":"http://www.lpnote.com/tags/参与开源/"},{"name":"skywalking","slug":"skywalking","permalink":"http://www.lpnote.com/tags/skywalking/"},{"name":"apm","slug":"apm","permalink":"http://www.lpnote.com/tags/apm/"}]},{"title":"合并写(write combining)","slug":"write-combining","date":"2019-01-31T02:46:33.000Z","updated":"2019-02-13T08:59:06.552Z","comments":true,"path":"2019/01/31/write-combining/","link":"","permalink":"http://www.lpnote.com/2019/01/31/write-combining/","excerpt":"","text":"转载自并发编程网 – ifeve.com 本文链接地址: 合并写(write combining) 译者：无叶 校对：丁一 现代CPU采用了大量的技术来抵消内存访问带来的延迟。读写内存数据期间，CPU能执行成百上千条指令。 多级SRAM缓存是减小这种延迟带来的影响的主要手段。此外，SMP系统采用消息传递协议来实现缓存之间的一致性。遗憾的是，现代的CPU实在是太快了，即使是使用了缓存，有时也无法跟上CPU的速度。因此，为了进一步减小延迟的影响，一些鲜为人知的缓冲区派上了用场。 本文将探讨“合并写存储缓冲区（write combining store buffers）”，以及如何写出有效利用它们的代码。 CPU缓存是一种高效的非链式结构的hash map，每个桶（bucket）通常是64个字节。这就是一个“缓存行（cache line）”。缓存行是内存交换的实际单位。例如，主存中地址A会映射到一个给定的缓存行C。 如果CPU需要访问的地址hash后的行尚不在缓存中，那么缓存中对应位置的缓存行会被清除，以便载入新的行。例如，如果我们有两个地址，通过hash算法hash到同一缓存行，那么新的值会覆盖老的值。 当CPU执行存储指令（store）时，它会尝试将数据写到离CPU最近的L1缓存。如果此时出现缓存未命中，CPU会访问下一级缓存。此时，无论是英特尔还是许多其它厂商的CPU都会使用一种称为“合并写（write combining）”的技术。 在请求L2缓存行的所有权尚未完成时，待存储的数据被写到处理器自身的众多跟缓存行一样大小的存储缓冲区之一。这些芯片上的缓冲区允许CPU在缓存子系统未准备好接收和处理数据时继续执行指令。当数据不在任何其它级别的缓存中时，将获得最大的优势。 当后续的写操作需要修改相同的缓存行时，这些缓冲区变得非常有趣。在将后续的写操作提交到L2缓存之前，可以进行缓冲区写合并。 这些64字节的缓冲区维护了一个64位的字段，每更新一个字节就会设置对应的位，来表示将缓冲区交换到外部缓存时哪些数据是有效的。 也许你要问，如果程序要读取已被写入缓冲区的某些数据，会怎么样？我们的硬件工程师已经考虑到了这点，在读取缓存之前会先去读取缓冲区的。 这一切对我们的程序意味着什么？ 如果我们能在缓冲区被传输到外部缓存之前将其填满，那么将大大提高各级传输总线的效率。如何才能做到这一点呢？好的程序将大部分时间花在循环处理任务上。 这些缓冲区的数量是有限的，且随CPU模型而异。例如在Intel CPU中，同一时刻只能拿到4个。这意味着，在一个循环中，你不应该同时写超过4个不同的内存位置，否则你将不能享受到合并写（write combining）的好处。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public final class WriteCombining &#123; private static final int ITERATIONS = Integer.MAX_VALUE; private static final int ITEMS = 1 &lt;&lt; 24; private static final int MASK = ITEMS - 1; private static final byte[] arrayA = new byte[ITEMS]; private static final byte[] arrayB = new byte[ITEMS]; private static final byte[] arrayC = new byte[ITEMS]; private static final byte[] arrayD = new byte[ITEMS]; private static final byte[] arrayE = new byte[ITEMS]; private static final byte[] arrayF = new byte[ITEMS]; public static void main(final String[] args) &#123; for (int i = 1; i &lt;= 3; i++) &#123; out.println(i + \" SingleLoop duration (ns) = \" + runCaseOne()); out.println(i + \" SplitLoop duration (ns) = \" + runCaseTwo()); &#125; int result = arrayA[1] + arrayB[2] + arrayC[3] + arrayD[4] + arrayE[5] + arrayF[6]; out.println(\"result = \" + result); &#125; public static long runCaseOne() &#123; long start = System.nanoTime(); int i = ITERATIONS; while (--i != 0) &#123; int slot = i &amp; MASK; byte b = (byte) i; arrayA[slot] = b; arrayB[slot] = b; arrayC[slot] = b; arrayD[slot] = b; arrayE[slot] = b; arrayF[slot] = b; &#125; return System.nanoTime() - start; &#125; public static long runCaseTwo() &#123; long start = System.nanoTime(); int i = ITERATIONS; while (--i != 0) &#123; int slot = i &amp; MASK; byte b = (byte) i; arrayA[slot] = b; arrayB[slot] = b; arrayC[slot] = b; &#125; i = ITERATIONS; while (--i != 0) &#123; int slot = i &amp; MASK; byte b = (byte) i; arrayD[slot] = b; arrayE[slot] = b; arrayF[slot] = b; &#125; return System.nanoTime() - start; &#125;&#125; 这个程序在我的Windows 7 64位英特尔酷睿i7860@2.8 GHz系统上产生的输出如下： 1 SingleLoop duration (ns) = 14019753545 1 SplitLoop duration (ns) = 8972368661 2 SingleLoop duration (ns) = 14162455066 2 SplitLoop duration (ns) = 8887610558 3 SingleLoop duration (ns) = 13800914725 3 SplitLoop duration (ns) = 7271752889 上面的例子说明：如果在一个循环中修改6个数组位置（内存地址），程序的运行时间明显长于将任务拆分的方式，即，先写前3个位置，再修改后3个位置。 通过拆分循环，我们做了更多的工作，但程序花费的时间更少！欢迎利用神奇的“合并写（write combining）”。通过使用CPU架构的知识，正确的填充这些缓冲区，我们可以利用底层硬件加速我们的程序。 不要忘了超线程（hyper-threading），可能会有2个线程竞争同一个核的缓冲区。 转载自并发编程网 – ifeve.com 本文链接地址: 合并写(write combining)","categories":[{"name":"转载文章","slug":"转载文章","permalink":"http://www.lpnote.com/categories/转载文章/"}],"tags":[{"name":"java","slug":"java","permalink":"http://www.lpnote.com/tags/java/"},{"name":"多线程","slug":"多线程","permalink":"http://www.lpnote.com/tags/多线程/"},{"name":"并发","slug":"并发","permalink":"http://www.lpnote.com/tags/并发/"}]},{"title":"伪共享(False Sharing)","slug":"false-sharing","date":"2019-01-31T02:27:31.000Z","updated":"2019-02-13T08:59:06.524Z","comments":true,"path":"2019/01/31/false-sharing/","link":"","permalink":"http://www.lpnote.com/2019/01/31/false-sharing/","excerpt":"转载自并发编程网 – ifeve.com 本文链接地址: 伪共享) 原文地址：http://ifeve.com/false-sharing/ 作者：Martin Thompson 译者：丁一 缓存系统中是以缓存行（cache line）为单位存储的。缓存行是2的整数幂个连续字节，一般为32-256个字节。最常见的缓存行大小是64个字节。当多线程修改互相独立的变量时，如果这些变量共享同一个缓存行，就会无意中影响彼此的性能，这就是伪共享。缓存行上的写竞争是运行在SMP系统中并行线程实现可伸缩性最重要的限制因素。有人将伪共享描述成无声的性能杀手，因为从代码中很难看清楚是否会出现伪共享。 为了让可伸缩性与线程数呈线性关系，就必须确保不会有两个线程往同一个变量或缓存行中写。两个线程写同一个变量可以在代码中发现。为了确定互相独立的变量是否共享了同一个缓存行，就需要了解内存布局，或找个工具告诉我们。Intel VTune就是这样一个分析工具。本文中我将解释Java对象的内存布局以及我们该如何填充缓存行以避免伪共享。","text":"转载自并发编程网 – ifeve.com 本文链接地址: 伪共享) 原文地址：http://ifeve.com/false-sharing/ 作者：Martin Thompson 译者：丁一 缓存系统中是以缓存行（cache line）为单位存储的。缓存行是2的整数幂个连续字节，一般为32-256个字节。最常见的缓存行大小是64个字节。当多线程修改互相独立的变量时，如果这些变量共享同一个缓存行，就会无意中影响彼此的性能，这就是伪共享。缓存行上的写竞争是运行在SMP系统中并行线程实现可伸缩性最重要的限制因素。有人将伪共享描述成无声的性能杀手，因为从代码中很难看清楚是否会出现伪共享。 为了让可伸缩性与线程数呈线性关系，就必须确保不会有两个线程往同一个变量或缓存行中写。两个线程写同一个变量可以在代码中发现。为了确定互相独立的变量是否共享了同一个缓存行，就需要了解内存布局，或找个工具告诉我们。Intel VTune就是这样一个分析工具。本文中我将解释Java对象的内存布局以及我们该如何填充缓存行以避免伪共享。 图1说明了伪共享的问题。在核心1上运行的线程想更新变量X，同时核心2上的线程想要更新变量Y。不幸的是，这两个变量在同一个缓存行中。每个线程都要去竞争缓存行的所有权来更新变量。如果核心1获得了所有权，缓存子系统将会使核心2中对应的缓存行失效。当核心2获得了所有权然后执行更新操作，核心1就要使自己对应的缓存行失效。这会来来回回的经过L3缓存，大大影响了性能。如果互相竞争的核心位于不同的插槽，就要额外横跨插槽连接，问题可能更加严重。 Java内存布局(Java Memory Layout)对于HotSpot JVM，所有对象都有两个字长的对象头。第一个字是由24位哈希码和8位标志位（如锁的状态或作为锁对象）组成的Mark Word。第二个字是对象所属类的引用。如果是数组对象还需要一个额外的字来存储数组的长度。每个对象的起始地址都对齐于8字节以提高性能。因此当封装对象的时候为了高效率，对象字段声明的顺序会被重排序成下列基于字节大小的顺序： doubles (8) 和 longs (8) ints (4) 和 floats (4) shorts (2) 和 chars (2) booleans (1) 和 bytes (1) references (4/8) &lt;子类字段重复上述顺序&gt;（译注：更多HotSpot虚拟机对象结构相关内容:http://www.infoq.com/cn/articles/jvm-hotspot） 了解这些之后就可以在任意字段间用7个long来填充缓存行。在Disruptor里我们对RingBuffer的cursor和BatchEventProcessor的序列进行了缓存行填充。 为了展示其性能影响，我们启动几个线程，每个都更新它自己独立的计数器。计数器是volatile long类型的，所以其它线程能看到它们的进展。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public final class FalseSharing implements Runnable &#123; public final static int NUM_THREADS = 4; // change public final static long ITERATIONS = 500L * 1000L * 1000L; private final int arrayIndex; private static VolatileLong[] longs = new VolatileLong[NUM_THREADS]; static &#123; for (int i = 0; i &lt; longs.length; i++) &#123; longs[i] = new VolatileLong(); &#125; &#125; public FalseSharing(final int arrayIndex) &#123; this.arrayIndex = arrayIndex; &#125; public static void main(final String[] args) throws Exception &#123; final long start = System.nanoTime(); runTest(); System.out.println(\"duration = \" + (System.nanoTime() - start)); &#125; private static void runTest() throws InterruptedException &#123; Thread[] threads = new Thread[NUM_THREADS]; for (int i = 0; i &lt; threads.length; i++) &#123; threads[i] = new Thread(new FalseSharing(i)); &#125; for (Thread t : threads) &#123; t.start(); &#125; for (Thread t : threads) &#123; t.join(); &#125; &#125; public void run() &#123; long i = ITERATIONS + 1; while (0 != --i) &#123; longs[arrayIndex].value = i; &#125; &#125; public final static class VolatileLong &#123; public volatile long value = 0L; public long p1, p2, p3, p4, p5, p6; // comment out &#125;&#125; 结果(Results)运行上面的代码，增加线程数以及添加/移除缓存行的填充，下面的图2描述了我得到的结果。这是在我4核Nehalem上测得的运行时间。 从不断上升的测试所需时间中能够明显看出伪共享的影响。没有缓存行竞争时，我们几近达到了随着线程数的线性扩展。 这并不是个完美的测试，因为我们不能确定这些VolatileLong会布局在内存的什么位置。它们是独立的对象。但是经验告诉我们同一时间分配的对象趋向集中于一块。 所以你也看到了，伪共享可能是无声的性能杀手。 转载自并发编程网 – ifeve.com 本文链接地址: 伪共享)","categories":[{"name":"转载文章","slug":"转载文章","permalink":"http://www.lpnote.com/categories/转载文章/"}],"tags":[{"name":"java","slug":"java","permalink":"http://www.lpnote.com/tags/java/"},{"name":"多线程","slug":"多线程","permalink":"http://www.lpnote.com/tags/多线程/"},{"name":"并发","slug":"并发","permalink":"http://www.lpnote.com/tags/并发/"}]},{"title":"groovy-expr-usage","slug":"groovy-expr-usage","date":"2019-01-08T11:23:32.000Z","updated":"2019-02-13T08:59:06.528Z","comments":true,"path":"2019/01/08/groovy-expr-usage/","link":"","permalink":"http://www.lpnote.com/2019/01/08/groovy-expr-usage/","excerpt":"在开发单据规则计算引擎的时候引入了groovy脚本计算引擎，其中有一个规则函数：正则函数需要使用正则表达式计算，顺便找了一下groovy里的正则表达式： groovy中对于正则表达式的书写进行了简化，同时它仍然是引用的java核心的正则表达式引擎，并没有自己实现一套正则引擎，更多的是从语法糖的形式上进行优化，让人使用起来格外的舒服。 查找（find）操作符：=~ 返回Matcher类型 匹配（match）操作符：==~ 返回boolean类型 模式(pattern)操作符：~String 返回Pattern类型","text":"在开发单据规则计算引擎的时候引入了groovy脚本计算引擎，其中有一个规则函数：正则函数需要使用正则表达式计算，顺便找了一下groovy里的正则表达式： groovy中对于正则表达式的书写进行了简化，同时它仍然是引用的java核心的正则表达式引擎，并没有自己实现一套正则引擎，更多的是从语法糖的形式上进行优化，让人使用起来格外的舒服。 查找（find）操作符：=~ 返回Matcher类型 匹配（match）操作符：==~ 返回boolean类型 模式(pattern)操作符：~String 返回Pattern类型 123456789101112class ExprCheck implements FunctionInvoke &#123; def EXPR_PARAM = \"expr\"; FunctionResult invoke(FunctionContext ctx) &#123; def currentVal = ctx.currentVal; def exprStr = ctx.systemParams.get(EXPR_PARAM); def expr = ~exprStr; return new FunctionResult(currentVal ==~ expr); &#125;&#125; 测试：123456789101112131415161718192021222324252627282930313233343536class ExprCheckTest &#123; @Test public void test1() &#123; FunctionContext ctx = new FunctionContext(); ctx.currentVal = \"hello\" ctx.systemParams = [\"expr\":\"hello\"] ExprCheck check = new ExprCheck(); FunctionResult result = check.invoke(ctx); assertTrue(result.valid); &#125; @Test public void test2() &#123; FunctionContext ctx = new FunctionContext(); ctx.currentVal = \"hello1\" ctx.systemParams = [\"expr\":\"hellod+\"] ExprCheck check = new ExprCheck(); FunctionResult result = check.invoke(ctx); assertTrue(!result.valid); &#125; @Test public void test3() &#123; FunctionContext ctx = new FunctionContext(); ctx.currentVal = \"hello1\" ctx.systemParams = [\"expr\":\"hello\\\\d+\"] ExprCheck check = new ExprCheck(); FunctionResult result = check.invoke(ctx); assertTrue(result.valid); &#125;&#125;","categories":[{"name":"原创文章","slug":"原创文章","permalink":"http://www.lpnote.com/categories/原创文章/"}],"tags":[{"name":"groovy","slug":"groovy","permalink":"http://www.lpnote.com/tags/groovy/"},{"name":"正则表达式","slug":"正则表达式","permalink":"http://www.lpnote.com/tags/正则表达式/"}]},{"title":"jackson-ctrl-char-problem-resovle","slug":"jackson-ctrl-char-problem-resovle","date":"2019-01-08T08:32:32.000Z","updated":"2019-02-13T08:59:06.532Z","comments":true,"path":"2019/01/08/jackson-ctrl-char-problem-resovle/","link":"","permalink":"http://www.lpnote.com/2019/01/08/jackson-ctrl-char-problem-resovle/","excerpt":"","text":"在使用swagger传递json数据的时候，突然报错：1org.codehaus.jackson.JsonParseException: Illegal unquoted character ((CTRL-CHAR, code 10)) 意思是说使用了在json内容中使用了控制字符。而这个code 10是说使用了换行字符。 解决方法: 方式1. 使用显式转义方式 使用\\n代替控制性转行（不可打印）字符 方式2. 配置Jackson12ObjectMapper mapp = new ObjectMapper();mapper.configure(JsonParser.Feature.ALLOW_UNQUOTED_CONTROL_CHARS, true);","categories":[{"name":"原创文章","slug":"原创文章","permalink":"http://www.lpnote.com/categories/原创文章/"}],"tags":[{"name":"java","slug":"java","permalink":"http://www.lpnote.com/tags/java/"},{"name":"json","slug":"json","permalink":"http://www.lpnote.com/tags/json/"},{"name":"jackson","slug":"jackson","permalink":"http://www.lpnote.com/tags/jackson/"}]},{"title":"solve-a-disk-warning-illusion-caused-by-rocketmq","slug":"solve-a-disk-warning-illusion-caused-by-rocketmq","date":"2019-01-04T07:45:30.000Z","updated":"2019-02-13T08:59:06.536Z","comments":true,"path":"2019/01/04/solve-a-disk-warning-illusion-caused-by-rocketmq/","link":"","permalink":"http://www.lpnote.com/2019/01/04/solve-a-disk-warning-illusion-caused-by-rocketmq/","excerpt":"","text":"最近一段时间在运维部署rocketmq的过程中，启动时频繁报一个奇怪的错： 12345678910111213141516171819202018-12-19 22:20:58 INFO StoreScheduledThread1 - begin to delete before 336 hours file. timeup: false spacefull: true manualDeleteFileSeveralTimes: 0 cleanAtOnce: false2018-12-19 22:20:58 WARN StoreScheduledThread1 - disk space will be full soon, but delete file failed.2018-12-19 22:21:08 INFO StoreScheduledThread1 - physic disk maybe full soon, so reclaim space, -1.02018-12-19 22:21:08 INFO StoreScheduledThread1 - begin to delete before 336 hours file. timeup: false spacefull: true manualDeleteFileSeveralTimes: 0 cleanAtOnce: false2018-12-19 22:21:08 WARN StoreScheduledThread1 - disk space will be full soon, but delete file failed.2018-12-19 22:21:15 INFO StoreStatsService - [STORETPS] put_tps 0.0 get_found_tps 0.0 get_miss_tps 1.799730040493926 get_transfered_tps 0.02018-12-19 22:21:15 INFO StoreStatsService - [PAGECACHERT] TotalPut 0, PutMessageDistributeTime [&lt;=0ms]:0 [0~10ms]:0 [10~50ms]:0 [50~100ms]:0 [100~200ms]:0 [200~500ms]:0 [500ms~1s]:0 [1~2s]:0 [2~3s]:0 [3~4s]:0 [4~5s]:0 [5~10s]:0 [10s~]:0 2018-12-19 22:21:18 INFO StoreScheduledThread1 - physic disk maybe full soon, so reclaim space, -1.02018-12-19 22:21:18 INFO StoreScheduledThread1 - begin to delete before 336 hours file. timeup: false spacefull: true manualDeleteFileSeveralTimes: 0 cleanAtOnce: false2018-12-19 22:21:18 WARN StoreScheduledThread1 - disk space will be full soon, but delete file failed.2018-12-19 22:21:28 INFO StoreScheduledThread1 - physic disk maybe full soon, so reclaim space, -1.02018-12-19 22:21:28 INFO StoreScheduledThread1 - begin to delete before 336 hours file. timeup: false spacefull: true manualDeleteFileSeveralTimes: 0 cleanAtOnce: false2018-12-19 22:21:28 WARN StoreScheduledThread1 - disk space will be full soon, but delete file failed.2018-12-19 22:21:38 INFO StoreScheduledThread1 - physic disk maybe full soon, so reclaim space, -1.02018-12-19 22:21:38 INFO StoreScheduledThread1 - begin to delete before 336 hours file. timeup: false spacefull: true manualDeleteFileSeveralTimes: 0 cleanAtOnce: false2018-12-19 22:21:38 WARN StoreScheduledThread1 - disk space will be full soon, but delete file failed.2018-12-19 22:21:48 INFO StoreScheduledThread1 - physic disk maybe full soon, so reclaim space, -1.02018-12-19 22:21:48 INFO StoreScheduledThread1 - begin to delete before 336 hours file. timeup: false spacefull: true manualDeleteFileSeveralTimes: 0 cleanAtOnce: false2018-12-19 22:21:48 WARN StoreScheduledThread1 - disk space will be full soon, but delete file failed.2018-12-19 22:21:58 INFO StoreScheduledThread1 - physic disk maybe full soon, so reclaim space, -1.0 而当时查看了磁盘的容量，远远没有达到rocketmq的磁盘容量警告阀值。剩余的磁盘空间还非常多，一开始是怀疑运维人员没有将rocketmq的存储目录挂载到数据盘，但是经过沟通后发现已经挂载了。 最后没办法只能是通过阅读rocketmq源代码找原因： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758private boolean isSpaceToDelete() &#123; double ratio = DefaultMessageStore.this.getMessageStoreConfig().getDiskMaxUsedSpaceRatio() / 100.0; cleanImmediately = false; &#123; String storePathPhysic = DefaultMessageStore.this.getMessageStoreConfig().getStorePathCommitLog(); double physicRatio = UtilAll.getDiskPartitionSpaceUsedPercent(storePathPhysic); if (physicRatio &gt; diskSpaceWarningLevelRatio) &#123; boolean diskok = DefaultMessageStore.this.runningFlags.getAndMakeDiskFull(); if (diskok) &#123; DefaultMessageStore.log.error(\"physic disk maybe full soon \" + physicRatio + \", so mark disk full\"); &#125; cleanImmediately = true; &#125; else if (physicRatio &gt; diskSpaceCleanForciblyRatio) &#123; cleanImmediately = true; &#125; else &#123; boolean diskok = DefaultMessageStore.this.runningFlags.getAndMakeDiskOK(); if (!diskok) &#123; DefaultMessageStore.log.info(\"physic disk space OK \" + physicRatio + \", so mark disk ok\"); &#125; &#125; if (physicRatio &lt; 0 || physicRatio &gt; ratio) &#123; DefaultMessageStore.log.info(\"physic disk maybe full soon, so reclaim space, \" + physicRatio); return true; &#125; &#125; &#123; String storePathLogics = StorePathConfigHelper .getStorePathConsumeQueue(DefaultMessageStore.this.getMessageStoreConfig().getStorePathRootDir()); double logicsRatio = UtilAll.getDiskPartitionSpaceUsedPercent(storePathLogics); if (logicsRatio &gt; diskSpaceWarningLevelRatio) &#123; boolean diskok = DefaultMessageStore.this.runningFlags.getAndMakeDiskFull(); if (diskok) &#123; DefaultMessageStore.log.error(\"logics disk maybe full soon \" + logicsRatio + \", so mark disk full\"); &#125; cleanImmediately = true; &#125; else if (logicsRatio &gt; diskSpaceCleanForciblyRatio) &#123; cleanImmediately = true; &#125; else &#123; boolean diskok = DefaultMessageStore.this.runningFlags.getAndMakeDiskOK(); if (!diskok) &#123; DefaultMessageStore.log.info(\"logics disk space OK \" + logicsRatio + \", so mark disk ok\"); &#125; &#125; if (logicsRatio &lt; 0 || logicsRatio &gt; ratio) &#123; DefaultMessageStore.log.info(\"logics disk maybe full soon, so reclaim space, \" + logicsRatio); return true; &#125; &#125; return false; &#125; 关键错误出现在： 1double physicRatio = UtilAll.getDiskPartitionSpaceUsedPercent(storePathPhysic); 这里出现返回-1的情况，仔细捋了一把这个工具类的源码： 123456789101112131415161718192021222324public static double getDiskPartitionSpaceUsedPercent(final String path) &#123; if (null == path || path.isEmpty()) return -1; try &#123; File file = new File(path); if (!file.exists()) return -1; long totalSpace = file.getTotalSpace(); if (totalSpace &gt; 0) &#123; long freeSpace = file.getFreeSpace(); long usedSpace = totalSpace - freeSpace; return usedSpace / (double) totalSpace; &#125; &#125; catch (Exception e) &#123; return -1; &#125; return -1; &#125; 综合以上的现象发现只能是发生了异常，而在异常这里，rocketmq自己吃掉了异常，并返回了-1。 这里个人感觉rocketmq团队在这里处理的方式非常不友好，不仅吃掉了异常而且还返回了一个没意义的值！ 而为什么在计算磁盘空间的时候会出现异常呢，目前能想到的一个原因可能是因为安全原因，导致问题出现，而在linux下selinux是产生文件方面安全问题的重要原因。 解决方案：叫运维关闭selinux后，情况恢复正常。","categories":[{"name":"原创文章","slug":"原创文章","permalink":"http://www.lpnote.com/categories/原创文章/"}],"tags":[{"name":"rocketmq","slug":"rocketmq","permalink":"http://www.lpnote.com/tags/rocketmq/"}]},{"title":"hexoclient-usage","slug":"hexoclient-usage","date":"2018-12-30T04:30:21.000Z","updated":"2019-02-13T08:59:06.528Z","comments":true,"path":"2018/12/30/hexoclient-usage/","link":"","permalink":"http://www.lpnote.com/2018/12/30/hexoclient-usage/","excerpt":"","text":"HexoClient使用帮助 使用方法链接见：HexoClient使用帮助","categories":[{"name":"转载文章","slug":"转载文章","permalink":"http://www.lpnote.com/categories/转载文章/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://www.lpnote.com/tags/hexo/"}]},{"title":"定制springcloud服务注册到consul中的instanceId","slug":"customize-instance-id-with-consul-service-registry","date":"2018-12-23T05:30:09.000Z","updated":"2019-02-13T08:59:06.520Z","comments":true,"path":"2018/12/23/customize-instance-id-with-consul-service-registry/","link":"","permalink":"http://www.lpnote.com/2018/12/23/customize-instance-id-with-consul-service-registry/","excerpt":"背景在使用SpringCloud构建微服务过程中，我们使用Consul作为服务的注册中心，中间过程也踩了不少的坑，今天又踩了一个：我们根据官方的建议，在注册springcloud服务的时候，instanceId使用的是以下的配置：1234567891011spring: cloud: consul: host: 127.0.0.1 port: 8500 discovery: health-check-path: /management/health service-name: mq-gateway health-check-interval: 10s prefer-ip-address: true instance-id: $&#123;spring.cloud.consul.discovery.service-name&#125;:$&#123;server.port&#125;:$&#123;random.value&#125; 重点就在于这个instance-id的配置，它由服务名+服务端口+随机值组成。这种看起来唯一且没有什么问题的配置，却是接下来坑的开始。","text":"背景在使用SpringCloud构建微服务过程中，我们使用Consul作为服务的注册中心，中间过程也踩了不少的坑，今天又踩了一个：我们根据官方的建议，在注册springcloud服务的时候，instanceId使用的是以下的配置：1234567891011spring: cloud: consul: host: 127.0.0.1 port: 8500 discovery: health-check-path: /management/health service-name: mq-gateway health-check-interval: 10s prefer-ip-address: true instance-id: $&#123;spring.cloud.consul.discovery.service-name&#125;:$&#123;server.port&#125;:$&#123;random.value&#125; 重点就在于这个instance-id的配置，它由服务名+服务端口+随机值组成。这种看起来唯一且没有什么问题的配置，却是接下来坑的开始。 问题在微服务的开发过程中，不断有开发人员抱怨在开发过程中一些不正常的停止微服务会导致consul上的服务注册实例越来越多，而且IP和端口都一模一样，究其原因是因为不正常的停止导致consul无法正常反注册服务，导致服务注册驻留在consul上，并变为critical状态，而当程序重启时，重新注册的instance-id又会随着${random.value}的配置而与之前的配置不同，这就导致了不断有不同instance-id的实例注册到consul上，而且他们的健康检测url都一样，这个时候当新服务启动后，所有的原有的critical状态的服务的健康检测都能通过，这时候看到的现象就是consul上这个服务挂了很多个实例（其实这些实例都是同一个服务实例）。 而且出于安全的原因，有个非常蛋疼的地方在于consul的服务实例反注册还只能由服务注册所在的机器发起才能反注册。 关于实例重复被注册，在SpringCloud的Github上也有讨论，链接在这里。不过从维护者的回答看出来，好像官方并没有打算做这方面的改进措施。 求人不如求己，自己也试着来看看有没有解决方案吧。 解决方案一、通过注册修改微服务健康检测的url来规避因为多个实例中健康检测的url相同，所以没法区分哪个是正常的实例，所以我们只需要将健康检测的url变成不相同即可，简单的实现如下： 1234567891011spring: cloud: consul: host: 127.0.0.1 port: 8500 discovery: health-check-path: /$&#123;spring.cloud.consul.discovery.instance-id&#125;/management/health service-name: mq-gateway health-check-interval: 10s prefer-ip-address: true instance-id: $&#123;spring.cloud.consul.discovery.service-name&#125;:$&#123;server.port&#125;:$&#123;random.value&#125; 但是这种方案有一个很大的弊端在于：健康检测的url对于每个服务来说变得不可得，都是一些随机的url，会导致外部的一些监控程序无法通过某种规则构造服务的健康检测url，从而掌握服务的健康状况，这是一种对于监控系统来说非常不友好的方式。 二、通过IP和端口确定instance-id的唯一性同一个程序，多次启动导致instance-id不相同的原因在于${random.value}，我们尝试去掉它，而${spring.cloud.consul.discovery.service-name}:${server.port}并不能保证唯一性，我们需要加上一个特征使它变得唯一，很好想到的就是用IP来限制：服务名+IP+PORT，这样基本就限制住了唯一性。 但是有一个问题是SpringBoot或者SpringCloud并没有提供一个获取本地IP的配置项。这里我们需要仿造${random.value}的配置原理，构造一个我们自己的IP配置获取方式。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192public class RandomValuePropertySource extends PropertySource&lt;Random&gt; &#123; /** * Name of the random &#123;@link PropertySource&#125;. */ public static final String RANDOM_PROPERTY_SOURCE_NAME = \"random\"; private static final String PREFIX = \"random.\"; private static final Log logger = LogFactory.getLog(RandomValuePropertySource.class); public RandomValuePropertySource(String name) &#123; super(name, new Random()); &#125; public RandomValuePropertySource() &#123; this(RANDOM_PROPERTY_SOURCE_NAME); &#125; @Override public Object getProperty(String name) &#123; if (!name.startsWith(PREFIX)) &#123; return null; &#125; if (logger.isTraceEnabled()) &#123; logger.trace(\"Generating random property for '\" + name + \"'\"); &#125; return getRandomValue(name.substring(PREFIX.length())); &#125; private Object getRandomValue(String type) &#123; if (type.equals(\"int\")) &#123; return getSource().nextInt(); &#125; if (type.equals(\"long\")) &#123; return getSource().nextLong(); &#125; String range = getRange(type, \"int\"); if (range != null) &#123; return getNextIntInRange(range); &#125; range = getRange(type, \"long\"); if (range != null) &#123; return getNextLongInRange(range); &#125; if (type.equals(\"uuid\")) &#123; return UUID.randomUUID().toString(); &#125; return getRandomBytes(); &#125; private String getRange(String type, String prefix) &#123; if (type.startsWith(prefix)) &#123; int startIndex = prefix.length() + 1; if (type.length() &gt; startIndex) &#123; return type.substring(startIndex, type.length() - 1); &#125; &#125; return null; &#125; private int getNextIntInRange(String range) &#123; String[] tokens = StringUtils.commaDelimitedListToStringArray(range); int start = Integer.parseInt(tokens[0]); if (tokens.length == 1) &#123; return getSource().nextInt(start); &#125; return start + getSource().nextInt(Integer.parseInt(tokens[1]) - start); &#125; private long getNextLongInRange(String range) &#123; String[] tokens = StringUtils.commaDelimitedListToStringArray(range); if (tokens.length == 1) &#123; return Math.abs(getSource().nextLong() % Long.parseLong(tokens[0])); &#125; long lowerBound = Long.parseLong(tokens[0]); long upperBound = Long.parseLong(tokens[1]) - lowerBound; return lowerBound + Math.abs(getSource().nextLong() % upperBound); &#125; private Object getRandomBytes() &#123; byte[] bytes = new byte[32]; getSource().nextBytes(bytes); return DigestUtils.md5DigestAsHex(bytes); &#125; public static void addToEnvironment(ConfigurableEnvironment environment) &#123; environment.getPropertySources().addAfter( StandardEnvironment.SYSTEM_ENVIRONMENT_PROPERTY_SOURCE_NAME, new RandomValuePropertySource(RANDOM_PROPERTY_SOURCE_NAME)); logger.trace(\"RandomValuePropertySource add to Environment\"); &#125; 这个给了我们很大的提示，我们自己也可以在SpringBoot程序启动的时候注入一个我们自己的ProperySource将机器的IP作为配置项作为其它其它配置项的引用。并结合我们自己的配置中心客户端，可以在开发人员不感知的情况下就把这个事情给做掉。 123456789101112131415161718192021222324252627282930313233public class CustomizeApplication extends SpringApplication &#123; public static ConfigurableApplicationContext run(Object source, String... args) &#123; return run(new Object[] &#123; source &#125;, args); &#125; public static ConfigurableApplicationContext run(Object[] sources, String[] args) &#123; advanceFetchApolloConfig(sources); return new CustomizeApplication(sources).run(args); &#125; ...//此处省略 @Override protected void configurePropertySources(ConfigurableEnvironment environment, String[] args) &#123; super.configurePropertySources(environment, args); //add local overwrite config file if(localOverwriteConfig != null)&#123; environment.getPropertySources().addFirst(localOverwriteConfig); &#125; environment.getPropertySources().addAfter(ConfigConsts.PREDECESSOR_OF_APOLLO, bootstrapConfig); //注入Server相关属性及配置 environment.getPropertySources().addAfter(bootstrapConfig.getName(), new ServerPropertiesSource(environment)); &#125; ...//此处省略&#125; 自定义的PropertySource:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182public class ServerPropertiesSource extends PropertySource&lt;Object&gt; &#123; private Logger logger = LoggerFactory.getLogger(ServerPropertiesSource.class); public static final String SERVER_PROPERTIES_NAME = \"xxx.server\"; public static final String SERVER_ADDR_PATTERN = SERVER_PROPERTIES_NAME + \".addr.pattern\"; private LoadingCache&lt;String, Object&gt; loadingCache = CacheBuilder.newBuilder() .expireAfterAccess(60, TimeUnit.SECONDS) .maximumSize(1000).build(new CacheLoader&lt;String, Object&gt;() &#123; @Override public Object load(String key) throws Exception &#123; return _getProperty(key); &#125; &#125;); private Environment environment; public ServerPropertiesSource(Environment environment) &#123; this(SERVER_PROPERTIES_NAME, environment); &#125; public ServerPropertiesSource(String name, Environment environment) &#123; super(name, new Object()); this.environment = environment; &#125; @Override public Object getProperty(String name) &#123; try &#123; return loadingCache.get(name); &#125; catch (Exception e) &#123; return null; &#125; &#125; public Object _getProperty(String name) &#123; if (!StringUtils.startsWithIgnoreCase(name, SERVER_PROPERTIES_NAME)) &#123; return null; &#125; if (logger.isTraceEnabled()) &#123; logger.trace(\"get server property for '\" + name + \"'\"); &#125; return getServerProperty(name.substring(SERVER_ADDR_PATTERN.length())); &#125; private Object getServerProperty(String subName) &#123; if (StringUtils.startsWithIgnoreCase(subName, \".addr\")) &#123; return getServerIp(); &#125; return null; &#125; private Object getServerIp() &#123; try &#123; String serverAddrPattern = this.environment.getProperty(SERVER_ADDR_PATTERN); if (serverAddrPattern != null) &#123; Pattern pattern = Pattern.compile(serverAddrPattern); return InetAddressUtils.getLocalAddress(pattern); &#125; return InetAddressUtils.getLocalAddress(); &#125; catch (Exception e) &#123; logger.error(e.getMessage(),e); throw e; &#125; &#125;应用配置：```yamlspring: cloud: consul: host: 127.0.0.1 port: 8500 discovery: health-check-path: /management/$&#123;instance-id&#125;/health service-name: mq-gateway health-check-interval: 10s prefer-ip-address: true instance-id: $&#123;spring.cloud.consul.discovery.service-name&#125;:$&#123;xxx.server.addr&#125;:$&#123;server.port&#125; 123456789101112SpringBoot启动：```java@SpringBootApplication@EnableDiscoveryClient@EnableApolloConfig(&#123; &quot;application&quot;, &quot;common.consul&quot;&#125;)public class DemoApplication &#123; public static void main(String[] args) &#123; CustomizeApplication.run(DemoApplication.class, args); &#125;&#125; 注册到Consul中的服务： 至此，大功告成！","categories":[{"name":"原创文章","slug":"原创文章","permalink":"http://www.lpnote.com/categories/原创文章/"}],"tags":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://www.lpnote.com/tags/SpringCloud/"},{"name":"微服务","slug":"微服务","permalink":"http://www.lpnote.com/tags/微服务/"}]},{"title":"Java DNS缓存","slug":"java-dns-cache","date":"2018-11-23T15:30:21.000Z","updated":"2019-02-13T08:59:06.532Z","comments":true,"path":"2018/11/23/java-dns-cache/","link":"","permalink":"http://www.lpnote.com/2018/11/23/java-dns-cache/","excerpt":"","text":"jdk1.5和1.5之前版本默认DNS缓存时间是永久缓存 jdk 1.6以后与security manager策略有关。 如果没有启用security manager，默认解析成功的DNS缓存时间为30秒，解析失败的DNS缓存时间为10秒。 策略配置文件：JAVA_HOME/jre/lib/security/java.policy 注意事项对于多条A记录DNS，在缓存有效期内，取到的IP永远是缓存中全部A记录的第一条，并没有轮循之类的策略。缓存失效之后重新进行DNS解析，如果每次域名解析返回的A记录顺序会发生变化，缓存中的数据顺序也会发生变化，取到的IP也变化。 缓存修改方法 jvm启动参数里面配置-Dsun.net.inetaddr.ttl=value 修改配置文件$JDK_HOME/lib/security/java.security相应的参数networkaddress.cache.ttl=value 代码里直接设置：java.security.Security.setProperty(”networkaddress.cache.ttl” , “value”);","categories":[{"name":"转载文章","slug":"转载文章","permalink":"http://www.lpnote.com/categories/转载文章/"}],"tags":[{"name":"java","slug":"java","permalink":"http://www.lpnote.com/tags/java/"}]},{"title":"delete-git-submodule","slug":"delete-git-submodule","date":"2018-10-23T15:09:23.000Z","updated":"2019-02-13T08:59:06.520Z","comments":true,"path":"2018/10/23/delete-git-submodule/","link":"","permalink":"http://www.lpnote.com/2018/10/23/delete-git-submodule/","excerpt":"","text":"删除一个submodule 删除 .gitsubmodule中对应submodule的条目 删除 .git/config 中对应submodule的条目 执行 git rm –cached {submodule_path}。注意，路径不要加后面的“/”。例如：你的submodule保存在 theme/maupassant/ 目录。执行命令为： git rm –cached theme/maupassant","categories":[{"name":"原创文章","slug":"原创文章","permalink":"http://www.lpnote.com/categories/原创文章/"},{"name":"问题记录","slug":"原创文章/问题记录","permalink":"http://www.lpnote.com/categories/原创文章/问题记录/"}],"tags":[{"name":"git","slug":"git","permalink":"http://www.lpnote.com/tags/git/"}]},{"title":"解决前端JS与后端数据交互长整型精度失真的问题","slug":"accuracy-problems-between-java-and-javascript","date":"2017-12-06T16:00:00.000Z","updated":"2019-02-13T08:59:06.516Z","comments":true,"path":"2017/12/06/accuracy-problems-between-java-and-javascript/","link":"","permalink":"http://www.lpnote.com/2017/12/06/accuracy-problems-between-java-and-javascript/","excerpt":"在项目中采用了twitter开源的snowflake算法的id生成器，生成的id是一个long型的大数，因数值太大，通过json形式传输到前端后，在js解析时，会丢失精度。","text":"在项目中采用了twitter开源的snowflake算法的id生成器，生成的id是一个long型的大数，因数值太大，通过json形式传输到前端后，在js解析时，会丢失精度。 解决办法： 将长整型的数字转为String类型传输到前端，由前端自己负责类型解析。 如果使用的是Jackson工具包： 12@JsonSerialize(using= ToStringSerializer.class)private Long id; 如果使用Fastjson工具包： 局部配置：1234567891011121314151617181920public final static SerializeConfig serializeConfig = new SerializeConfig();static&#123; serializeConfig.put(Long.class, new ObjectSerializer() &#123; @Override public void write(JSONSerializer serializer, Object object, Object fieldName, Type fieldType, int features) throws IOException &#123; SerializeWriter out = serializer.getWriter(); out.writeString(Objects.toString(object, null)); &#125; &#125;);&#125;protected &lt;T&gt; String getResult(T value) &#123; JSONObject json = new JSONObject(); json.put(\"state\", state.getState()); json.put(\"desc\", state.getDesc()); json.put(\"value\", value); return JSON.toJSONString(json, serializeConfig, SerializerFeature.DisableCircularReferenceDetect);&#125; 全局配置：1234567891011SerializeConfig.getGlobalInstance().put(Long.class, new ObjectSerializer() &#123; @Override public void write(JSONSerializer serializer, Object object, Object fieldName, Type fieldType, int features) throws IOException &#123; SerializeWriter out = serializer.getWriter(); if (fieldType == long.class || fieldType == Long.class) &#123; out.writeString(Objects.toString(object, null)); &#125; &#125; &#125;); 另一个方式是自己编写JSONSerializer和JSONDeserializer。 123456789public class LongJsonSerializer extends JsonSerializer&lt;Long&gt; &#123; @Override public void serialize(Long value, JsonGenerator jsonGenerator, SerializerProvider serializerProvider) throws IOException, JsonProcessingException &#123; String text = (value == null ? null : String.valueOf(value)); if (text != null) &#123; jsonGenerator.writeString(text); &#125; &#125;&#125; 12345678910111213public class LongJsonDeserializer extends JsonDeserializer&lt;Long&gt; &#123; private static final Logger logger = LoggerFactory.getLogger(LongJsonDeserializer.class); @Override public Long deserialize(JsonParser jsonParser, DeserializationContext deserializationContext) throws IOException, JsonProcessingException &#123; String value = jsonParser.getText(); try &#123; return value == null ? null : Long.parseLong(value); &#125; catch (NumberFormatException e) &#123; throw new RuntimeException(e); &#125; &#125;&#125; 123@JsonSerialize(using = LongJsonSerializer.class)@JsonDeserialize(using = LongJsonDeserializer.class)private Long id;","categories":[{"name":"问题记录","slug":"问题记录","permalink":"http://www.lpnote.com/categories/问题记录/"}],"tags":[{"name":"问题解析","slug":"问题解析","permalink":"http://www.lpnote.com/tags/问题解析/"},{"name":"java","slug":"java","permalink":"http://www.lpnote.com/tags/java/"},{"name":"javascript","slug":"javascript","permalink":"http://www.lpnote.com/tags/javascript/"}]},{"title":"Apache Kafka,Purgatory以及多级时间轮","slug":"apache-kafka-purgatory-hierarchical-timing-wheels","date":"2017-11-15T16:00:00.000Z","updated":"2019-02-13T08:59:06.516Z","comments":true,"path":"2017/11/15/apache-kafka-purgatory-hierarchical-timing-wheels/","link":"","permalink":"http://www.lpnote.com/2017/11/15/apache-kafka-purgatory-hierarchical-timing-wheels/","excerpt":"原文地址：Apache Kafka, Purgatory, and Hierarchical Timing Wheels Apache Kafka有一个称为“请求Purgatory”的数据结构。 这个数据结构会hold住任何尚未达到标准的成功，但又尚未造成错误的请求。 问题是：我们如何有效地跟踪群集中数以万计的的满足要求的异步请求？ Kafka实现了几个不能立即回应的延时请求类型。 例子： 只有在所有同步副本已经确认写入之后，acks = all的产生请求才能被认为是完整的，并且如果领导失败，我们可以保证它不会丢失。 对于min.bytes = 1的提取请求只有在至少有1个byte的数据能够被消费者消费时才会被回答。 这允许“长时间轮询”，使得消费者不必忙于等待检查新数据到达。 这些请求被认为是完成的： （a）他们所要求的标准完成（b）或者发生一些超时 时刻增长这些异步操作的数量与连接的数量成比例，对于Kafka来说，这往往是成千上万的连接数量。 请求Purgatory被设计用于如此大规模的请求处理，但是旧的实现有一些缺陷。 在这个博客中，我想解释一下旧执行的问题以及新实现如何解决这个问题。 我也将呈现基准测试结果。","text":"原文地址：Apache Kafka, Purgatory, and Hierarchical Timing Wheels Apache Kafka有一个称为“请求Purgatory”的数据结构。 这个数据结构会hold住任何尚未达到标准的成功，但又尚未造成错误的请求。 问题是：我们如何有效地跟踪群集中数以万计的的满足要求的异步请求？ Kafka实现了几个不能立即回应的延时请求类型。 例子： 只有在所有同步副本已经确认写入之后，acks = all的产生请求才能被认为是完整的，并且如果领导失败，我们可以保证它不会丢失。 对于min.bytes = 1的提取请求只有在至少有1个byte的数据能够被消费者消费时才会被回答。 这允许“长时间轮询”，使得消费者不必忙于等待检查新数据到达。 这些请求被认为是完成的： （a）他们所要求的标准完成（b）或者发生一些超时 时刻增长这些异步操作的数量与连接的数量成比例，对于Kafka来说，这往往是成千上万的连接数量。 请求Purgatory被设计用于如此大规模的请求处理，但是旧的实现有一些缺陷。 在这个博客中，我想解释一下旧执行的问题以及新实现如何解决这个问题。 我也将呈现基准测试结果。 旧的Purgatory的设计请求purgatory包括一个超时计时器和事件驱动处理的观察者列表哈希映射。 如果一个请求的条件得不到满足而不能马上满足，就需要把它放入Purgatory中。 当条件满足时，Purgatory中的请求会被完成，或者当超过请求的超时参数指定的时间时被强制完成（超时）。 在旧的设计中，它使用Java DelayQueue来实现定时器。 当请求完成时，请求不会立即从定时器或观察者列表中删除。 相反，完成的请求会在条件检查期间被删除。 当删除不跟上时，服务器可能会耗尽JVM堆并导致OutOfMemoryError。 为了缓解这种情况，一个单独的线程（称为收割者线程）在Purgatory中的请求数量（挂起或已完成）超过配置的数量时，清除Purgatory中完成的请求。 清除操作扫描定时器队列和所有观察者列表以找到完成的请求并删除它们。 通过将此配置参数设置为较低值，服务器可以表面上避免内存问题。 但是，如果服务器太频繁地扫描所有列表，则会付出比较大的性能损失。 新Purgatory的设计新设计的目标是允许立即删除已完成的请求，并显着减少昂贵的清除过程的负担。 它需要交叉引用定时器和请求中的条目。 此外，强烈希望具有O(1)插入/删除成本，因为每个请求/完成都会发生插入/删除操作。 为了满足这些要求，我们设计了一个基于分级时间轮的新的Purgatory实现[1]。 分级时间轮大小为n的定时轮具有n个时段，并且可以在n u个时间间隔内保持定时器任务。每个桶包含落在相应时间范围内的定时器任务。首先，第一个桶保存[0，u]的任务，第二个桶保存[u （n-1），u n）中的[u，2u），…，第n个桶的任务。每一个时间间隔单位u，计时器滴答并移动到下一个桶，然后终止所有计时器任务。所以，定时器从不在当前时间插入任务，因为它已经过期了。计时器立即运行过期的任务。因此，如果当前时间段为时间t，则在空闲时间之后，空的时间段将成为[t + u n，t +（n + 1）* u）的时间段。定时轮具有O（1）插入/删除（启动定时器/停止定时器）的开销，而基于优先级队列的定时器（例如java.util.concurrent.DelayQueue和java.util.Timer）具有O（log n）插入/删除成本。请注意，DelayQueue或Timer都不支持随机删除。 一个简单的时间轮的一个主要缺点是它假定一个定时器请求在距当前时间n u的时间间隔内。如果一个定时器请求超出这个时间间隔，这是一个溢出。分层的时间轮处理这种溢出。这是一个分层组织的时间轮代表溢出到上层轮子。最底层有最好的时间分辨率。时间分辨率越来越粗糙，如果某一级的轮子的分辨率为u，大小为n，则分辨率应该是第二级的n u，第三级的n2 * u，依此类推。在每个级别，溢出都被委托给高一级的车轮。当较高级别的轮子时间到达时，它将计时器任务重新插入较低级别。高一级的时间轮可以按需创建。当溢出存储桶中的存储桶到期时，其中的所有任务将被递归地重新插入定时器。然后任务被移动到更精细的轮子或被执行。插入（启动定时器）的开销是O（m），其中m是车轮的数量，通常与系统中的请求数量相比非常小，并且删除（停止计时）的开销仍然是O（1 ）。 双向链轮列表中的时间轮桶在新的设计中，我们使用自己的双向链表来实现时序轮中的桶。 双向链表的优点是它允许O（1）插入/删除一个列表项，如果我们有访问链表单元的话。 计时器任务实例在排队到计时器队列时将链接单元保存在自身中。 任务完成或取消时，使用保存在任务本身中的链接单元更新列表。 使用DelayQueue驱动时钟一个简单的实现可以使用一个线程，唤醒每个单位时间，并做滴答，检查是否有任何任务在桶中。 Purgatory的单位时间是1ms（u = 1ms）。 如果最低级别的请求稀疏，这可能是浪费的。 通常情况下是这样的，因为大多数请求在插入最低级别的车轮之前是满足的。 如果一个线程只有在非空的存储桶过期才会唤醒，那将会很好。 新的Purgatory通过使用java.util.concurrent.DelayQueue类似于旧的实现，但是我们排队任务桶而不是单独的任务。 这种设计具有性能优势。 DelayQueue中的项目数量以桶的数量为上限，通常远小于任务数量，因此DelayQueue内的优先级队列的offer/poll操作的数量将显着减少。 清除watch列表在旧的实现中，观察者列表的清除操作由总大小触发。问题是，即使没有太多请求清除，观察者列表也可能会超出阈值。发生这种情况时，会增加很多CPU负载。理想情况下，清除操作应该由观察者列出的已完成请求的数量触发。 在新设计中，已经完成的请求立即以O（1）成本从定时器队列中移除。这意味着任何时候定时器队列中的请求数量是待处理请求数量。因此，如果我们知道Purgatory中不同请求的总数，包括未决请求数量和已完成但仍然监视的数量的总和，我们可以避免不必要的清除操作。跟踪Purgatory中不同请求的确切数量是不太现实的，因为一个请求可能被监视，也可能不被监视，状态可能只在一瞬间变换。在新设计中，我们只粗略预估Purgatory中的请求总数，而不是试图维持正确的数量统计。 估计的请求数量按以下保持： 估计的请求总数E会随着新的请求被监视而增加。 在开始清除操作之前，我们将估计的总请求数重置为定时器队列的大小。这是当前的待处理请求的数量。如果在清除期间没有任何请求被添加到Purgatory，则E是清除后正确的请求数量。 如果清除过程中某些请求被添加到Purgatory，则E增加到E+新观察请求的数量。这可能被高估，因为有可能在清除操作期间完成一些新的请求并从观察者列表中删除。我们预计高估和高估的可能性很小。 基准测试我们比较了两个Purgatory实施的入队表现，旧的实施和新的实施。这是一个微观基准。它只是衡量Purgatory入队的表现。Purgatory与系统的其他部分分离，并使用一个没有用处的测试要求。因此，真实系统中Purgatory的吞吐量可能会低于测试所显示的数量。 在测试中，请求的间隔假定遵循指数分布。每个请求都需要从对数正态分布中抽取一段时间。通过调整对数正态分布的形状，我们可以测试不同的超时率。 刻度大小为1ms，轮子大小为20.超时设置为200ms。请求的数据大小是100字节。对于较低的超时率情况，我们选择75%均线 = 60ms和50%均线 = 20。对于高超时率情况，我们选择75%均线 = 400ms和50%均线 = 200ms。总共有100万个请求在每次运行中排队。 请求由一个单独的线程主动完成。应该在超时之前完成的请求被排队到另一个DelayQueue。而一个单独的线程保持轮询并完成它们。实际完成时间无法保证准确性。 JVM堆大小设置为200M来重现内存紧张的情况。 结果表明，高排队率区域有显着差异。随着目标机率的提高，两种实施方式都能满足要求。然而，在低超时的情况下，旧的实现极限大约40000 RPS（请求每秒），而新的实现并没有显示任何显着的性能下降，在高超时的情况下，旧的实现极限大约25000 RPS，而新的实现在这个基准测试中达到了105000 RPS。 另外，在新的实现中CPU的使用情况要好得多。 请注意，由于可伸缩性的限制，旧的实现没有高于〜40000 RPS的数据点。 同时也注意到它的CPU时间在1.2左右饱和，而在新的实现中稳步上升。 这表明旧的实现可能由于同步而遇到并发问题。 最后，我们测量了ParNew收集和CMS收集的总GC时间（毫秒）。 旧的设计和新的设计在维持的入队率没有太大差别。 再次注意，由于可伸缩性限制，旧的实现没有高于〜40000 RPS的数据点。 概要总结在新设计中，我们使用多级时间轮作为定时器桶的超时定时器和DelayQueue按需提前时钟。 O（1）成本立即从计时器队列中删除已完成的请求。 桶仍然在延迟队列中，但桶的数量是有限的。 而且，在一个健康的系统中，大多数请求应该在超时之前完成，并且在离开延迟队列之前许多桶变空了。 因此，计时器应该很少有较低间隔的桶。 这种设计的优点是，定时器队列中的请求数量是任何时候的待处理请求数量。 这使我们能够估计需要清除的请求数量。 我们可以避免观察者列表的不必要的清除操作。 因此，我们在请求速率方面实现了更高的可扩展性，CPU使用率更高。 引用[1] George Varghese , Anthony Lauck, Hashed and hierarchical timing wheels: efficient data structures for implementing a timer facility, IEEE/ACM Transactions on Networking (TON), v.5 n.6, p.824-834, Dec. 1997","categories":[{"name":"翻译文章","slug":"翻译文章","permalink":"http://www.lpnote.com/categories/翻译文章/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://www.lpnote.com/tags/kafka/"},{"name":"timewheels","slug":"timewheels","permalink":"http://www.lpnote.com/tags/timewheels/"},{"name":"timer","slug":"timer","permalink":"http://www.lpnote.com/tags/timer/"}]},{"title":"基于Hash和多级时间轮：实现定时器的高效数据结构","slug":"hashed-and-hierarchical-timing-wheels","date":"2017-11-15T16:00:00.000Z","updated":"2019-02-13T08:59:06.528Z","comments":true,"path":"2017/11/15/hashed-and-hierarchical-timing-wheels/","link":"","permalink":"http://www.lpnote.com/2017/11/15/hashed-and-hierarchical-timing-wheels/","excerpt":"原文地址：Hashed and Hierarchical Timing Wheels: Data Structures for the Efficient Implementation of a Timer Facility Yashiro Matsuda最近写了一篇博文Apache Kafka’s use of Hierarchical Timing Wheels 用于监控大量的延时操作。 在Kafka用例中，每个请求都处于“Purgatory”数据结构中，并且与事件驱动处理的超时计时器和观察者列表图相关联。 有效跟踪到期定时器是一个常见问题。 这个原则可以适用于任何跟踪未完成的请求或延时消息系统。 今天的选择是Varghese和Lauck在1987年发表的一篇论文，他们在这篇论文中研究了一些有效管理定时器的方法，并介绍了Kafka所使用的分层定时轮的概念。 他们将定时器建模为两个面向用户的操作，即启动和停止，以及两个内部操作：每个滴答步进和过期处理。 启动计时器由客户端调用，指定一个计时器持续时间和一个回调。在作者的模型中，客户端还传入一个请求ID来区分计时器，但是现在我们更倾向于返回一个计时器ID来响应启动计时器的请求。 停止定时器接收一个请求（定时器）ID，并找到并停止（删除）相关的定时器。 在计时器时钟的每个“滴答声”上都会发生清算。如果设置定时器的粒度单位是T个单位时间（例如1秒），则每T个单位时间将发生一个清算。它检查是否有任何未完成的定时器已经过期，如果是则删除它们并调用过期处理。 到期处理负责调用用户提供的回调（或其他用户请求的操作，具体取决于您的模型）。 不同的数据结构和算法在执行这些操作的成本方面有不同的复杂性（例如，启动一个定时器是一个恒定的时间操作，取决于现有定时器的数量，或者甚至是一些其他变量？）。 我们有七种不同的计时器管理方案，指导方针是“对于一个普通的定时器模块，这个模块预计在各种环境下都能正常工作，我们推荐方案6或7”。方案6是“散列定时轮”和方案7是“分层定时轮”。","text":"原文地址：Hashed and Hierarchical Timing Wheels: Data Structures for the Efficient Implementation of a Timer Facility Yashiro Matsuda最近写了一篇博文Apache Kafka’s use of Hierarchical Timing Wheels 用于监控大量的延时操作。 在Kafka用例中，每个请求都处于“Purgatory”数据结构中，并且与事件驱动处理的超时计时器和观察者列表图相关联。 有效跟踪到期定时器是一个常见问题。 这个原则可以适用于任何跟踪未完成的请求或延时消息系统。 今天的选择是Varghese和Lauck在1987年发表的一篇论文，他们在这篇论文中研究了一些有效管理定时器的方法，并介绍了Kafka所使用的分层定时轮的概念。 他们将定时器建模为两个面向用户的操作，即启动和停止，以及两个内部操作：每个滴答步进和过期处理。 启动计时器由客户端调用，指定一个计时器持续时间和一个回调。在作者的模型中，客户端还传入一个请求ID来区分计时器，但是现在我们更倾向于返回一个计时器ID来响应启动计时器的请求。 停止定时器接收一个请求（定时器）ID，并找到并停止（删除）相关的定时器。 在计时器时钟的每个“滴答声”上都会发生清算。如果设置定时器的粒度单位是T个单位时间（例如1秒），则每T个单位时间将发生一个清算。它检查是否有任何未完成的定时器已经过期，如果是则删除它们并调用过期处理。 到期处理负责调用用户提供的回调（或其他用户请求的操作，具体取决于您的模型）。 不同的数据结构和算法在执行这些操作的成本方面有不同的复杂性（例如，启动一个定时器是一个恒定的时间操作，取决于现有定时器的数量，或者甚至是一些其他变量？）。 我们有七种不同的计时器管理方案，指导方针是“对于一个普通的定时器模块，这个模块预计在各种环境下都能正常工作，我们推荐方案6或7”。方案6是“散列定时轮”和方案7是“分层定时轮”。 让我们来看看这些方案： 1.无序列表的定时器保留一个无序的列表定时器，并跟踪每个定时器的剩余时间。开始时，只需将新的计时器添加到列表中。每个“嘀嗒”周期必须遍历完整列表，并在每笔记帐中减少每个计时器的剩余时间。如果一个定时器到达零，它将从列表中删除，并调用过期处理。因此启动一个定时器是O（1），停止一个定时器是O（1），并且每个滴答处理是O（n），其中n是未完成定时器的数量。 2.有序列表计时器保留方案1中的列表定时器，但记录绝对到期时间（不是剩余时间），并保持定时器列表的排序时间（定时器最接近于列表头部的到期时间）。在每个时钟周期比较列表头部的定时器的到期时间和当前的时钟，并且如果定时器的到期时间是小于当前时间，则删除到期定时器; 继续这样做这样的比较，直到列表的头部包含一个过期时间大于当前时间的计时器。由于在列表中搜索正确的位置来插入它，所以现在启动一个计时器为O（n），但是每个嘀嗒处理是O（1）。 3.定时器树对于比较大的n，我们可以通过在基于tree的数据结构中保留定时器来改进方案2。 这意味着我们可以在O(log(n))内为有序列表插入（启动）定时器。 4.简单的时间轮当所有定时器的最大周期不超过MaxInterval时，简单的定时轮的方法是适用的，我们可以用MaxInterval槽（每个代表一个滴答）构造一个循环缓冲区。当前时间由缓冲区中的索引表示。插入一个计时器，过期时间j(小于MaxInterval）; 在未来的MaxInterval时间单位中，我们移动环上的j个时隙，并将定时器添加到该时隙中的定时器列表中。 每次“嘀嗒”（模拟时钟，非常形象的描述了时间轮的走动），当前时间索引移动环中的一个槽，并在新槽中的所有定时器上执行到期处理。 开始，停止和每个“嘀嗒”操作都是O(1)。 5.带有序定时器列表的散列轮如果MaxInterval比较大（例如32位定时器），简单的定时轮就可能会使用大量的内存。 我们可以使用散列的形式而不是每时间单位使用一个插槽。 构建一个具有固定数量的槽的循环缓冲区(2的指数会比较有效率)，并且当前时间索引像以前一样在环上前进一个位置。 要插入将来会过期j个时间单位的计时器，计算一个增量时隙 s = j％num-buckets。 将定时器插入环中，并等待其到期。 由于在任何给定的时隙中可能有多个定时器，因此我们为每个时隙维护一个有序的定时器列表。每次处理时移动当前时间索引并处理在方案2中找到的定时器列表。插入定时器的最坏情况延迟是O（n），但是平均值是O（1）。 每次处理“嘀嗒”是O（1）。 6.无序定时器列表的哈希轮这是方案5中的一个变体，其中不是存储绝对的到期时间，而是存储每个计时器将来在遍历环的次数。 为了插入一个计时器，将来会计算一个时间单位，计算一个计数器值c = j / num-buckets和一个时隙delta s = j％num-bucket。 用计数器值c将定时器的槽插入环中。保持定时器在每个槽中的无序列表中。 现在启动一个计时器是O（1），而每个滴答簿记是最坏的情况O（n），但是O（1）是平均的。 7.分级时间轮处理由简单的定时轮方法引起的存储器问题的另一种方式是在层次结构中使用多个定时轮。假设我们要存储第二个粒度的定时器，将来可以设置长达100天。我们可以建造四个轮子： 一个“天”轮有100个插槽 一个“小时”轮有24插槽 一个“分钟”轮有60个插槽 一个“秒钟”轮有60个插槽 这总共有244个插槽，总计864万个可能的计时器值。每当我们在一个轮子上完成一次完整的转动，我们就把下一个较大的轮子向前推进一个槽位（本文用分钟，小时和星期计时钟来描述一个微小的变化，但效果是一样的）。例如，当秒轮转回到索引“0”时，我们将分针轮中的索引指针移动一个位置。然后，我们把时间轮上的所有定时器（将在接下来的60秒内到期），并将它们插入到秒针轮中正确的位置。秒轮中的过期时间处理完全按照方案4中所述的方式工作（这只是一个简单的计时轮，恰好在每次旋转时得到补充）。 要插入一个计时器，找到计时器应该到期的一个或多个车轮单元的第一个车轮（从最大单位到最小）。例如，一个计时器将会在未来11小时15分15秒的时间内插入小时轮的current-index + 11时隙，用计时器存储剩余的15分15秒。在小时轮前进11个位置后，该计时器将从该轮上移除，并在分针轮中的当前索引+ 15个插槽中插入，存储剩余的15秒。当分钟轮随后前进15个位置时，该计时器将从轮中移出，并放置在秒针轮中的“当前索引+15”轮槽中。 15秒后，计时器将过期！ 插入为O（n），而每个滴答簿记是最坏的情况O（n），但是O（1）是平均的。 注意：本文使用秒，分，小时，天的例子，这当然使得它很容易遵循及更容易理解和记忆，但如果你只是给定时器，例如，在未来的t秒内达到32位计时器值，那么简单地将其分成四个轮子，每个轮子有28个槽或类似的轮子（这使得确定进入哪个轮子是非常有效的）。 在方案6和7之间选择 在任何给定的情况下，方案6或7是否更好取决于许多参数： n，定时器的数量 M，可用插槽的总数 m，级别的数量（用于分级方法） T，平均时间间隔 根据方案6计算一个条目的散列和索引成本在方案7（将计时器条目移动到下一个轮子的成本）之下。 对于方案6，成本大约是n’s indexcost / M，方案7是nm’s migratecost / T。 由于costindex和costmigrate不会有很大的不同，对于较小的T值和较大的M值，方案6对于START-TIMER和PER-TICK-BOOKKEEPING都可能比方案7更好。然而，对于大的T值和小的M值，方案7对于PER-TICK-BOOKKEEPING将具有更好的平均成本（等待时间），但对于START-TIMER来说成本更高。","categories":[{"name":"翻译文章","slug":"翻译文章","permalink":"http://www.lpnote.com/categories/翻译文章/"}],"tags":[{"name":"timer","slug":"timer","permalink":"http://www.lpnote.com/tags/timer/"},{"name":"timewheel","slug":"timewheel","permalink":"http://www.lpnote.com/tags/timewheel/"}]},{"title":"Nginx_Session_Sticky踩坑记录","slug":"problem-analysis-of-nginx-session-sticky","date":"2017-09-11T16:00:00.000Z","updated":"2019-02-13T08:59:06.536Z","comments":true,"path":"2017/09/11/problem-analysis-of-nginx-session-sticky/","link":"","permalink":"http://www.lpnote.com/2017/09/11/problem-analysis-of-nginx-session-sticky/","excerpt":"常见Session方案一个多用户的WEB系统一定离不开多用户的登录和会话保持的问题，用户登录可以通过SSO单点登录解决，但是用户的SESSION会话保持是需要一个基础设施来支撑的。对于传统的单机部署的WEB应用，SESSION会话由本机的应用服务器（tomcat/jetty/jboss）负责应用SESSION会话的保持。但是对于分布式部署的WEB应用来说，单机的会话保持显然并不能适用在这种场景下面，下面是分布式WEB应用场景时一般采取的策略： 方案一：通过前端负载均衡进行SESSION_STICKY 方案二：应用服务器层SESSION同步 方案三：应用SESSION层统一管理","text":"常见Session方案一个多用户的WEB系统一定离不开多用户的登录和会话保持的问题，用户登录可以通过SSO单点登录解决，但是用户的SESSION会话保持是需要一个基础设施来支撑的。对于传统的单机部署的WEB应用，SESSION会话由本机的应用服务器（tomcat/jetty/jboss）负责应用SESSION会话的保持。但是对于分布式部署的WEB应用来说，单机的会话保持显然并不能适用在这种场景下面，下面是分布式WEB应用场景时一般采取的策略： 方案一：通过前端负载均衡进行SESSION_STICKY 方案二：应用服务器层SESSION同步 方案三：应用SESSION层统一管理 1、对于方案一，操作简单，应用不需要进行任何设置，同一用户首次访问应用提供服务的某台服务器后，后续的访问请求会一直发往该台服务器进行处理，这里解决的就是SESSION本机存储的问题，如果采用的是SESSION本地存储，然后请求又是在多台机器之间分发，那么会造成用户不断的登录和退出，无法正常使用应用。这种方案一般在负载均衡服务器(Nginx/Apache等）上进行配置即可。 2、对于方案二，操作方式稍微比方案一要复杂，但是仍然向使用者屏蔽了使用细节。对于该方案要求后端的真实应用服务器之间要建立同步通道进行SESSION会话数据的同步，这种方案在后端真实服务器相对较少时可以采用没有太多问题，但是一旦服务器数量以及用户数量并发超过一定数量会造成网络风暴的问题。试想，一个用户的SESSION在某台服务器进行修改后要同步到所有其它服务器的SESSION管理存储上，这是一个1+N的过程，性能和网络都会无法承担这样的开销。 3、对于方案三，这是目前大型互联网公司采用的方案,如淘宝，同样对使用用户屏蔽实现细节，用户使用过程中就好像使用原生的SESSION一样。方案三采用集中式SESSION存放，应用服务器并不负责用户SESSION的管理，SESSION管理交由统一封装的SESSION框架层负责处理，SESSION框架层拦截应用服务器的SESSION存取并与SESSION存储交互获取和设置数据，这里的SESESSION的存储又分为多种方式，常见的有：缓存服务，Cookie等。一些不重要的，非关键性的用户数据可以通过SESSION框架存入Cookie中，而重要的用户数据存入远程的缓存服务中。 问题现象我们这里有一个应用，线上会部署多台服务器，当时为了方便快速上线就采用了上面方案一的方式，在线上Nginx上配置了session_sticky，然而这正是问题的始源： 通过我们自己的APM监控系统发现在最初后端的两台服务器正常的各自分担了50%的网络流量，但是在后面的一段时间里流量会慢慢的向基中一台机器聚集，而另一台机器流量几乎降低到微乎其微。这个问题困扰了好几天，前面几天一直发现了该问题，但是一直忙于处理其它事务，今天终于有时间慢慢来分析这方面的问题。 问题排查1、首先查看的Nginx的配置是否正确 采用了SESSION_STICKY，并且两台服务器间采用一样的权重比率。没问题。 2、排查用户访问IP的问题 最开始一直认为Nginx的SESSION_STICKY是通过用户的IP进行的分流（其实后面证明我的想法是错的），所以想到的是查找用户访问的IP，通过询问运维，得到的结论是用户都是通过内网统一一个IP访问，这里有一个误导，导致我认为这就是导致该问题的原因。如果真是按IP对用户进行分流切分的话，那如何解释之前可以平均分配流量的问题呢？我一直在不停的反问我自己。 3、在多个不同用户的机器上重现问题 我使用了多个同事的电脑进行操作以及查看资料，发现Nginx的SESSION_STICKY是通过Nginx反写cookie实现的，通过查看多个同事的浏览器cookie，我发现了这个cookie：route=739d4e2d09f01c606bc43936e6e743e3; 基本上是所有的同事浏览器cookie都是一样的，这也应验了为什么基本上的流量都会往一台机器上发送了。 问题分析既然有了上面的问题排查，那么最重要的一个问题就落在了为什么不同的用户会产生同样的cookie呢，我试着将我自己的电脑上的cookie清空，然后再重新登录，再查看该cookie值。重复这样几次后，我发现均衡正常了，可以按一定的比率会话分别粘滞在两台机器上。同时我也仔细翻看了Nginx的SESSION_STICKY说明，其中有一条也让我恍然大悟：Nginx SESSION_STICKY产生的cookie是根据配置按后端可用的upstream服务器中的一台的IP通过MD5加密（或明文，可配置）后得到的一串数字，而并非是由前端的IP决定。 NGINX SESSION_STICKY 原理： 这里导致上面的问题的原因慢慢的开始浮出水面： 1、为什么Cookie是一直不变的？ 原因是大部分的同事都是使用笔记本，特别是大部分人都是MacPro控，所以对于他们来说，工作或者下班时是重来不需要关闭电脑的，电脑一合就走人，所以浏览器是一直打开的未关闭过，这种情况也在部分使用台式机的同事存在，也是下班电脑睡眠就走人，并未关闭浏览器。对于route这个cookie是浏览器关闭才会失效，所以一直开启的浏览器时该cookie会一直有效。 2、为什么基本上的同事的Cookie都是一样的？ 这个问题就要从应用的发布说起了。我们的发布流程是灰度发布过程，在应用发布时是一台一台的发布的，总共两台机器，其中第一台的发布的过程中大量的请求被定向到另一台机器，而第一台发布完成时流量并不会切换回来，因为新产生的cookie已经是第二台机器的cookie，并且该cookie是一直有效的，除非有人为的手动关闭整个浏览器。这就解释了为什么应用在第一次上线时是流量均衡的，但是一旦后面上线过后流量变成只向其中一台聚集的情况，这也是使用Nginx Session_Sticky的一个问题。 问题解决问题解决方案其实已经在上面第一段内容提及了，实现会话保持的三个方案中，选择其中一个，对于我们之前使用的第一种方案，其实有相应的缓解方式，就是增开几台机器，让流量在发布的时候也分布到其它机器，只是在我们的场景下，只有两台机器，当发布进行时，所有的流量都汇聚到其中一台上，以后也固化到这台访问了。如果将机器扩充多一些，那么在一台机器发布时，流量会分担到其它机器，整个集群相对来说还是比较均衡的（其中只有一台没啥流量，相当于是浪费一台机器），能保证整体流量在N-1台机器上均衡（N为应用机器总量）。 我们解决的方式是使用方案三，使用外置的Session会话存储的方案。这里我们可以自己写一套Session管理的框架，但是介于开源世界已经有实现方案了，比如Spring的session方案。于是我们的方案就是基于此来做改造。我们使用spring session框架，基于redis集群的会话保持方案改造了自己的应用，非常简单快速的实现了session的外置存储支持，以及应用的无状态化。基于此，nginx的session_sticky配置也可以去掉了，应用也可以很好的扩容了。","categories":[{"name":"原创文章","slug":"原创文章","permalink":"http://www.lpnote.com/categories/原创文章/"}],"tags":[{"name":"nginx","slug":"nginx","permalink":"http://www.lpnote.com/tags/nginx/"},{"name":"踩坑","slug":"踩坑","permalink":"http://www.lpnote.com/tags/踩坑/"},{"name":"session","slug":"session","permalink":"http://www.lpnote.com/tags/session/"}]},{"title":"leetcode算法3_最长无重复子串长度","slug":"leetcode-3-longest-substring-without-repeating-characters","date":"2017-09-07T16:00:00.000Z","updated":"2019-02-13T08:59:06.532Z","comments":true,"path":"2017/09/07/leetcode-3-longest-substring-without-repeating-characters/","link":"","permalink":"http://www.lpnote.com/2017/09/07/leetcode-3-longest-substring-without-repeating-characters/","excerpt":"题目 给定一个字符串，找到最长子串的长度，而不重复字符。 例子给定“abcabcbb”，答案是“abc”，长度为3。 给定“bbbbb”，答案是“b”，长度为1。 给定“pwwkew”，答案是“wke”，长度为3.请注意，答案必须是子字符串，“pwke”是子序列而不是子字符串。","text":"题目 给定一个字符串，找到最长子串的长度，而不重复字符。 例子给定“abcabcbb”，答案是“abc”，长度为3。 给定“bbbbb”，答案是“b”，长度为1。 给定“pwwkew”，答案是“wke”，长度为3.请注意，答案必须是子字符串，“pwke”是子序列而不是子字符串。 思路利用滑动窗口思想，滑动窗口内的字符将不会重复，滑动窗口利用两个索引i,j分别指向窗口的前后界限，通过分别移动i，j指针来寻求最大子串长度。 方案方法＃1 暴力方式[超时] 思路 检查所有子字符串逐个查看是否没有重复的字符。 算法 假设我们有一个函数boolean allUnique（String substring），如果子字符串中的字符都是唯一的，否则返回true，否则为false。我们可以遍历给定字符串s的所有可能的子字符串，并调用allUnique函数。如果事实证明是正确的，那么我们更新我们的子字符串的最大长度的答案，而不会有重复的字符。 现在我们填写缺失的部分： 要枚举给定字符串的所有子字符串，我们枚举它们的开始和结束索引。假设开始和结束指数分别为i和j。那么我们有0 &lt;= i &lt; j &lt;=n（这里的结束索引j按照惯例排除）。因此，使用从0到n-1的i的两个嵌套循环和从i + 1到n的j，我们可以枚举s的所有子串。 要检查一个字符串是否有重复的字符，我们可以使用一个字符串。我们遍历字符串中的所有字符，并将它们逐个放入。在放置一个字符之前，我们检查该集合是否已经包含它。如果是这样，我们返回false。循环后，我们返回true。 Javaw代码 1234567891011121314151617181920public class Solution&#123; public int lengthOfLongestSubstring（String s）&#123; int n = s.length（）; int ans = 0; for（int i = 0; i &lt;n; i ++） for（int j = i + 1; j &lt;= n; j ++） if（allUnique（s，i，j））ans = Math.max（ans，j-i）; return ans; &#125; public boolean allUnique（String s，int start，int end）&#123; set &lt;Character&gt; set = new HashSet &lt;&gt;（）; for（int i = start; i &lt;end; i ++）&#123; Character ch = s.charAt（i）; if（set.contains（ch））return false; set.add（ch）; &#125; return true; &#125;&#125; 复杂性分析 时间复杂度：O（n ^ 3） 空间复杂度：O（min（n，m））O（min（n，m））。我们需要O（k）空格用于检查子串没有重复字符，其中k是Set的大小。集合的大小由字符串n的大小和字符集/字母表m的大小限定。 方法＃2滑动窗口[已接受] 算法 上面算法一方法非常简单。但是太慢了那么我们如何才能优化呢？ 在上面的方法中，我们反复检查一个子字符串，看看它是否具有重复的字符。但这是没有必要的。如果一个子字符串s[i,j）是从索引i到j-1已经被检查为没有重复的字符。我们只需要检查s[j]是否已经在子串s[i,j)中。要检查字符是否已经在子字符串中，我们可以扫描子字符串，导致O（n ^ 2）算法。但我们可以做得更好。 通过使用HashSet作为滑动窗口，检查当前的字符是否可以在O（1）中完成。 滑动窗口是数组/字符串问题中常用的抽象概念。窗口是数组/字符串中通常由开始和结束索引定义的元素范围，即[i，j)（左闭合，右开）。滑动窗口是一个窗口，将其两个边界滑动到某个方向。例如，如果我们通过一个元素将[i，j）向右滑动，则它变为[i + 1，j + 1）（左闭右开）。 回到我们的问题。我们使用HashSet将字符存储在当前窗口[i，j）（j = i）。然后我们将索引j向右滑动。如果不在HashSet中，我们会进一步滑动j。这样做直到s[j]已经在HashSet中。在这一点上，我们发现没有重复字符的子字符串的最大大小从索引i开始。重复上面的步骤，我们就能得到我们的答案。 Java代码 123456789101112131415161718public class Solution &#123; public int lengthOfLongestSubstring(String s) &#123; int n = s.length(); Set&lt;Character&gt; set = new HashSet&lt;&gt;(); int ans = 0, i = 0, j = 0; while (i &lt; n &amp;&amp; j &lt; n) &#123; // try to extend the range [i, j] if (!set.contains(s.charAt(j)))&#123; set.add(s.charAt(j++)); ans = Math.max(ans, j - i); &#125; else &#123; set.remove(s.charAt(i++)); &#125; &#125; return ans; &#125;&#125; 复杂性分析 时间复杂度： O(2n) = O(n)。最糟糕的情况是每个字符都需要被i,j指针访问两次。空间复杂度： O(min(m,n))。 和上面的方案一样，我们同样需要一个O(k)的空间用于滑动窗口，k表示滑动窗口大小。这个大小取决于字符串n的大小以及字符集的大小m 方法＃3滑动窗优化[已接受]上述方法2中解决方案最多需要2n步。 事实上，它可以被优化，只需要n个步骤。 我们可以定义一个字符与其索引的映射，而不是使用一个字符来判断一个字符是否存在。 然后，当我们发现重复的字符时，我们可以立即跳过这些字符。 原因是如果s[j]在索引j的范围[i，j）中具有重复，重复的这个索引为j’，我们不需要一点一点地增加i。 我们可以跳过[i，j’]范围内的所有元素,直接令i=j’+ 1。 Java代码 123456789101112131415public class Solution &#123; public int lengthOfLongestSubstring(String s) &#123; int n = s.length(), ans = 0; Map&lt;Character, Integer&gt; map = new HashMap&lt;&gt;(); // current index of character // try to extend the range [i, j] for (int j = 0, i = 0; j &lt; n; j++) &#123; if (map.containsKey(s.charAt(j))) &#123; i = Math.max(map.get(s.charAt(j)), i); &#125; ans = Math.max(ans, j - i + 1); map.put(s.charAt(j), j + 1); &#125; return ans; &#125;&#125; 更优代码 假设字符集为ASCII 128 以前的实现都没有对字符串的字符集的假设。 如果我们知道字符集相当小，我们可以将整数数组替换为直接访问表。 常用的表格有： int[26] 用于’a’-‘z’以及’A’-‘Z’ int[128] 用于ASCII集 int[256] 用于ASCII扩展集 12345678910111213public class Solution &#123; public int lengthOfLongestSubstring(String s) &#123; int n = s.length(), ans = 0; int[] index = new int[128]; // current index of character // try to extend the range [i, j] for (int j = 0, i = 0; j &lt; n; j++) &#123; i = Math.max(index[s.charAt(j)], i); ans = Math.max(ans, j - i + 1); index[s.charAt(j)] = j + 1; &#125; return ans; &#125;&#125; 复杂度分析 时间复杂度：O(n)Hashmap空间复杂度：O(min(m,n))Table方式空间复杂度：O(m) m表示字符集的大小 原文地址：Longest Substring Without Repeating Characters","categories":[{"name":"leetcode","slug":"leetcode","permalink":"http://www.lpnote.com/categories/leetcode/"}],"tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://www.lpnote.com/tags/leetcode/"},{"name":"算法","slug":"算法","permalink":"http://www.lpnote.com/tags/算法/"}]},{"title":"leetcode算法4_两排序数组求中位平均数","slug":"leetcode-4-median-of-two-sorted-arrays","date":"2017-09-07T16:00:00.000Z","updated":"2019-02-13T08:59:06.532Z","comments":true,"path":"2017/09/07/leetcode-4-median-of-two-sorted-arrays/","link":"","permalink":"http://www.lpnote.com/2017/09/07/leetcode-4-median-of-two-sorted-arrays/","excerpt":"题目有两个排序的数组nums1和nums2分别为m和n大小。 找到两个排序数组的中位数。 整体运行时间复杂度应为O（log（m + n））。 示例1：nums1 = [1,3]nums2 = [2] 中位数为2.0示例2：nums1 = [1,2]nums2 = [3,4] 中位数为（2 + 3）/ 2 = 2.5","text":"题目有两个排序的数组nums1和nums2分别为m和n大小。 找到两个排序数组的中位数。 整体运行时间复杂度应为O（log（m + n））。 示例1：nums1 = [1,3]nums2 = [2] 中位数为2.0示例2：nums1 = [1,2]nums2 = [3,4] 中位数为（2 + 3）/ 2 = 2.5 分析本题中的中位数是指对于一个长度为n的数组，如果n为偶数，则中位数为下标为n/2和n/2+1的两数相加取平均值；如果n为奇数，则中位数为下标为n/2+1的数。 对于两个数组，我们知道合并成一个数组后，同样适用上面的方式。所以我们可以根据上面的描述确定对于两个数组遍历得到的中位数下标，同时我们定义一个计数器，以确定在按序遍历时的计数，当计数与我们确定的中位数下标达到一致状态时，即可确定我们的中位数。 本题中主要关注已排序这个前提条件，我们可以使用两个指针分别指向两个数组的头部（数组下标为0的位置），两个指针对应的数进行比较得出较小值，将计数器与预计算的中位数的下标进行对比，判断是否达到中位数下标，最后累加计数器。 算法代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152class Solution &#123; public double findMedianSortedArrays(int[] nums1, int[] nums2) &#123; int idx1 = 0,idx2 = 0; int idx1Max = nums1.length; int idx2Max = nums2.length; int total = idx1Max + idx2Max; int[] middles = (total % 2) == 0 ? (new int[]&#123;total / 2,total / 2 + 1&#125; ) : (new int[]&#123;total / 2 + 1&#125;); int counter = 1; int num = 0; int median = 0; while(idx1 &lt; idx1Max || idx2 &lt; idx2Max)&#123; if(idx1 &lt; idx1Max &amp;&amp; idx2 &lt; idx2Max)&#123; if(nums1[idx1] &gt; nums2[idx2])&#123; num = nums2[idx2]; idx2++; &#125;else&#123; num = nums1[idx1]; idx1++; &#125; &#125;else if(idx1 &gt;= idx1Max &amp;&amp; idx2 &lt; idx2Max)&#123; num = nums2[idx2]; idx2++; &#125;else if(idx1 &lt; idx1Max &amp;&amp; idx2 &gt;= idx2Max)&#123; num = nums1[idx1]; idx1++; &#125;else&#123; throw new RuntimeException(\"can't reach here\"); &#125; if(middles.length == 1 &amp;&amp; counter == middles[0])&#123; return (double) num; &#125;else if(middles.length == 2)&#123; if(counter == middles[0])&#123; median += num; &#125;else if(counter == middles[1])&#123; median += num; return (double) median / 2; &#125; &#125; counter++; &#125; return 0; &#125;&#125; 复杂度分析空间复杂度：O（1），没有使用额外的空间用于计算，只有一些变量值，空间忽略。时间复杂度：O(log(m+n))，因在一般情况下对于两个数组基本确定在遍历到一半的情况下都能找到结果，故在m+n两数组总长度与计算耗时上存在2的倍数关系，故为O(log(m+n))。 原文地址：4. Median of Two Sorted Arrays","categories":[{"name":"leetcode","slug":"leetcode","permalink":"http://www.lpnote.com/categories/leetcode/"}],"tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://www.lpnote.com/tags/leetcode/"},{"name":"算法","slug":"算法","permalink":"http://www.lpnote.com/tags/算法/"}]},{"title":"leetcode算法题2_两数相加","slug":"leetcode-2-add-two-numbers","date":"2017-09-06T16:00:00.000Z","updated":"2019-02-13T08:59:06.532Z","comments":true,"path":"2017/09/06/leetcode-2-add-two-numbers/","link":"","permalink":"http://www.lpnote.com/2017/09/06/leetcode-2-add-two-numbers/","excerpt":"给定两个非空的链表，表示两个非负整数。 数字以相反的顺序存储，每个节点包含一个数字。 添加两个数字并将其作为链表返回。 您可以假设两个数字不包含任何前导零，除了数字0本身。","text":"给定两个非空的链表，表示两个非负整数。 数字以相反的顺序存储，每个节点包含一个数字。 添加两个数字并将其作为链表返回。 您可以假设两个数字不包含任何前导零，除了数字0本身。 思路： 参考常规的两数相加算式以及进位思想，两数相加与10相除得到该位相加后数值，两数相加与10取余得到该位相加后进位数。 方案： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263/** * Definition for singly-linked list. * public class ListNode &#123; * int val; * ListNode next; * ListNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public ListNode addTwoNumbers(ListNode l1, ListNode l2) &#123; ListNode root = null; ListNode parent = null; ListNode n1 = l1; ListNode n2 = l2; int val = 0; int carry = 0; while(true)&#123; if(n1 == null &amp;&amp; n2 == null)&#123; //在此情况下两个数各个位数都已经相加完成，只剩最后的进位数 if(carry != 0)&#123; ListNode child = new ListNode(carry); parent.next = child; parent = child; &#125; break; &#125;else if(n1 != null &amp;&amp; n2 != null)&#123; val = n1.val + n2.val + carry; carry = val / 10; val = val % 10; n1 = n1.next; n2 = n2.next; &#125;else if(n1 == null &amp;&amp; n2 != null)&#123; val = n2.val + carry; carry = val / 10; val = val % 10; n1 = null; n2 = n2.next; &#125;else if(n1 != null &amp;&amp; n2 == null)&#123; val = n1.val + carry; carry = val / 10; val = val % 10; n1 = n1.next; n2 = null; &#125; if(root == null)&#123; root = new ListNode(val); parent = root; &#125;else&#123; ListNode child = new ListNode(val); parent.next = child; parent = child; &#125; &#125; return root; &#125;&#125; 原文地址：2. Add Two Numbers","categories":[{"name":"leetcode","slug":"leetcode","permalink":"http://www.lpnote.com/categories/leetcode/"}],"tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://www.lpnote.com/tags/leetcode/"},{"name":"算法","slug":"算法","permalink":"http://www.lpnote.com/tags/算法/"}]},{"title":"leetcode算法题167_两数之和（II）","slug":"leetcode-two-sum-part-2","date":"2017-09-06T16:00:00.000Z","updated":"2019-02-13T08:59:06.532Z","comments":true,"path":"2017/09/06/leetcode-two-sum-part-2/","link":"","permalink":"http://www.lpnote.com/2017/09/06/leetcode-two-sum-part-2/","excerpt":"给定一个已经按升序排序的整数数组，找到两个数字，使它们相加到一个特定的目标数。 函数twoSum应该返回两个数字的索引，使它们相加到目标，其中index1必须小于index2。 请注意，您返回的答案（index1和index2）都不是基于零的。 您可以假设每个输入都将具有一个解决方案，您可能不会使用相同的元素两次。 输入：numbers = {2，7，11，15}，target = 9输出：index1 = 1，index2 = 2","text":"给定一个已经按升序排序的整数数组，找到两个数字，使它们相加到一个特定的目标数。 函数twoSum应该返回两个数字的索引，使它们相加到目标，其中index1必须小于index2。 请注意，您返回的答案（index1和index2）都不是基于零的。 您可以假设每个输入都将具有一个解决方案，您可能不会使用相同的元素两次。 输入：numbers = {2，7，11，15}，target = 9输出：index1 = 1，index2 = 2 方案： 12345678910111213141516class Solution &#123; public int[] twoSum(int[] numbers, int target) &#123; Map&lt;Integer,Integer&gt; indexMap = new HashMap&lt;Integer,Integer&gt;(); for(int i=0;i&lt;numbers.length;++i)&#123; indexMap.put(numbers[i],i); &#125; for(int i=0;i&lt;numbers.length;++i)&#123; Integer idx = indexMap.get(target-numbers[i]); if(idx != null &amp;&amp; i != idx)&#123; return new int[]&#123;i+1,idx+1&#125;; &#125; &#125; return null; &#125;&#125; 此题同时可见：leetcode算法题1_两数之和 原文地址：https://leetcode.com/problems/two-sum-ii-input-array-is-sorted/description/","categories":[{"name":"leetcode","slug":"leetcode","permalink":"http://www.lpnote.com/categories/leetcode/"}],"tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://www.lpnote.com/tags/leetcode/"},{"name":"算法","slug":"算法","permalink":"http://www.lpnote.com/tags/算法/"}]},{"title":"leetcode算法题653_两数之和（IV）","slug":"leetcode-two-sum-part-4","date":"2017-09-06T16:00:00.000Z","updated":"2019-02-13T08:59:06.532Z","comments":true,"path":"2017/09/06/leetcode-two-sum-part-4/","link":"","permalink":"http://www.lpnote.com/2017/09/06/leetcode-two-sum-part-4/","excerpt":"给定二进制搜索树和目标数字，如果BST中存在两个元素，使得它们的和等于给定的目标，则返回true。 Example 1: Input: 5 / \\ 3 6 / \\ \\ 2 4 7 Target = 9 Output: True Example 2: Input: 5 / \\ 3 6 / \\ \\ 2 4 7 Target = 28 Output: False","text":"给定二进制搜索树和目标数字，如果BST中存在两个元素，使得它们的和等于给定的目标，则返回true。 Example 1: Input: 5 / \\ 3 6 / \\ \\ 2 4 7 Target = 9 Output: True Example 2: Input: 5 / \\ 3 6 / \\ \\ 2 4 7 Target = 28 Output: False 方案： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849 /** * Definition for a binary tree node. * public class TreeNode &#123; * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public boolean findTarget(TreeNode root, int k) &#123; return traval(root,root,k); &#125; /** * 进行遍历，root用于在遍历和搜索中透传，node为要遍历的节点，k为target值 **/ public boolean travel(TreeNode root,TreeNode node, int k)&#123; if(searchVal(root,node,k-node.val)) return true; if(node.left != null)&#123; if(travel(root,node.left,k))&#123; return true; &#125; &#125; if(node.right != null)&#123; if(travel(root,node.right,k))&#123; return true; &#125; &#125; return false; &#125; /** * 搜索node节点及其子节点中是否有值和val相等，并且节点不为src **/ public boolean searchVal(TreeNode node,TreeNode src, int val)&#123; if(node == null) return false; if(node.val == val &amp;&amp; node != src) return true; else if(val &gt; node.val &amp;&amp; node.right != null)&#123; return searchVal(node.right,src,val); &#125; else if(val &lt; node.val &amp;&amp; node.left != null)&#123; return searchVal(node.left,src,val); &#125;else&#123; return false; &#125; &#125;&#125; 原文地址：Two Sum IV - Input is a BST","categories":[{"name":"leetcode","slug":"leetcode","permalink":"http://www.lpnote.com/categories/leetcode/"}],"tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://www.lpnote.com/tags/leetcode/"},{"name":"算法","slug":"算法","permalink":"http://www.lpnote.com/tags/算法/"}]},{"title":"leetcode算法题1_两数之和","slug":"leetcode-two-sum","date":"2017-09-06T16:00:00.000Z","updated":"2019-02-13T08:59:06.532Z","comments":true,"path":"2017/09/06/leetcode-two-sum/","link":"","permalink":"http://www.lpnote.com/2017/09/06/leetcode-two-sum/","excerpt":"给定一个整型数组，以及一个目标数值V，要求返回两个能够相对等于V值的两个数字的索引序列。每个数字只能使用一次。 例子：`Given nums = [2, 7, 11, 15], target = 9, Because nums[0] + nums[1] = 2 + 7 = 9,return [0, 1].`","text":"给定一个整型数组，以及一个目标数值V，要求返回两个能够相对等于V值的两个数字的索引序列。每个数字只能使用一次。 例子：`Given nums = [2, 7, 11, 15], target = 9, Because nums[0] + nums[1] = 2 + 7 = 9,return [0, 1].` 方案：暴力运算 12345678910111213class Solution &#123; public int[] twoSum(int[] nums, int target) &#123; if(nums.length &lt; 2) return null; for(int i=0;i&lt;nums.length;++i)&#123; for(int j=i+1;j&lt;nums.length;++j)&#123; if((nums[i] + nums[j]) == target)&#123; return new int[]&#123;i,j&#125;; &#125; &#125; &#125; return null; &#125;&#125; 时间复杂度O(n*n)，空间复杂度O(n) 扩展方案：带索引 12345678910111213141516class Solution &#123; public int[] twoSum(int[] nums, int target) &#123; Map&lt;Integer,Integer&gt; indexMap = new HashMap&lt;Integer,Integer&gt;(); for(int i=0;i&lt;nums.length;++i)&#123; indexMap.put(nums[i],i); &#125; for(int i=0;i&lt;nums.length;++i)&#123; Integer idx = indexMap.get(target-nums[i]); if(idx != null &amp;&amp; i != idx)&#123; return new int[]&#123;i,idx&#125;; &#125; &#125; return null; &#125;&#125; 时间复杂度O(n)，空间复杂度O(n) 注：该种方法有一定问题，前提需要加强条件：序列中没有重复的数字。不然该indexMap会被冲突覆盖 如：Input: [0,4,3,0] 0Output：nullExpected: [0,3] 原文地址：https://leetcode.com/problems/two-sum/description/","categories":[{"name":"leetcode","slug":"leetcode","permalink":"http://www.lpnote.com/categories/leetcode/"}],"tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://www.lpnote.com/tags/leetcode/"},{"name":"算法","slug":"算法","permalink":"http://www.lpnote.com/tags/算法/"}]},{"title":"如何写好一个Git Commit Message","slug":"how-to-write-a-git-message","date":"2017-08-20T16:00:00.000Z","updated":"2019-02-13T08:59:06.528Z","comments":true,"path":"2017/08/20/how-to-write-a-git-message/","link":"","permalink":"http://www.lpnote.com/2017/08/20/how-to-write-a-git-message/","excerpt":"原文地址：How to Write a Git Commit Message 介绍：为什么好的提交信息很重要如果您随机浏览一个Git存储库的日志，您可能会发现其提交消息或多或少是一团糟。 例如，从早期的Spring提交日志来看这些问题点： 1234567$ git log --oneline -5 --author cbeams --before \"Fri Mar 26 2009\"e5f4b49 Re-adding ConfigurationPostProcessorTests after its brief removal in r814. @Ignore-ing the testCglibClassesAreLoadedJustInTimeForEnhancement() method as it turns out this was one of the culprits in the recent build breakage. The classloader hacking causes subtle downstream effects, breaking unrelated tests. The test method is still useful, but should only be run on a manual basis to ensure CGLIB is not prematurely classloaded, and should not be run as part of the automated build.2db0f12 fixed two build-breaking issues: + reverted ClassMetadataReadingVisitor to revision 794 + eliminated ConfigurationPostProcessorTests until further investigation determines why it causes downstream tests to fail (such as the seemingly unrelated ClassPathXmlApplicationContextTests)147709f Tweaks to package-info.java files22b25e0 Consolidated Util and MutableAnnotationUtils classes into existing AsmUtils7f96f57 polishing 与同一存储库中的这些最近的提交进行比较： 1234567$ git log --oneline -5 --author pwebb --before \"Sat Aug 30 2014\"5ba3db6 Fix failing CompositePropertySourceTests84564a0 Rework @PropertySource early parsing logice142fd1 Add tests for ImportSelector meta-data887815f Update docbook dependency and generate epubac8326d Polish mockito usage 哪一个你更愿意去阅读呢？ 前者在长度和形式上变化很大; 后者简洁而一致。 前者是默认情况; 后者不会偶然发生。 虽然许多Git存储库的日志看起来像前者，但也有例外。 Linux内核和Git本身就是很好的例子。 看看Spring Boot，或者由Tim Pope管理的任何存储库。","text":"原文地址：How to Write a Git Commit Message 介绍：为什么好的提交信息很重要如果您随机浏览一个Git存储库的日志，您可能会发现其提交消息或多或少是一团糟。 例如，从早期的Spring提交日志来看这些问题点： 1234567$ git log --oneline -5 --author cbeams --before \"Fri Mar 26 2009\"e5f4b49 Re-adding ConfigurationPostProcessorTests after its brief removal in r814. @Ignore-ing the testCglibClassesAreLoadedJustInTimeForEnhancement() method as it turns out this was one of the culprits in the recent build breakage. The classloader hacking causes subtle downstream effects, breaking unrelated tests. The test method is still useful, but should only be run on a manual basis to ensure CGLIB is not prematurely classloaded, and should not be run as part of the automated build.2db0f12 fixed two build-breaking issues: + reverted ClassMetadataReadingVisitor to revision 794 + eliminated ConfigurationPostProcessorTests until further investigation determines why it causes downstream tests to fail (such as the seemingly unrelated ClassPathXmlApplicationContextTests)147709f Tweaks to package-info.java files22b25e0 Consolidated Util and MutableAnnotationUtils classes into existing AsmUtils7f96f57 polishing 与同一存储库中的这些最近的提交进行比较： 1234567$ git log --oneline -5 --author pwebb --before \"Sat Aug 30 2014\"5ba3db6 Fix failing CompositePropertySourceTests84564a0 Rework @PropertySource early parsing logice142fd1 Add tests for ImportSelector meta-data887815f Update docbook dependency and generate epubac8326d Polish mockito usage 哪一个你更愿意去阅读呢？ 前者在长度和形式上变化很大; 后者简洁而一致。 前者是默认情况; 后者不会偶然发生。 虽然许多Git存储库的日志看起来像前者，但也有例外。 Linux内核和Git本身就是很好的例子。 看看Spring Boot，或者由Tim Pope管理的任何存储库。 对这些存储库的贡献者来说，精心设计的Git提交消息是向同事开发人员（以及其将来的自己）传达关于变更的上下文的最佳方式。 差异会告诉你什么改变了，但只有提交消息可以正确地告诉你为什么。 彼得·赫特雷尔（Peter Hutterer）表示： Re-establishing the context of a piece of code is wasteful. We can’t avoid it completely, so our efforts should go to reducing it [as much] as possible. Commit messages can do exactly that and as a result, a commit message shows whether a developer is a good collaborator. 如果你没有太多思考什么是一个很好的Git提交消息，可能是没有花费太多时间使用git日志和相关工具。 这里有一个恶性循环：因为提交历史是非结构化和易变的，所以不会花太多时间使用或在意它。 并且因为它没有得到使用或在意，所以它仍然是非结构化和易变的。 但是一个很好的提交日志是一个优美而有用的事情。 git blame, revert，rebase，log，shortlog等子命令。 审查他人的commits和pull requests成为值得做的事情。 了解几个月或几年前为什么会发生的事情不仅变得可能而且会很有效率。 一个项目的长期成功取决于其可维护性（除其他外），维护者的工具比其项目的日志更强大。 值得花时间学习如何妥善照顾。 最初的麻烦，很快就会成为习惯，最终成为所有参与者的骄傲和生产力的源泉。 在这篇文章中，我只讨论保持健康的提交历史的最基本的元素：如何编写一个单独的提交信息。 还有其他一些重要的做法，如我不在这里处理的强调。 也许我会在随后的一篇文章中这样做。 大多数编程语言对于什么构成习语风格，即命名，格式化等都有完整的约定。 当然，这些惯例有差异，但是大多数开发人员认为，选择一个并坚持下去，远远超过每个人自己独特个性化的风格所造成的混乱。 一个团队对其提交日志的方法应该没有什么不同。 为了创建一个有用的修订历史，团队应首先同意至少定义以下三件事情的提交消息约定： 样式 标记语法，包含边距，语法，大小写，标点符号。 拼出这些东西，消除猜测，尽可能简单。 最终结果将是一个非常一致的日志，不仅是阅读的乐趣，而且实际上可以定期阅读。 内容 提交消息的主体（如果有）包含什么样的信息？ 它不包含什么？ 元数据 如何引用跟踪ID，提取请求号等？ 幸运的是，实际上关于怎么让Git提交消息有一个很完善的约定没有什么是你需要重新规划或者发明创造的。 其中许多是以某些Git命令的功能为基础的，只要按照下面的七条规则，你就可以像个专业人士一样提交你的Git日志。 一个伟大的Git提交消息的七个规则 将摘要与详细内容用空行分开 将摘要行限制为50个字符 将摘要首字母大写 摘要结尾处不要使用标点符号 在摘要行中使用必要的语气 正文内容以72个字符为限进行换行 使用详细消息部分来解释what、why、how 例如： 123456789101112131415161718192021222324252627Summarize changes in around 50 characters or lessMore detailed explanatory text, if necessary. Wrap it to about 72characters or so. In some contexts, the first line is treated as thesubject of the commit and the rest of the text as the body. Theblank line separating the summary from the body is critical (unlessyou omit the body entirely); various tools like `log`, `shortlog`and `rebase` can get confused if you run the two together.Explain the problem that this commit is solving. Focus on why youare making this change as opposed to how (the code explains that).Are there side effects or other unintuitive consequences of thischange? Here&apos;s the place to explain them.Further paragraphs come after blank lines. - Bullet points are okay, too - Typically a hyphen or asterisk is used for the bullet, preceded by a single space, with blank lines in between, but conventions vary hereIf you use an issue tracker, put references to them at the bottom,like this:Resolves: #123See also: #456, #789 将摘要与详细内容用空行分开 Though not required, it’s a good idea to begin the commit message with a single short (less than 50 character) line summarizing the change, followed by a blank line and then a more thorough description. The text up to the first blank line in a commit message is treated as the commit title, and that title is used throughout Git. For example, Git-format-patch(1) turns a commit into email, and it uses the title on the Subject line and the rest of the commit in the body. 首先，并不是每一个提交日志都要求一个摘要和一个详细描述。 有时单行很好，特别是当变化如此简单，不需要进一步的上下文。 例如：1Fix typo in introduction to user guide 没有更多的需要说 如果读者想知道错字是什么，她可以简单地看一下变化本身，即使用git show或git diff或git log -p。如果你想在命令行中提交了这样的内容，那么可以使用-m选项来简单地提交：1$ git commit -m&quot;Fix typo in introduction to user guide&quot; 但是，当一个提交有一点解释和上下文的时候，你需要写一个正文。 例如：12345Derezz the master control programMCP turned out to be evil and had become intent on world domination.This commit throws Tron&apos;s disc into MCP (causing its deresolution)and turns it back into a chess game. 使用-m选项提交带有主体的消息不是很容易编写。 你最好在适当的文本编辑器中编写消息。 如果您还没有在命令行中设置一个用于Git的编辑器，请阅读Pro Git的这一部分。 在任何情况下，提交日志摘要与详细描述的分离在浏览日志时都会付出代价。 这是完整的日志条目：12345678910$ git logcommit 42e769bdf4894310333942ffc5a15151222a87beAuthor: Kevin Flynn &lt;kevin@flynnsarcade.com&gt;Date: Fri Jan 01 00:00:00 1982 -0200 Derezz the master control program MCP turned out to be evil and had become intent on world domination. This commit throws Tron&apos;s disc into MCP (causing its deresolution) and turns it back into a chess game. 现在使用git log –online，打印出主题行：12$ git log --oneline42e769 Derezz the master control program 或者使用git shortlog，这个命令会按用户进行分组汇总提交，再次显示简洁的主题行：1234567891011121314$ git shortlogKevin Flynn (1): Derezz the master control programAlan Bradley (1): Introduce security program &quot;Tron&quot;Ed Dillinger (3): Rename chess program to &quot;MCP&quot; Modify chess program Upgrade chess programWalter Gibbs (1): Introduce protoype chess program 在Git中还有其他一些上下文，其中摘要行和主体内容之间的区别是在两者之间没有空白行的情况下都是不能正常工作的。 将主题行限制为50个字符50个字符不是一个极限，只是一个经验法则。 保持这个长度的主题确保了它们的可读性，并迫使作者想了解一下最简洁的方式来解释发生了什么。 提示：如果你很难总结，你可能是一次性提交了太多更改。 你需要争取将这次提交变成多个原子提交（一个提交对应一个主题域的修改）。 GitHub的UI完全了解这些约定。 如果你超过50个字符的限制，它会发出警告： 并且将使用省略号截断长度超过72个字符的任何主题行： 所以50个字符是合适的，最大不要超过72的硬限制。 摘要行首字母大写这听起来很简单。 用大写字母开始所有主题行。 例如： Accelerate to 88 miles per hour 代替： accelerate to 88 miles per hour 摘要结尾处不要使用标点符号主题行中不需要拖尾的标点符号。 此外，当你试图把它们保持在50个字符或更少时，空间是宝贵的。 例如: Open the pod bay doors 代替: Open the pod bay doors. 在摘要行中使用必要的语气命令式的语气只是意味着“说出来或写出来，就像给出命令或指示”一样。 几个例子： 收拾你的房间关门把垃圾带出去你正在阅读的七个规则中的每一个现在都写在命令中（“将消息内容以72个字符换行”等）。 命令可能听起来有点粗鲁; 这就是为什么我们不经常使用它。 但它却是比较适合的Git日志主题行的。 这样做的一个原因是，当Git自己代表你创建一个提交时，它就会使用这个命令。 例如，使用git merge时创建的默认消息如下：1Merge branch &apos;myfeature&apos; 当使用git revert时：123Revert &quot;Add the thing with the stuff&quot;This reverts commit cc87791524aedd593cff5a74532befe7ab69ce9d. 或者当点击GitHub PR请求上的“合并”按钮时：1Merge pull request #123 from someuser/somebranch 所以当你写下你的提交信息在命令行中时，你需要遵循Git自己的内置约定。 例如： Refactor subsystem X for readability Update getting started documentation Remove deprecated methods Release version 1.0.0 起初写这种方式可能有点尴尬。 我们更习惯于以指示性的心情来说话，这是关于报告事实。 这就是为什么提交信息通常最终会如下所示： Fixed bug with Y Changing behavior of X 有时候，提交消息将作为其内容的描述： More fixes for broken stuff Sweet new API methods 为了消除任何混乱，这里有一个简单的规则，每次都可以正确使用。 正确形成的Git提交主题行应始终能够完成以下句子： If applied, this commit will your subject line here 例如： If applied, this commit will refactor subsystem X for readability If applied, this commit will update getting started documentation If applied, this commit will remove deprecated methods If applied, this commit will release version 1.0.0 If applied, this commit will merge pull request #123 from user/branch 请注意理解为什么这对于其他非必要形式不起作用： If applied, this commit will fixed bug with Y If applied, this commit will changing behavior of X If applied, this commit will more fixes for broken stuff If applied, this commit will sweet new API methods 请记住：只有在主题行中，以上的规约才是重要的。 当你在书写详细描述时，你可以放松这个限制。 正文内容以72个字符为限进行换行Git不会自动包装文本。 当您写入提交消息的正文时，您必须记住其正确的边距，并手动换行。 建议是以72个字符做这个，所以Git有足够的空间来缩进文字，同时保持一切不超过80个字符。 一个好的文本编辑器可以提供帮助的。 例如，当您编写Git提交时，可以轻松地将Vim配置为包含72个字符的文本。 然而，传统上，IDE在提交消息中为文本包装提供智能支持非常可怕（尽管在最近的版本中，IntelliJ IDEA终于得到了更好的解决）。 使用详细消息部分来解释what、why、howBitcoin Core的这个提交是一个很好的例子，说明改变了什么，为什么要做这样的改变：123456789101112131415161718192021commit eb0b56b19017ab5c16c745e6da39c53126924ed6Author: Pieter Wuille &lt;pieter.wuille@gmail.com&gt;Date: Fri Aug 1 22:57:55 2014 +0200 Simplify serialize.h&apos;s exception handling Remove the &apos;state&apos; and &apos;exceptmask&apos; from serialize.h&apos;s stream implementations, as well as related methods. As exceptmask always included &apos;failbit&apos;, and setstate was always called with bits = failbit, all it did was immediately raise an exception. Get rid of those variables, and replace the setstate with direct exception throwing (which also removes some dead code). As a result, good() is never reached after a failure (there are only 2 calls, one of which is in tests), and can just be replaced by !eof(). fail(), clear(n) and exceptions() are just never called. Delete them. 看看完整的差异，只是想想作者通过花时间在这里和现在提供这个上下文来节省他们和未来的提交者多少时间。 如果他没有，它可能会永远失去。 在大多数情况下，你可以省略有关如何进行更改的详细信息。 代码在这方面通常是不言自明的（如果代码如此复杂，需要在散文中解释，那就是源代码注释）。 只要重点明确你为什么首先做出改变的原因 - 变革之前工作的方式（以及这是什么问题），现在的工作方式，以及为什么决定以你所做的方式解决。 未来的维护者，谢谢你可能是你自己！ 提示学会喜爱命令行。 离开IDE。 由于多种原因，拥抱命令行是明智之举。 Git非常强大，IDE也是这样，但是每个IDE都有不同的方式。 我每天都使用一个IDE（IntelliJ IDEA），并广泛使用过其它的（Eclipse），但我从未看过Git的IDE集成可以像命令行一样强大。 某些与Git相关的IDE功能是非常宝贵的，例如在删除文件时调用git rm，并在重命名时使用git进行正确的操作。 当你开始尝试通过IDE提交，合并，重新生成或进行复杂的历史分析时，所有的事情都会分崩离析，导致你无法在IDE上很好的工作。 当谈到发挥Git的全部力量时，它就是命令行。 请记住，无论是使用Bash还是Zsh或Powershell，都有一些标辅助功能脚本，可以辅助进行一些命令提示。 Read Pro GitPro Git可以在线免费获得，这太棒了。","categories":[{"name":"翻译文章","slug":"翻译文章","permalink":"http://www.lpnote.com/categories/翻译文章/"}],"tags":[{"name":"git","slug":"git","permalink":"http://www.lpnote.com/tags/git/"},{"name":"github","slug":"github","permalink":"http://www.lpnote.com/tags/github/"}]},{"title":"DIY配置中心系列（三）：高可用设计","slug":"high-availability-design-of-DIY-configcenter","date":"2017-08-17T16:00:00.000Z","updated":"2019-02-13T08:59:06.528Z","comments":true,"path":"2017/08/17/high-availability-design-of-DIY-configcenter/","link":"","permalink":"http://www.lpnote.com/2017/08/17/high-availability-design-of-DIY-configcenter/","excerpt":"前言前面两篇文章中描述了配置中心的基本架构与基本概念，本节将讲述如何实现配置存储与获取的高可用设计。 配置中心作为整个应用运维体系的基础组件，其重要性可见一斑。如果配置系统不具备高可用的特性，一旦发生配置中心服务不可用，那么影响到的将要成千上万的在线应用，由此带来的损失简直是无法估量。做到配置中心高可用架构可以从以下几个方面着手： 存储高可用 API服务高可用 客户端容灾 整体架构 Operator --------&gt; Web主控台 -------&gt; MySQL Master | | (notify) (sync) | | ----------| | | --------------------- SDK Client ---（Load Banlancer）----&gt; API Server Cluster | | | | | slave1 slave2 slave3 ... (cache) (dump file) ^ ^ ^ | | | | | Local Cache |----&gt;Slave Proxy(HaProxy) ------------------------","text":"前言前面两篇文章中描述了配置中心的基本架构与基本概念，本节将讲述如何实现配置存储与获取的高可用设计。 配置中心作为整个应用运维体系的基础组件，其重要性可见一斑。如果配置系统不具备高可用的特性，一旦发生配置中心服务不可用，那么影响到的将要成千上万的在线应用，由此带来的损失简直是无法估量。做到配置中心高可用架构可以从以下几个方面着手： 存储高可用 API服务高可用 客户端容灾 整体架构 Operator --------&gt; Web主控台 -------&gt; MySQL Master | | (notify) (sync) | | ----------| | | --------------------- SDK Client ---（Load Banlancer）----&gt; API Server Cluster | | | | | slave1 slave2 slave3 ... (cache) (dump file) ^ ^ ^ | | | | | Local Cache |----&gt;Slave Proxy(HaProxy) ------------------------ 存储高可用存储目前使用的是MySQL关系型服务器，采用一主多从的部署方式，主库只负责写入，不负责读取，所以以写为主。配置的读取和提供服务由API服务器承担，API服务器连接的是从库代理，保证只要有一台从库可用，整个服务不会出现问题。配置中心WEB主控台发生配置编辑后，同步写入MySQL主库，随后通知MySQL自身的数据同步，将发生改变的数据同步至多台从库中，这个MySQL之间的数据同步基本可控在毫秒范围。WEB主控台在发生配置编辑后同时会发送一个MQ消息通知API Server集群完成新配置的Dump。 API服务器高可用API服务器是配置中心SDK与配置中心打交道的门户，它提供RESTFUL服务供客户端使用。API服务设计为无状态，可以水平扩展更多的服务器，以应对更多的应用并发获取配置的流量增长。 每台API服务器会定时从MySQL从库中Dump完整的配置数据到本地磁盘上进行缓存，并在内存中更新该配置的指纹信息。 同时API服务器会订阅WEB主控台发出的消息，对实时的配置修改进行实时的配置Dump。 API服务器提供给配置中心SDK配置拉取服务，它们之间会存在一个负载均衡器来分担流量。 客户端的容灾客户端通过SDK与API服务器交互取得配置，并缓存获取到的配置写入本地缓存文件。 当应用程序启动时，SDK会携带module/profile/version参数去拉取最新的配置。 应用启动后，SDK会定时再去API服务拉取更新配置，这时携带的参数为module/profile/version/signature。多增加的一个signature是用于指纹对比，只有发生了指纹变化的配置才会被重新拉取 客户端SDK提供给用户三种不同的配置加载策略： SERVER模式 这种模式要求每次启动都必须从API服务器拉取最新配置，如果拉取成功存入本地缓存，如果拉取过程失败，则程序启动失败 LOCAL模式 这种模式实际上是兼容以前的本地配置模式，这种模式下不会与配置中心进行任何交互，直接使用的是本地配置文件 AUTO模式 这种模式在启动时会尝试从API服务器拉取最新的配置，如果在有限的几次拉取重试失败后，改由本地的上次拉取成功的缓存文件加载引导程序启动 通过客户端的缓存，尽量避免了因为配置中心的故障导致的应用无法启动的问题，当然如果应用是在第一次启动，并且配置中心处于宕机，这个时候启动会失败 =( 总结配置中心通过三个不同维度的存储与缓存，解决了在分布式环境下配置获取的高可用问题。 配置数据存储高可用 API服务器高可用 客户端容灾设计","categories":[{"name":"原创文章","slug":"原创文章","permalink":"http://www.lpnote.com/categories/原创文章/"}],"tags":[{"name":"配置中心","slug":"配置中心","permalink":"http://www.lpnote.com/tags/配置中心/"},{"name":"DIY","slug":"DIY","permalink":"http://www.lpnote.com/tags/DIY/"}]},{"title":"DIY配置中心系列（二）：接入定义","slug":"join-up-definition-of-DIY-configcenter","date":"2017-08-15T16:00:00.000Z","updated":"2019-02-13T08:59:06.532Z","comments":true,"path":"2017/08/15/join-up-definition-of-DIY-configcenter/","link":"","permalink":"http://www.lpnote.com/2017/08/15/join-up-definition-of-DIY-configcenter/","excerpt":"前言对于配置中心主控台的设计，其实我设计的原则是设计成一个统一管理元数据（应用接入、环境定义、版本定义、人员权限划分、配置管控等）功能的一个综合平台。对于一个应用接入配置中心来说，很自然想到需要一些实体来承担配置的管理载体。主控台是用户使用操作最为频繁的地方，良好的组织结构与交互设计能够给人带来办事效率上的提高，所以这块我也是经过了很多次版本的设计与修改，最终产出的是一个类似树型的结构体系。","text":"前言对于配置中心主控台的设计，其实我设计的原则是设计成一个统一管理元数据（应用接入、环境定义、版本定义、人员权限划分、配置管控等）功能的一个综合平台。对于一个应用接入配置中心来说，很自然想到需要一些实体来承担配置的管理载体。主控台是用户使用操作最为频繁的地方，良好的组织结构与交互设计能够给人带来办事效率上的提高，所以这块我也是经过了很多次版本的设计与修改，最终产出的是一个类似树型的结构体系。 Module定义对于一个接入来说，我们配置中心平台需要知道该接入的一些元数据信息，比如该应用的名称是什么，它是由谁负责，联系方式是什么，是归属于哪个部门，是什么类型的应用等等，这些都是属于接入定义的范畴。这些属性当中有的我们可以通过其它外围系统获取，而有些则需要在新建接入的时候录入。 我们的配置中心除了希望应用能够很好的接入以外，还希望能够接入一些自研的基础组件的配置。所以我们在定义这个载体的时候并没有直接采用application这个名称，而是使用了module这个名称。这个名称涵盖了应用接入和基础组件的接入，具体这两种接入的区别以及不同点，稍后会有介绍。 Module定义的主要元数据如下： 模块名称这里是一个接入的唯一标识，由英文字母以及一些数字、下划线组成 描述描述该接入的一些其它方面的描述 接入语言多语言接入选择接入语言，如Java/NodeJS/PHP等 管理员设定设定该模块的管理员，模块的管理员对该模块的配置授权有决定作用 归属部门设定该模块归属于哪个部门 Profile定义在配置中心中，环境的术语名称叫Profile，这其实是一个非常标准化的称呼。在国外的很多网站上，个人设置其实就是用的这个Profile词汇；而在Spring开发中，对于不同类型的配置的区分也是通过叫做Profile的选项进行区分。所以这里我们也用Profile这个词来指代环境的定义。 传统的开发过程中，系统在各个不同的环境中是部署了多套系统，比如对应开发、测试、预发布和线上的环境，软件系统在各个环境都部署一套。这样做的好处是可以做到系统的环境的完全隔离，但是代价就是系统部署的量级随着环境套数的变化成指数增长。新定义一套环境是比较容易的，但是新建一套环境对应的系统部署则是非常麻烦的事情。 环境划分配置中心在这里采用的是另一种环境划分方式： 将环境划分成线上环境和线下环境。 线上环境: 所有的部署于线上的系统连接线上的配置中心部署，并与线下完全隔离，做到安全隔离。线上的环境可以自己进行定义，目前定义了两套环境：灰度环境(stage)、正式环境（product），当然可以增加更多的环境定义。 线下环境: 所有的部署于线下的系统连接线下的配置中心部署，线下的配置中心定义的环境有：开发（dev）、测试（test）、预发布（pre）等。 这样的环境定义后，为我们后面部署配置中心打下了基础，我们部署的配置中心将只需要部署两套系统（因为考虑安全的因素，线上与线下完全隔离）。不必像传统软件系统那样，新增一套环境则需要新增加一套软件部署。 Profile定义的主要元数据如下： 环境名称定义了环境的英文名称，该环境名称也包括了系统内置的一些预定义的环境名称，系统标准内置了dev/test/pre/stage/product等一系列的标准环境定义，也可以自己定义新的环境名称 环境描述描述环境名称以外更多的描述 环境类型环境类型用以区分是内置类型还是自定义类型 所属模块定义了该环境是属于哪个模块的， Version定义对于Version这个版本来说，如果大家使用过Dubbo，一定会不对它里面的Version的概念陌生，而我们这里的Version概念与之差不太多。Version主要用于多版本配置的并行运行或者新旧滚动发布。对于Dubbo中Version的主要使用方式就是在涉及到不兼容升级接口时，在滚动升级过程中同时并存新旧版本的服务，让接口消费方有一定的时间窗口逐步迁移至新的接口服务中来。当所有的旧接口调用都迁移到新接口调用中后，老版本的接口或者服务器就可以完全下线了。 在配置中心中，Version同样用于多版本并存、滚动更新这样的场景。当一个应用在某次的功能修改中发生了非常大的变更，而该变更对于当前正在线上运行的应用来说是不兼容的，也就是我们在上线操作过程中不能直接对线上的配置进行修改，因为这样直接的不兼容修改会直接反馈到线上运行应用，导致线上应用出现故障。最稳妥的做法就是新建一个版本，在应用中依赖该版本的配置进行发布，上线后线上的运行程序就会依赖新版本的配置，旧版本的配置将会失效，这时我们就可以安全的删除旧版本的配置了。 同时Version因为是树型结构的最底层，所以也充当了配置数据存储的角色。所有的Module-Profile-Version形成的坐标都对应了一组的配置，该配置将以JSON的形式存储在Version的元数据中 Version的主要元数据如下： 版本名称在Module-Profile的路径下唯一确定一个版本信息 所属Profile归属的环境信息 所属Module归属的接入定义 配置内容（JSON）配置内容，内部以JSON格式组织，配置存储的结构将在后面文章中再详细介绍 数据版本用户编辑配置的数据版本演进，该属性对于配置的顺序应用有非常重要的作用 配置指纹对配置内容进行的一个指纹签名，该属性对于后续SDK接入后配置的对比更新有非常重要的作用 描述对该版本的产生原因进行一些说明 小结通过上面的描述，Module/Profile/Version共同组成了一个三维坐标标记了一组配置。我们可以很快的画出一个带树状的组织结构： 应用接入型 Module ------&gt; Module | ----------------------------------------------------------- | | | | Dev Test Pre ... ------&gt; Profile | | | ----------------- ----------- ------------------ | | | | | | | | default v2 ... default v0.1 default v10 ... ------&gt; Version 问题以上Module/Profile/Version的划分并非没有问题，这种划分很好的解决了以应用为维度的接入，但是对于另一个以基础组件为维度的接入却显得比较困难。 基础组件如何接入对于一层的Module定义来说，可以很好的满足应用接入的需求，但是对于基础中间件的接入来说，它涉及到的维度就有两层了，一层是中间件本身的接入定义，另一层是使用该中间件的应用接入定义。怎么理解这两层含义呢？我们来举个例子： 在现有的基础之上，假如我们要接入一个消费中间件MQ的组件SDK，该SDK在初始化以及运行过程中需要一些动态可调的配置，现在我们把它接入配置中心。那我们就在主控台上新建一个Module，取名我们就定为MiddleWare-MQ，好了现在定义好了一个Module，该Module下分各种环境及配置信息。那么问题来了： 不同应用接入的中间件配置有可能不同，并不是一个大一统的配置 不同应用接入的中间件配置需要该应用的负责人有权限修改配置，即接入配置中心的中间件各接入方要有权限能够修改自己的接入配置 以上两点来说，刚才我们以应用接入的设计并不能很好的满足。 SubModule定义基于此现状，对基于应用接入的结构进行了扩展，将Module层进行扩展，支持多层Module父子关系（目前两层已经足够）： 第一层定义接入中间件属性，该层属于公共层，由接入方（中间件团队）维护，这层下面不直接挂接Profile，而是挂接SubModule。 第二层定义接入方属性，该层属于个性化层，由接入方（中间件团队）新建和维护，同时将配置编辑权限授权接入业务方，让业务方有权参与中间件配置编辑。该层下面才开始挂接Profile。 这里需要说明一下，第二层的新建建议是由中间件基础组件团队来执行，而执行前是需要业务方通过申请基础组件接入，由基础组件团队新建完第二层后将该层编辑权限授予业务方。 这样经过扩展之后整体的树型结构变成这样： 基础组件接入型 Module ------&gt; Module | -------------------------------------------- | | | | subModule1 subModule2 subModule3 ... ------&gt; SubModule | | | ----------------------------------------------------------- | | | | Dev Test Pre ... ------&gt; Profile | | | ----------------- ----------- ------------------ | | | | | | | | default v2 ... default v0.1 default v10 ... ------&gt; Version 那么这两种树型融合后的结果就是：Module分两种类型(type)，一种是下面直接挂载Profile，而另一种是下面还需要挂载SubModule。那么这时Module的主要元数据就需要增加至少两个属性： type 用以区分该Module是哪种类型 parent 用以标识该Module的父Module是谁，当type为应用接入类型时，该module的parent自然就为空了 最新Module定义的主要元数据如下： 模块名称这里是一个接入的唯一标识，由英文字母以及一些数字、下划线组成 类型用以区分该Module是哪种类型 父Module用以标识该Module的父Module是谁 描述描述该接入的一些其它方面的描述 接入语言多语言接入选择接入语言，如Java/NodeJS/PHP等 管理员设定设定该模块的管理员，模块的管理员对该模块的配置授权有决定作用 归属部门设定该模块归属于哪个部门 总结Module/Profile/Version共同组成了一个三维坐标标记了一组配置，它的组织结构形如以下结构： 应用接入型 Module ------&gt; Module | ----------------------------------------------------------- | | | | Dev Test Pre ... ------&gt; Profile | | | ----------------- ----------- ------------------ | | | | | | | | default v2 ... default v0.1 default v10 ... ------&gt; Version 基础组件接入型 Module ------&gt; Module | -------------------------------------------- | | | | subModule1 subModule2 subModule3 ... ------&gt; SubModule | | | ----------------------------------------------------------- | | | | Dev Test Pre ... ------&gt; Profile | | | ----------------- ----------- ------------------ | | | | | | | | default v2 ... default v0.1 default v10 ... ------&gt; Version 除了这种树型结构，在后面我们还引用了灰度控制的概念，以便于在变更配置项时能先验证后发布的过程，极大的降低了因错误地修改配置而导致的故障出现概率。 这种结构可以很好的适应各种部署问题。比如也可以把Profile抽象成机房IDC的概念，那么假如现在线上有两个数据中心，北京(bj)和上海(sh)，那我们完全可以在所有数据中心中只部署一套配置中心，仅仅使用profile就可以实现多数据中心的配置管理，如profile可以定义成product-sh，product-bj等等。 当然所有数据中心只部署一套配置中心就需要解决如何高效部署的问题，这个点我会在后面的文章中描述。","categories":[{"name":"原创文章","slug":"原创文章","permalink":"http://www.lpnote.com/categories/原创文章/"}],"tags":[{"name":"配置中心","slug":"配置中心","permalink":"http://www.lpnote.com/tags/配置中心/"},{"name":"DIY","slug":"DIY","permalink":"http://www.lpnote.com/tags/DIY/"}]},{"title":"DIY配置中心系列（一）：起步","slug":"start-of-DIY-configcenter","date":"2017-08-13T16:00:00.000Z","updated":"2019-02-13T08:59:06.544Z","comments":true,"path":"2017/08/13/start-of-DIY-configcenter/","link":"","permalink":"http://www.lpnote.com/2017/08/13/start-of-DIY-configcenter/","excerpt":"配置中心从开发到线上接入运行已经过去快半年时间了，目前配置中心整体运行非常平稳，达到当初的设计目的，这里才敢有勇气拿出来分享设计，毕竟一样东西拿出来与人分享是需要勇气和底气的。同时对自己的实践过程进行一些总结，希望自己有时间回头瞭望时会有新的认识和发现。 前言对于一个可运行的程序来说，配置可以说是驱动它运行的灵魂。良好的配置能指导程序正确的运行，并产出符合期望的结果。对于现有的软件来说，配置可以是多种多样的，配置可以写在配置文件中由程序运行时读取，也可以在启动程序时通过命令行方式传入，配置也可以是各种环境参数由程序在运行时进行读取，甚至有些参数直接写死在程序代码中。 对于程序开发来说，程序中某个参数的值在各种部署环境下或者在某种情况下需要更改，那这种值就可以抽象成配置项。在没有引入配置文件之前，要对各种部署进行适应的方式就是修改源代码中相应值然后重新打包并部署，而如果将该值抽象成配置后独立于配置文件中，这样就能够独立于程序之外单独进行修改而不用重新编译整体程序。","text":"配置中心从开发到线上接入运行已经过去快半年时间了，目前配置中心整体运行非常平稳，达到当初的设计目的，这里才敢有勇气拿出来分享设计，毕竟一样东西拿出来与人分享是需要勇气和底气的。同时对自己的实践过程进行一些总结，希望自己有时间回头瞭望时会有新的认识和发现。 前言对于一个可运行的程序来说，配置可以说是驱动它运行的灵魂。良好的配置能指导程序正确的运行，并产出符合期望的结果。对于现有的软件来说，配置可以是多种多样的，配置可以写在配置文件中由程序运行时读取，也可以在启动程序时通过命令行方式传入，配置也可以是各种环境参数由程序在运行时进行读取，甚至有些参数直接写死在程序代码中。 对于程序开发来说，程序中某个参数的值在各种部署环境下或者在某种情况下需要更改，那这种值就可以抽象成配置项。在没有引入配置文件之前，要对各种部署进行适应的方式就是修改源代码中相应值然后重新打包并部署，而如果将该值抽象成配置后独立于配置文件中，这样就能够独立于程序之外单独进行修改而不用重新编译整体程序。 常见传统配置方式对于传统程序开发来说，往往存在以下几种常见的配置方式： 方式一：从应用中加载配置 在传统的应用开发中，开发人员习惯于将配置放入到项目的相对路径下，如类路径resources下面等等，配置文件的内容随着线下各种环境的值进行修改，等待发布时再修改项目中的配置文件中的配置项为线上配置值。对于这种配置的方式，我们可以数数从软件的生命周期开始需要经过多少次修改，至少应该是包括以下几次修改： 开发阶段 开发阶段时，开发人员需要根据自己本机的开发环境的相应配置修改配置文件并打包编译，以确保正确的配置值能够在本地环境驱动程序运行。 测试阶段 测试阶段开始，测试人员或者开发人员需要修改测试环境对应的配置文件并打包编译，以确保正确的配置值能够在本地环境驱动程序运行。 上线阶段 在上线前，仍然需要手工修改线上的配置值然后再打包并部署。 从上面可以看出此种方式缺点很明显：很容易出错！很容易在各种环境配置的切换中配置错误，修改越多，出错的概率就越大。 方式二：Maven多profile配置管理 该配置方式充分利用的是maven工具的profile过滤替换功能。 将应用的运行环境划分成不同的环境profile：开发/测试/预发/灰度/线上等等profile 项目结构下分别建立各环境的配置文件，如：config_dev.properties/config_test.properties等 通过maven打包时的profile过滤机制替换项目中的占位符为特定profile的配置值 关于使用Maven profile机制打包的方法这里不作叙述，大家可以自行google。 这种方式改进了方式一的缺点，让配置的修改各自有了归属，对开发环境的profile配置修改不会影响到测试环境的profile配置以及线上的配置。但是这样的配置是有缺点的： 对于应用的开发有一定的阻碍，要想配置修改生效，必须每次都得重新指定profile并通过maven编译后部署，十分繁琐。 对于线上的配置值，尤其是一些比较重要的、机密的配置值裸露在项目中，存在一定的安全问题。 方式三：Spring多profile配置管理该机制与上面的Maven的profile很像，但是机制确有所有不同： 将应用的运行环境划分成不同的环境profile：开发/测试/预发/灰度/线上等等profile 项目结构下分别建立各环境的配置文件，如：config_dev.properties/config_test.properties等 Spring配置文件新建profiles节点，并将不同profile的配置bean进行配置加载 应用在启动时传入启用的profile名称，由Spring启用对应的profile 这种方式相对于Maven Profile的方式来说增强的地方在于：修改配置后可以直接重启而不用经过编译的繁琐过程。但是这种方式仍然存在Maven Profile的其它缺点外还存在的一个问题是：它必须和Spring深度捆绑！ 方式四：服务器配置覆盖替换项目中保留本地开发的默认配置文件，各环境（开发、测试、预发、灰度、线上）在应用服务器特定目录下放置配置替换文件，修改应用服务器启动脚本加载特定目录下面的配置文件进行替换覆盖。 以Jetty部署为例： 12345678910111213141516171819202122232425262728293031 #应用环境配置PROJECT_DIR='/data/www/java/$&#123;APP_DEPLOY_DIR&#125;'LOGGER_ROOT=$&#123;PROJECT_DIR&#125;/logs if [ ! -d $LOGGER_ROOT ];then mkdir -p $LOGGER_ROOTfi #JettyJETTY_HOME='/opt/jetty'JETTY_LOGS=$&#123;LOGGER_ROOT&#125;JETTY_RUN=$&#123;JETTY_HOME&#125;/run#配置文件JAVA_OPTIONS=\"-Dconf.file=file://$&#123;PROJECT_DIR&#125;/conf/conf.properties -Dlogger.root=$&#123;LOGGER_ROOT&#125;\" ##JVM参数设置JAVA_OPTIONS=\"$JAVA_OPTIONS -server -Xms4096m -Xmx4096m -XX:+UseConcMarkSweepGC -XX:+UseCMSCompactAtFullCollection -XX:CMSMaxAbortablePrecleanTime=5000 -XX:+CMSClassUnloadingEnabled -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=80\"JAVA_OPTIONS=\"$JAVA_OPTIONS -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=$&#123;LOGGER_ROOT&#125;/java.hprof\"JAVA_OPTIONS=\"$JAVA_OPTIONS -verbose:gc -Xloggc:$&#123;LOGGER_ROOT&#125;/gc.log -XX:+PrintGCDetails -XX:+PrintGCDateStamps\"JAVA_OPTIONS=\"$JAVA_OPTIONS -Djava.awt.headless=true\"JAVA_OPTIONS=\"$JAVA_OPTIONS -Dsun.net.client.defaultConnectTimeout=10000\"JAVA_OPTIONS=\"$JAVA_OPTIONS -Dsun.net.client.defaultReadTimeout=30000\"JAVA_OPTIONS=\"$JAVA_OPTIONS -XX:+DisableExplicitGC\" usage()&#123; echo \"Usage: $&#123;0##*/&#125; [-d] &#123;start|stop|run|restart|check|supervise&#125; [ CONFIGS ... ] \" exit 1&#125; 其中的-Dconf.properties=file://$PROJECT_DIR/conf/conf.properties定义了加载配置替换文件的路径。当然要实现特定环境的配置文件替换还需要在应用中配置时加入类似如下的配置：1234567891011121314&lt;bean id=\"propertyConfigurer\" class=\"org.springframework.beans.factory.config.PropertyPlaceholderConfigurer\"&gt; &lt;property name=\"fileEncoding\" value=\"UTF-8\"/&gt; &lt;property name=\"systemPropertiesModeName\" value=\"SYSTEM_PROPERTIES_MODE_OVERRIDE\"/&gt; &lt;property name=\"ignoreResourceNotFound\" value=\"true\"/&gt; &lt;property name=\"locations\"&gt; &lt;list&gt; &lt;!-- 本地开发配置文件 --&gt; &lt;value&gt;classpath:props/config.properties&lt;/value&gt; &lt;!-- 线上覆盖替换文件 --&gt; &lt;value&gt;$&#123;conf.properties&#125;&lt;/value&gt; &lt;/list&gt; &lt;/property&gt;&lt;/bean&gt; 这样实现了配置文件的加载替换覆盖。 这种方式解决了上面几种配置方式的安全问题，同时也满足一部分开发的便利方式。但是该方式引出了新的问题：配置管理的繁琐性，不同的环境需要不同的替换配置文件，环境越多，配置的替换文件越多，对线下线上的服务器应用配置维护带来了很大的难度。开发的生命周期中，需要对很多地方做修改工作，容易疏忽和遗漏，开发人员在开发、测试等线下环境需要自行登录到线下服务器进行配置的修改，繁琐且不易管理。 配置中心什么是配置中心基于传统配置管理上的问题，统一的配置管理平台呼之欲出，主要目的就是要彻底解决以上遇到的所有问题，使应用与配置相对独立分开，应用打包交付后放入一个指定环境服务器，即可自动感知并拉取远程配置中心中对应的配置，这种方式也为现在非常流行的基于docker的容器化交付体系提供了实现基础。 那什么是配置中心？为什么要实施配置中心？ 配置中心提供了配置的统一规范化管理平台，提供统一的操作平台供开发，测试，运维各种人员使用，提供各种应用部署环境的配置集中修改及配置的分发操作，减少QA测试、运维的机械性地配置文件修改劳动。并维护一个所有人员的权限操作列表，以区分不同人员的权限范围。对任何配置的修改操作进行日志审计记录，随时可查询配置的变更历史信息。可以做到应用上线安装好应用服务器后不做任何修改放入应用包直接运行的效果，这也是应用容器化的最根本的前置条件。同时也做到了应用迁移的便利性，以前的应用扩容，运维人员需要COPY整个应用服务器及相关目录的结构和内容到新扩容机器，接入配置中心后，应用的扩容将会是直接分配新机器，扔入应用包，直接启动就可以，无需其它繁杂的应用配置工作。 配置中心体系实现的功能它实现了： 支持对接入应用的定义 支持对各运行环境/数据中心的定义 支持对配置版本的定义 支持对灰度配置的定义 支持配置修改的权限管理及历史记录 支持全兼容的原有应用无痛接入（支持直接导入项目properties文件） 支持提供Restful API接口供其他语言开发SDK或者Agent 支持对Spring框架的全兼容 开源界的配置中心在我们的配置开始我们DIY的配置中心开发之前，业界已经存在的以下几个开源的配置中心方案： 淘宝开源的Diamond 360开源的QConf 百度开源的Disconf 携程开源的Apollo（在我们开始自研配置中心之前它并没有开源，我们也不知道有这个东西） QConf是基于zookeeper实现的一款多语言的配置中心，整体架构采用配置中心+Agent的方式，它需要在各部署机器安装Agent以代理配置的获取，这给我们的实施带来了困难并没有得到采用。Disconf支持的配置方式很多,与spring集成的很好，有web管理，client只支持java，这也许是放弃它的原因。Apollo这个配置中心不得不说设计的方向和我们的完全一样，很多概念上和我们的配置中心设计思想都是一致的，当然实现手法上可能各有千秋。也许它早点开源，我们就会直接采用它的实现进行二次开发也说不定。Diamond结构简单，设计精巧，整体完全的分布式高可用设计非常清晰，但是介于它的层次划分仍然存在一些小的问题，但是瑕不掩瑜，我们打算依照Diamond的架构设计依葫芦画瓢开干。 DIY配置中心术语配置中心一些术语的定义： 模块/应用（Module）定义了接入的最小单位，从抽象意义上来说，它可以是一个应用，也可以是一个庞大应用的一个子模块，对于我们一般的应用来说，接入是以应用为单位申请新建，而对于像基础中间件这样的中间件业务来说，它就是配置中心中某个基础组件下接入一个新应用。 运行环境（Profile）对于配置中心来说，定义了接入应用的运行环境：开发（dev）/ 测试（test）/ 预发布（stage）/线上（product），不同的运行环境肯定会有配置值的差别。而对于服务器来说，会定义一个基于/etc/config.env的文件，文件内容形式大致如下： 12345678#配置中心本地缓存目录CONFIG_CACHE_DIR=/etc/config_center/cache#配置中心本地容灾目录CONFIG_LOCAL_DIR=/etc/config_center/local#服务器运行环境设定，应用程序根据该值从配置中心拉取指定的环境配置信息初始化配置RUN_ENV=test#配置中心API服务器地址CONFIG_SERVER=http://api.config.domain 配置版本（Version）定义了运行环境的版本信息，允许多版本的并行运行，多版本的概念同一些分布式的组件如dubbo类似，提供了配置版本的平滑过渡。配置版本只是方便用于在线配置与应用不兼容升级变更的平滑过渡，不建议运行环境长期存在多个版本的并存，在配置过渡完成后可以删除失效的版本。 配置灰度（Gray）这是一个很有用的功能，可以防止“手滑”党的很大一部分错误发生。通常一个应用下面会部署多台机器，而在配置中心对于配置的修改会准实时同步到应用机器。如果一个非常重要的可实时生效的配置项被“手滑”的修改成了一个错误的值，那么该应用下的所有机器将会应用该配置，那么产生的后果将会相当严重，轻则线上应用不能正常运行，重则整个应用集群宕机不可用。配置灰度的提出，主要是解决这种问题，配置灰度可以在现有的配置版本基础上创建一个灰度配置，灰度配置中需要指定应用该灰度的服务器信息（如：IP），只有符合该IP的服务器才会应用该灰度的配置，达到配置修改的小范围验证目的，当验证通过后可以通过“完成灰度”来将灰度配置应用到正式版本上完成配置的正式发布。 DIY配置中心整体架构 总结配置中心提供了配置的统一规范化管理平台，提供统一的操作平台供开发，测试，运维各种人员使用，提供各种应用部署环境的配置集中修改及配置的分发操作，减少QA测试、运维的机械性地配置文件修改劳动，大大提升了开发、测试、运维等人员的工作效率。上面大致描述了配置中心的作用和基本构成要素，在实现配置中心过程需要考虑的地方其实是很多的，在接下来的几篇文章中我将详细介绍实现整个配置中心过程中的一些细节。","categories":[{"name":"原创文章","slug":"原创文章","permalink":"http://www.lpnote.com/categories/原创文章/"}],"tags":[{"name":"配置中心","slug":"配置中心","permalink":"http://www.lpnote.com/tags/配置中心/"},{"name":"DIY","slug":"DIY","permalink":"http://www.lpnote.com/tags/DIY/"}]},{"title":"升级Ambari中的Presto组件","slug":"upgrade-the-presto-component-in-ambari","date":"2017-08-09T16:00:00.000Z","updated":"2019-02-13T08:59:06.548Z","comments":true,"path":"2017/08/09/upgrade-the-presto-component-in-ambari/","link":"","permalink":"http://www.lpnote.com/2017/08/09/upgrade-the-presto-component-in-ambari/","excerpt":"前言在几天前我的一篇文章中，提到了需要升级公司的Presto数据查询引擎，而我们的Presto引擎是由Ambari管理的，升级Presto并没有任何文档可以参考，都是自己不断摸索出来的路子。所以写下本文用于记录摸索过程，并留待以后方便追溯和查看。 对于Ambari管理的组件，对于HDP发行版来说，单个组件的升级可能导致HDP发行版其它组件间的冲突，所以Ambari并没有提供单个组件的升级。而Ambari提供的升级方式是Ambari主控台先进行升级，升级后的Ambari可以在WEB界面上完成各个组件的升级，这样所有组件的统一升级的结果是：拥有最新的比较稳定的版本，且各组件间可以很好的配合使用，不会出现冲突，这个是Hortonworks公司帮我已经解决的依赖及冲突问题，这也是使用类似CDH或者HDP这样的Hadoop发行版的原因，省事！ 当然值得一提的是，升级大数据平台组件是一件需要非常谨慎又小心的事情，不到万不得已请不要贸然升级，升级前还要做好万全的准备，比如数据备份，升级计划梳理，灾难预案等等。 我这里升级的是Presto，是一个比较独立的组件，所以我打算单独升级它，并不使用整体升级Ambari来更新所有的组件，而且当前Presto并没有提供更新的Ambari插件，所以这里需要我自己进行一些Hack。","text":"前言在几天前我的一篇文章中，提到了需要升级公司的Presto数据查询引擎，而我们的Presto引擎是由Ambari管理的，升级Presto并没有任何文档可以参考，都是自己不断摸索出来的路子。所以写下本文用于记录摸索过程，并留待以后方便追溯和查看。 对于Ambari管理的组件，对于HDP发行版来说，单个组件的升级可能导致HDP发行版其它组件间的冲突，所以Ambari并没有提供单个组件的升级。而Ambari提供的升级方式是Ambari主控台先进行升级，升级后的Ambari可以在WEB界面上完成各个组件的升级，这样所有组件的统一升级的结果是：拥有最新的比较稳定的版本，且各组件间可以很好的配合使用，不会出现冲突，这个是Hortonworks公司帮我已经解决的依赖及冲突问题，这也是使用类似CDH或者HDP这样的Hadoop发行版的原因，省事！ 当然值得一提的是，升级大数据平台组件是一件需要非常谨慎又小心的事情，不到万不得已请不要贸然升级，升级前还要做好万全的准备，比如数据备份，升级计划梳理，灾难预案等等。 我这里升级的是Presto，是一个比较独立的组件，所以我打算单独升级它，并不使用整体升级Ambari来更新所有的组件，而且当前Presto并没有提供更新的Ambari插件，所以这里需要我自己进行一些Hack。 准备工作虽然是在测试环境试验，但是仍然需要先对数据及配置进行备份，这是升级必备要做的事。而备份对于Presto引擎来说只限于备份配置数据，Presto引擎本身是基本纯内存计算的数据查询引擎，故并不存在持久化于磁盘之上的数据。 准备Presto升级用的RPM包根据Presto Ambari插件源码，它是通过Maven仓库进行下载的：123[download]presto_rpm_url = http://search.maven.org/remotecontent?filepath=com/facebook/presto/presto-server-rpm/0.161/presto-server-rpm-0.161.rpmpresto_cli_url = http://search.maven.org/remotecontent?filepath=com/facebook/presto/presto-cli/0.161/presto-cli-0.161-executable.jar 这里因为国内的一些因素从Maven中央仓库下载会非常缓慢，可以改成自己的私源Maven仓库或者将RPM包和Jar包从中央仓库手动下载后上传至私有静态文件服务器，然后把上面的文件路径地址改成你添加的新位置。 同时，因为目前最新的Presto RPM包版本为0.182，所以推荐将老版本的RPM包和Jar包更新成最新版，因为新版本还是解决了很多问题，其中有一个内存泄漏的问题，在前面的文章中提及过，也是为什么要升级Presto的原因。 具体链接如下：Presto内存溢出(OutOfMemory)问题排查 备份原有的Presto配置原有的Presto配置文件是通过Ambari Web管理控制台进行配置的，目前来看并没有针对单个组件的备份方式，Ambari整体备份方式是通过备份ambari的数据库表来达到的，这不是我们想要的。 既然没有简单的备份方式，那么唯一可做的就是将每个配置项扣出来，自己通过某个方式进行备份。通过分析Ambari Web主控台的交互，发现有一个Web API可以获取到服务组件的配置，该API接口返回Json结构数据，这个Json数据可以用来作为备份数据： API接口形如：/api/v1/clusters/test_cluster/configurations/service_config_versions?service_name.in(PRESTO)&amp;is_current=true&amp;fields=*&amp;_=1502331979100 {test_cluster}为集群的名称，需要根据自己的实际Ambari集群进行修改 {PRESTO}为安装的Presto服务名字，这个也需要根据自己安装定义的Presto服务组件名称指定 返回数据形式如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081&#123; \"href\" : \"http://172.17.31.251:8080/api/v1/clusters/test_cluster/configurations/service_config_versions?service_name.in(PRESTO)&amp;is_current=true&amp;fields=*&amp;_=1502331979100\", \"items\" : [ &#123; \"href\" : \"http://172.17.31.251:8080/api/v1/clusters/test_cluster/configurations/service_config_versions?service_name=PRESTO&amp;service_config_version=3\", \"cluster_name\" : \"test_cluster\", \"configurations\" : [ &#123; \"Config\" : &#123; \"cluster_name\" : \"test_cluster\", \"stack_id\" : \"HDP-2.5\" &#125;, \"type\" : \"config.properties\", \"tag\" : \"version1502248133724\", \"version\" : 9, \"properties\" : &#123; \"discovery.uri\" : \"http://master.hdp.test.cq:8285\", \"http-server.http.port\" : \"8285\", \"node-scheduler.include-coordinator\" : \"false\", \"query.max-memory\" : \"50\", \"query.max-memory-per-node\" : \"1\" &#125;, \"properties_attributes\" : &#123; &#125; &#125;, &#123; \"Config\" : &#123; \"cluster_name\" : \"test_cluster\", \"stack_id\" : \"HDP-2.5\" &#125;, \"type\" : \"connectors.properties\", \"tag\" : \"version1502269954279\", \"version\" : 11, \"properties\" : &#123; \"connectors.to.add\" : \"&#123;\\n 'hive': ['connector.name=hive-hadoop2','hive.metastore.uri=thrift://master.hdp.test.cq:9083,thrift://slave3.hdp.test.cq:9083','hive.recursive-directories=true','hive.config.resources=/etc/hadoop/conf/core-site.xml,/etc/hadoop/conf/hdfs-site.xml']\\n&#125;\", \"connectors.to.delete\" : \"[]\" &#125;, \"properties_attributes\" : &#123; &#125; &#125;, &#123; \"Config\" : &#123; \"cluster_name\" : \"test_cluster\", \"stack_id\" : \"HDP-2.5\" &#125;, \"type\" : \"jvm.config\", \"tag\" : \"version1502330557517\", \"version\" : 10, \"properties\" : &#123; \"jvm.config\" : \"-server\\n-Xmx8G\\n-XX:+UseG1GC\\n-XX:+UseGCOverheadLimit\\n-XX:+ExplicitGCInvokesConcurrent\\n-XX:+HeapDumpOnOutOfMemoryError\\n-XX:OnOutOfMemoryError=kill -9 %p\" &#125;, \"properties_attributes\" : &#123; &#125; &#125;, &#123; \"Config\" : &#123; \"cluster_name\" : \"test_cluster\", \"stack_id\" : \"HDP-2.5\" &#125;, \"type\" : \"node.properties\", \"tag\" : \"version1502269954279\", \"version\" : 12, \"properties\" : &#123; \"node.environment\" : \"test\", \"plugin.config-dir\" : \"/etc/presto/catalog\", \"plugin.dir\" : \"/usr/lib/presto/lib/plugin\" &#125;, \"properties_attributes\" : &#123; &#125; &#125; ], \"createtime\" : 1502330556753, \"group_id\" : -1, \"group_name\" : \"Default\", \"hosts\" : [ ], \"is_cluster_compatible\" : true, \"is_current\" : true, \"service_config_version\" : 3, \"service_config_version_note\" : \"\", \"service_name\" : \"PRESTO\", \"stack_id\" : \"HDP-2.5\", \"user\" : \"admin\" &#125; ]&#125; 升级步骤第一步：更新Ambari主控机上的Presto插件目录因为Ambari主控机上安装的Presto插件版本比较旧，且官方并没有怎么进行维护，有一些潜在的问题。这里我们需要对该Presto插件目录进行更新修复一些问题。 具体的修复过程，参见另一篇文章：修复ambari presto插件在重启时的BUG 将修复过后的文件夹拷贝至Ambari的插件目录(通常位于/var/lib/ambari-server/resources/stacks/HDP/2.5/services下，2.5为当前HDP的版本信息)替换原有的PRESTO目录。 完成更新后，需要重启Ambari-server，这样才能让Ambari-server重新识别新的Presto插件目录。 第二步：停止Presto服务在Ambari Web管控台上停止所有的Presto实例 第三步：在Ambari上删除Presto服务在第二步完成后（Ambari要求删除服务组件必须是在服务停止之后）进行Presto服务的删除操作： 第四步：在Presto机器上删除相关部署及部署缓存文件 Ambari的各个slave节点Agent在安装组件前会先将组件插件目录的packages下的一些脚本文件复制到本机并缓存，以便下次不再从主控机拷贝，由于我们这里进行了插件的升级更新，所以需要手工清理slave机器的缓存目录。 清理操作： 1rm -rf /var/lib/ambari-agent/cache/stacks/HDP/2.5/services/PRESTO &amp;&amp; ambari-agent restart 上述删除缓存目录并重启ambari-agent后，ambari-agent会重新从主控机master拉取服务插件的最新文件 Ambari不支持删除服务的同时对安装包进行卸载，也就是说删除的只是在Ambari管理的服务的可见性，服务其实仍然安装在Slave的机器上，如果我们需要安装新包，需要先卸载旧版本组件包。当然也可以对脚本进行进一步的修改，比如先检测组件包是否已经安装，如果安装先进行卸载等等操作，这里我们暂不这样操作。 卸载presto组件包： 1rpm -e `rpm -qa | grep presto` 第五步：在Ambari主控机上重新添加服务并配置部署机器上面第三步中删除了Presto，这里我们可以通过主控台的Admin-&gt;Stack And Versions重新添加服务，这里我们可以看到PRESTO服务已经是我们最新添加的版本了，点击Add Service根据提示一步步进行操作 第六步：还原配置项在安装过程中会提示进行配置，这个时候就是需要我们还原之前备份的配置项，这点Ambari做并不好，没有什么类似配置导入的功能，所以这里需要我们手动的从备份的文件中找出各个配置项手工填入。 第七步：根据提示完成部署在一切都准备就绪后，Ambari会自动进行组件的安装和配置，并会操作组件启动。 问题在安装过程中可能会出现以下问题： sudo: sorry, you must have a tty to run sudo 使用不同账户，执行执行脚本时候sudo经常会碰到 sudo: sorry, you must have a tty to run sudo这个情况，其实修改一下sudo的配置就好了 vi /etc/sudoers (最好用visudo命令) 注释掉 Default requiretty 一行 #Default requiretty 意思就是sudo默认需要tty终端。注释掉就可以在后台执行了。 彻底删除某个组件的服务 这里有一个链接，里面主要讲怎么彻底删除组件，这个删除包括从Ambari-Server的主控台上删除，还包括从物理安装中移除，参考链接在这里：Ambari里如何删除某指定的服务（图文详解） 总结Ambari的插件机制对于更新来说并不是特别方便，它自身提供的更新机制并不适用于自定义的组件升级，需要根据上面所说的进行自定义升级操作。对于HDP发行版自带的组件升级推荐使用升级Ambari版本的方式进行，这样可以做到HDP发行版的完全兼容处理。Ambari在自定义组件升级前一定要做好数据和配置的备份工作，以应对升级失败的回滚方案。 最后，对于一个企业来说，数据就是它的生命，要对数据存敬畏之心，任何一次组件的操作都可能千万数据的损坏和丢失，对于企业来说都是致命性的打击。对任何一次数据组件的升级都要小心又谨慎，制定周密的升级计划和灾难预案。在实施线上操作之前，勿必先在测试环境进行验证可行性，切不可只盲目听从文档或者网上言论就贸然实施。","categories":[{"name":"原创文章","slug":"原创文章","permalink":"http://www.lpnote.com/categories/原创文章/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.lpnote.com/tags/大数据/"},{"name":"presto","slug":"presto","permalink":"http://www.lpnote.com/tags/presto/"},{"name":"ambari","slug":"ambari","permalink":"http://www.lpnote.com/tags/ambari/"}]},{"title":"修复ambari presto插件在重启时的BUG","slug":"fix-the-bug-of-the-ambari-presto-plugin-at-restart","date":"2017-08-08T16:00:00.000Z","updated":"2019-02-13T08:59:06.524Z","comments":true,"path":"2017/08/08/fix-the-bug-of-the-ambari-presto-plugin-at-restart/","link":"","permalink":"http://www.lpnote.com/2017/08/08/fix-the-bug-of-the-ambari-presto-plugin-at-restart/","excerpt":"前言随着商业数据的不断累积与爆发式增长，传统的数据存储已经不能很好的满足日益增长的数据系统需要，传统的通过关系型数据进行数据获取的方式正在随着数据量的快速增长而出现了瓶颈，随着以Hadoop为代表的大数据平台的出现，有效解决了数据高速增长带来的数据存储和管理问题。现在大数据平台基本上每家公司都发展中长期必备的基础设施，公司基础数据平台使用的Hortonworks提供的大数据平台发行版，使用ambari进行数据平台的管理工作。前段时间大数据平台引入的Presto数据查询引擎作为即席查询工具，并使用ambari对presto的插件支持进行管理。 Ambari是由Hortonworks公司出品一款大数据平台管理软件，它将大数据平台所需要的各种组件进行统一管理，并提供大数据平台主机管理及各组件的安装制定、配置、启停、监控等一系列非常方便的功能。同时它也支持用户自定义组件插件，提供了良好的扩展能力。","text":"前言随着商业数据的不断累积与爆发式增长，传统的数据存储已经不能很好的满足日益增长的数据系统需要，传统的通过关系型数据进行数据获取的方式正在随着数据量的快速增长而出现了瓶颈，随着以Hadoop为代表的大数据平台的出现，有效解决了数据高速增长带来的数据存储和管理问题。现在大数据平台基本上每家公司都发展中长期必备的基础设施，公司基础数据平台使用的Hortonworks提供的大数据平台发行版，使用ambari进行数据平台的管理工作。前段时间大数据平台引入的Presto数据查询引擎作为即席查询工具，并使用ambari对presto的插件支持进行管理。 Ambari是由Hortonworks公司出品一款大数据平台管理软件，它将大数据平台所需要的各种组件进行统一管理，并提供大数据平台主机管理及各组件的安装制定、配置、启停、监控等一系列非常方便的功能。同时它也支持用户自定义组件插件，提供了良好的扩展能力。 问题Presto官方为Ambari提供了一个presto的插件，插件地址：Ambari-Presto-Service，该插件遵循Ambari的插件体系规范，可以集成到Ambari提供Presto的管理功能。 官方提供了一个Presto的Ambari插件集成文档地址，指明了如何将插件集成到Ambari中，该文档有点旧，只能是对照着大致看下，帮助理解。 官方的Presto存在在Ambari界面上对Presto进行重启会失败的问题,该问题由几个小问题组成，我都对其进行了修复。 多提一句的是：官方对于这个Ambari的插件的维护似乎并不上心，该插件最新的更新时间已经是7个月以前，而且最新的改动只是一个微小改动。所以，在官方网站上并不能找到相关的解决办法及修复，所有的所有只能是靠自己去理解Ambari插件的运行原理并根据思考去尝试修复。 问题1：Presto状态检测与Ambari状态检测的冲突问题在Ambari上进行Restart操作时，Ambari主控台实际上是依次调用了插件的stop/status/start函数，stop调用是为了让组件停止运行，而status调用是检测组件是否正确意义上停止运行，确保后续的start调用不会存在问题。对于Presto插件来说，节点为了coordinator和worker两种类型，但是获取他们的状态都通过Presto launcher.py脚本文件的status函数，该函数返回节点的状态：0表示正在运行；3表示已停止运行。而Presto插件脚本文件presto_coordinator.py在调用status时使用的Execute类，该类为Ambari内部工具类，使用就是执行你传入的脚本内容，并在你的脚本内容执行返回码不为0时抛出错误异常。 presto_coordinator.py脚本函数 123def status(self, env): from params import daemon_control_script Execute('&#123;0&#125; status'.format(daemon_control_script)) 现在问题就出现在这里，Ambari主控台调用presto_coordinator.py脚本的status函数时，Execute内部实现规定脚本正常执行完成时应该返回0，返回其它值将导致Execute抛出ExecutioinFailed异常。Execute实际是调用了Presto安装目录bin/launcher的status函数（该函数返回了状态码3），导致Execute收到了一个不被期望的返回值3，然后它抛出了一个ExecutionFailed异常，最终导致Ambari重启Presto异常中断。 /usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py文件内容： 123456789101112131415161718192021222324252627def restart(self, env): \"\"\" Default implementation of restart command is to call stop and start methods Feel free to override restart() method with your implementation. For client components we call install \"\"\" //此处省略N行 service_name = config['serviceName'] if config is not None and 'serviceName' in config else None try: #TODO Once the logic for pid is available from Ranger and Ranger KMS code, will remove the below if block. services_to_skip = ['RANGER', 'RANGER_KMS'] if service_name in services_to_skip: Logger.info('Temporarily skipping status check for &#123;0&#125; service only.'.format(service_name)) elif is_stack_upgrade: Logger.info('Skipping status check for &#123;0&#125; service during upgrade'.format(service_name)) else: self.status(env) raise Fail(\"Stop command finished but process keep running.\") except ComponentIsNotRunning as e: pass # expected except ClientComponentHasNoStatus as e: pass # expected //此处省略N行 从上面的代码可以看到调用self.status(env)后如果不抛出异常，则后面的raise Fail(&quot;Stop command finished but process keep running.&quot;)就会被执行，导致Ambari流程中断： 问题1解决方案对于这种不兼容情况，需要捕获异常并进行处理，在节点未运行的情况下（返回码3）给予Ambari主控台正确的信息： 修改后的presto_coordinator.py脚本函数12345678910def status(self, env): from params import daemon_control_script try: Execute('&#123;0&#125; status'.format(daemon_control_script)) except ExecutionFailed as ef: if ef.code == 3: #等于3表示Presto节点未运行 #这里只能抛出这个异常，这个异常在Ambari的框架中会被捕获并被正确理解和处理 raise ComponentIsNotRunning(\"ComponentIsNotRunning\") else: raise ef 问题2：脚本调用Python Http时传入参数类型不匹配 原因在于presto_coordinator.py脚本中start的时候传入构建PrestoClient对象的port属性并非string类型或者int类型 1234567891011def start(self, env): from params import daemon_control_script, config_properties, \\ host_info self.configure(env) Execute('&#123;0&#125; start'.format(daemon_control_script)) if 'presto_worker_hosts' in host_info.keys(): all_hosts = host_info['presto_worker_hosts'] + \\ host_info['presto_coordinator_hosts'] else: all_hosts = host_info['presto_coordinator_hosts'] smoketest_presto(PrestoClient('localhost','root',config_properties['http-server.http.port']),all_hosts) 问题2方案：进行类型转换修改下该函数：1234567891011def start(self, env): from params import daemon_control_script, config_properties, \\ host_info self.configure(env) Execute('&#123;0&#125; start'.format(daemon_control_script)) if 'presto_worker_hosts' in host_info.keys(): all_hosts = host_info['presto_worker_hosts'] + \\ host_info['presto_coordinator_hosts'] else: all_hosts = host_info['presto_coordinator_hosts'] smoketest_presto(PrestoClient('localhost','root',int(config_properties['http-server.http.port']),all_hosts) 解决该问题！ 问题3：重启冒烟测试时Presto Coodinator拒绝连接 原因在于： 在Presto Coordinator节点刚刚开始启动的情况下就进行了冒烟测试，这个启动并未保证Coordinator节点启动完成 问题3方案：增加延时时间，等待节点启动完毕 该方案只是延时的等待时间，在这个延时时间内一般情况下节点都会启动完成并能成功冒烟测试成功。 总结通过这次的问题排查，对于Ambari的插件体系也有一个大致的认识，也增强了自己问题排查、分析、解决问题的能力。对于Ambari Presto插件的修改我也会积极反馈至开源社区，提交相关ISSUE和PR，希望能得到社区的接纳。^_^ 提交的社区ISSUE: ISSUE-28 我的修复PR: PULL-29","categories":[{"name":"原创文章","slug":"原创文章","permalink":"http://www.lpnote.com/categories/原创文章/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.lpnote.com/tags/大数据/"},{"name":"bug","slug":"bug","permalink":"http://www.lpnote.com/tags/bug/"},{"name":"presto","slug":"presto","permalink":"http://www.lpnote.com/tags/presto/"},{"name":"ambari","slug":"ambari","permalink":"http://www.lpnote.com/tags/ambari/"},{"name":"参与开源","slug":"参与开源","permalink":"http://www.lpnote.com/tags/参与开源/"}]},{"title":"Presto内存溢出(OutOfMemory)问题排查","slug":"troubleshooting-of-outofmemory-in-presto","date":"2017-08-07T16:00:00.000Z","updated":"2019-02-13T08:59:06.548Z","comments":true,"path":"2017/08/07/troubleshooting-of-outofmemory-in-presto/","link":"","permalink":"http://www.lpnote.com/2017/08/07/troubleshooting-of-outofmemory-in-presto/","excerpt":"问题最近公司大数据查询引擎Presto运行非常不稳定，容易出现掉节点的问题（一些Presto节点会在莫名其妙的情况出现宕机），从日志上看到是内存溢出了。公司线上的Presto集群JVM使用的垃圾回收器是G1，这也是Presto引擎推荐的一款GC收集器，G1对于超大内存（6G+）的垃圾收集有着良好的效率，能够高效的进行垃圾收集，并控制垃圾收集导致的停顿时间。","text":"问题最近公司大数据查询引擎Presto运行非常不稳定，容易出现掉节点的问题（一些Presto节点会在莫名其妙的情况出现宕机），从日志上看到是内存溢出了。公司线上的Presto集群JVM使用的垃圾回收器是G1，这也是Presto引擎推荐的一款GC收集器，G1对于超大内存（6G+）的垃圾收集有着良好的效率，能够高效的进行垃圾收集，并控制垃圾收集导致的停顿时间。 追踪在出现宕机后，发现线上运行的JVM一直没有加GC日志打印，于是果断加上GC日志参数以便下次宕机收集GC情况： 12345678910111213-server-Xmx16G-XX:+UseG1GC-XX:+UseGCOverheadLimit-XX:+ExplicitGCInvokesConcurrent-XX:+HeapDumpOnOutOfMemoryError-XX:OnOutOfMemoryError=kill -9 %p-XX:+UseGCLogFileRotation-XX:NumberOfGCLogFiles=5-XX:GCLogFileSize=5M-XX:+PrintGCDetails-XX:+PrintGCDateStamps-Xloggc:/var/lib/presto/var/log/gc.log GC日志以5M的大小进行滚动，保留最近5个日志文件，便于GC日志文件的保留，同时JVM最大内存设置为16G。 分析在加上GC日志后不久又出现了宕机，这时收集得到了程序最后宕机时刻的GC日志文件，这里推荐一款GC日志的分析工具软件GCViewer，这款软件优于其它GC日志分析工具的地方在于它支持G1日志的分析，这是非常重要的一点。 由于GCViewer github上并没有打包好的现成品下载，所以需要自己下载后进行maven编译和打包，具体过程就不再叙述了。 通过命令行运行java -jar ./gcviewer-1.36-SNAPSHOT.jar就可以启动图形界面，并载入自己的GC日志文件进行分析。 图示： 粉红区域：Tenured Generation 黄色区域：Young Generation 蓝色线： Used Heap 紫色线： Used Tenured Heap 绿色线： GC Lines 1、从上图可以看到老年代占用了比较多的内存，基本在2G-15G之间波动。而年轻代则内存相对来说占用较少一些。老年代内存溢出时基本上占据了绝大部分内存。同时在GC日志中也找到了2017-08-08T09:31:11.187+0800: 47413.801: [GC pause (GCLocker Initiated GC) (young) (to-space exhausted), 0.2758880 secs]这样的日志打印，证明确实年轻代已经找不到内存可以分配使用了 2、从图中看GC时间比较正常，都基本维持在1s以下。 3、Presto在进行任务计算的时候会统计当前的内存使用量，并保证使用的内存不会超出限制，然而这个比较正常的GC日志内存溢出了。 可能的原因 正常使用的情况下，因为提交的任务过多，导致内存不足引发溢出 Presto存在内存泄漏 对于第一种情况，Presto提供了一些解决方案，比如限制单个任务的总数据量大小，单个任务每个节点的内存使用最大限制，以及可以采用Presto内置的队列排队等限制并发任务数等方式对于第二种情况，这种情况就比较麻烦了，需要确定是具体哪里的代码引起的内存泄漏 当在排查第一种情况的时候，检查Presto server日志的时候发现了一个隐藏在正常日志深处的一个错误日志： 拿着这个日志在Presto的GITHUB官方网站ISSUE列表上搜索，还真发现了一个和我这个错误日志一样的ISSUE: ISSUE-5688 根据ISSUE中的讨论初步认定该ISSUE存在内存泄漏的可能，并且在0.161版本上仍然存在，而我们线上使用的版本是0.160，恰好落在这个有问题的区间上。修复的提交是PULL-7099 根据tag历史可以知道，该问题是在0.166版本及以后才被进行了修复。 这比较肯定了我们线上内存溢出是因为这个bug导致的判断。 总结从目前跟踪到的情况来说，线上的内存溢出与旧版本Presto存在的内存泄漏有关。 接下来要着手处理的就是线上Presto版本的升级，这个升级我会另外再开篇文章来说明如何在Ambari管理软件中对已经存在的Presto进行升级处理。","categories":[{"name":"原创文章","slug":"原创文章","permalink":"http://www.lpnote.com/categories/原创文章/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.lpnote.com/tags/大数据/"},{"name":"presto","slug":"presto","permalink":"http://www.lpnote.com/tags/presto/"},{"name":"OOM","slug":"OOM","permalink":"http://www.lpnote.com/tags/OOM/"},{"name":"GC","slug":"GC","permalink":"http://www.lpnote.com/tags/GC/"}]},{"title":"SQL小记-改写SQL中Limit限制","slug":"rewrite-limit-part-in-sql","date":"2017-07-31T16:00:00.000Z","updated":"2019-02-13T08:59:06.536Z","comments":true,"path":"2017/07/31/rewrite-limit-part-in-sql/","link":"","permalink":"http://www.lpnote.com/2017/07/31/rewrite-limit-part-in-sql/","excerpt":"最近在做一款大数据查询系统，该系统提供给数据分析人员一个用户交互界面，让用户提供SQL语言进行交互查询，前台应用提供一些辅助功能，这样的数据查询系统要优于通过纯console的方式进行查询，查询平台提供了用户管理，权限管理，交互易用性扩展，多引擎框架支持等。 最近出现的一个问题是用户提交的SQL很大部分时间上并没有携带Limit限制返回条数，导致产生了大量的数据查询浪费，因为在界面上是不会展示所有的结果数据，一般我们在前台页面上只展现少量数据，比如只展现前面100条，而这100条在很大部分情况下已经满足了分析师同学的数据分析需求，然而不太规范的SQL编写可能导致无limit关键字而导致大量的查询，所以当前的一个需求是对用户提交的SQL进行改写，将SQL中存在limit限制的地方进行判断，如果没有设置limit数或者limit大于最大限制，设置成最大限制，限制数据的查询量和返回量。","text":"最近在做一款大数据查询系统，该系统提供给数据分析人员一个用户交互界面，让用户提供SQL语言进行交互查询，前台应用提供一些辅助功能，这样的数据查询系统要优于通过纯console的方式进行查询，查询平台提供了用户管理，权限管理，交互易用性扩展，多引擎框架支持等。 最近出现的一个问题是用户提交的SQL很大部分时间上并没有携带Limit限制返回条数，导致产生了大量的数据查询浪费，因为在界面上是不会展示所有的结果数据，一般我们在前台页面上只展现少量数据，比如只展现前面100条，而这100条在很大部分情况下已经满足了分析师同学的数据分析需求，然而不太规范的SQL编写可能导致无limit关键字而导致大量的查询，所以当前的一个需求是对用户提交的SQL进行改写，将SQL中存在limit限制的地方进行判断，如果没有设置limit数或者limit大于最大限制，设置成最大限制，限制数据的查询量和返回量。 思路因为SQL是动态变化的，想要解析出limit的话，可能需要解析整个SQL语句成SQL抽象语法树，然后遍历得到limit计算其值得到，但是对于目前这个需求，有个比较取巧的地方在于对于标准的SQL语句，Limit关键字始终位于语句最末尾的地方。形如：select * from a limit 100这样的SQL，可以抽象出一个正则表达式，用这个表达式去匹配用户提交的SQL，并解析出对应的limit数，并对解析的SQL进行limit替换就可以了 正则表达式我们要编写的其实是对SQL中limit的匹配关系解析，而SQL中limit形式其实是有两种的，一种是形如select * from a limit 100，另一种是形如select * from a limit 0,200。我在做的时候设计成了两组正则，因分析师在分析的时候大部分会使用第一种形式，如果在匹配的时候优先匹配第一种情况，这也是短路优化的一种手段吧。 我们的正则表达式为：123456//Limit关键字匹配: limit 10public static final String LIMIT_REPR_1 = \"^(.|\\\\s)+\\\\s+(limit|LIMIT)\\\\s+(\\\\d+)$\";//Limit关键字匹配: limit 10,100public static final String LIMIT_REPR_2 = \"^(.|\\\\s)+\\\\s+(limit|LIMIT)\\\\s+(\\\\d+)\\\\s*,\\\\s*(\\\\d+)$\"; 这时有一个问题，LIMIT_REPR_2表达式出现了严重的性能问题，匹配会一直卡在这个表达式的match上，非常糟糕，慢得无法忍受。 需要对上面我随手写的正则表达式进行优化。 网上有一篇文章讲Java正则表达式优化的，链接在这里：Java正则表达式优化，总体的思路是减少不确定的表达和累赘表达。 优化点： 表达式(.|\\s)+\\s+最开始我想表达的是允许limit关键字前面可以有任意的字符和空白，它其实可以简化成.+\\s+ (这里应该算是主要优化点，对于NFA的正则匹配方式优化前的表达式存在太多的折返导致性能极其低下) (limit|LIMIT)这部分去掉LIMIT，可以在匹配的时候将匹配串转成小写再匹配 优化后： 12345//Limit关键字匹配: limit 10public static final String LIMIT_REPR_1 = \"^.+\\\\s+limit\\\\s+(\\\\d+)$\";//Limit关键字匹配: limit 10,100public static final String LIMIT_REPR_2 = \"^.+\\\\s+limit\\\\s+\\\\d+\\\\s*,\\\\s*(\\\\d+)$\"; 最新更新：上述的正则表达式不能匹配带有格式（有空白符号和回车换行符的格式化语句）需要进行一些修改： 12345//Limit关键字匹配: limit 10public static final String LIMIT_REPR_1 = &quot;^(.|\\\\s)+\\\\s+limit\\\\s+(\\\\d+)$&quot;;//Limit关键字匹配: limit 10,100public static final String LIMIT_REPR_2 = &quot;^(.|\\\\s)+\\\\s+limit\\\\s+\\\\d+\\\\s*,\\\\s*(\\\\d+)$&quot;; 这组的正则表达式性能是极低的，复杂的sql基本上匹配会卡住非常非常长的时间 继续优化：12345//Limit关键字匹配: limit 10public static final String LIMIT_REPR_1 = &quot;^(.|[ \\\\f\\\\n\\\\r\\\\t\\\\v])+\\\\s+limit\\\\s+(\\\\d+)$&quot;;//Limit关键字匹配: limit 10,100public static final String LIMIT_REPR_2 = &quot;^(.|[ \\\\f\\\\n\\\\r\\\\t\\\\v])+\\\\s+limit\\\\s+\\\\d+\\\\s*,\\\\s*(\\\\d+)$&quot;; 实战中我也发现，正如上文中正则表达式优化所说的：\\s匹配任何空白字符，包括空格、制表符、换页符等等。等价于 [ \\f\\n\\r\\t\\v]。，但是如果我们用\\s来替换[ \\f\\n\\r\\t\\v]，得到的表达式性能极差，但是如果我们换用确定性的[ \\f\\n\\r\\t\\v]来替换，则性能则变得很好了，这也许就是Java正则表达式引擎做的优化手段吧？ 实现以下代码仅作参考，实则在某些sql条件下会有性能问题如果有更好的正则表达式的写法，欢迎联系我 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263/** * SQL查询Limit改写器 * Created by luopeng on 2017/8/1. * 因正则写得不太好，匹配时可能会出现性能问题，导致CPU 100% */@Deprecatedpublic class QuerySqlLimiter &#123; //Limit关键字匹配: limit 10 public static final String LIMIT_REPR_1 = \"^(.|[ \\\\f\\\\n\\\\r\\\\t\\\\v])+[ \\f\\n\\r\\t\\v]+limit[ \\f\\n\\r\\t\\v]+(\\\\d+)$\"; //Limit关键字匹配: limit 10,100 public static final String LIMIT_REPR_2 = \"^(.|[ \\\\f\\\\n\\\\r\\\\t\\\\v])+[ \\f\\n\\r\\t\\v]+limit[ \\f\\n\\r\\t\\v]+\\\\d+[ \\f\\n\\r\\t\\v]*,[ \\f\\n\\r\\t\\v]*(\\\\d+)$\"; private static Pattern p1 = Pattern.compile(LIMIT_REPR_1); private static Pattern p2 = Pattern.compile(LIMIT_REPR_2); public static String rewriteForPrestoSql(String querySql, int queryLimit) &#123; String result = matchPatter_1(querySql, queryLimit); if (result != null) return result; return querySql + \" limit \" + queryLimit; &#125; public static String rewriteForHiveSql(String querySql, int queryLimit) &#123; String result = matchPatter_1(querySql, queryLimit); if (result != null) return result; //匹配第二种情况 result = matchPatter_2(querySql, queryLimit); if (result != null) return result; return querySql + \" limit \" + queryLimit; &#125; private static String matchPatter_1(String querySql, int queryLimit) &#123; String result = resolveMatcher(querySql, queryLimit, p1.matcher(querySql.toLowerCase()), 2); if (result != null) return result; return null; &#125; private static String matchPatter_2(String querySql, int queryLimit) &#123; String result = resolveMatcher(querySql, queryLimit, p2.matcher(querySql.toLowerCase()), 2); if (result != null) return result; return null; &#125; private static String resolveMatcher(String querySql, int queryLimit, Matcher m, int group) &#123; if (m.matches()) &#123; int idx = m.start(group); int limit = Integer.parseInt(m.group(group)); if (limit &gt; queryLimit) &#123; return querySql.substring(0, idx) + \" \" + queryLimit; &#125; return querySql; &#125; return null; &#125;&#125; 验证123456789101112131415161718192021/** * Created by luopeng on 2017/7/26. */public class MyTest &#123; public static void main(String[] args) &#123; String sql = &quot;select * from (select a.id as userid,a.name as username from a left join b on a.id = b.userid order by a.id limit &quot; + &quot;10000) limit 300&quot;; //未超出限制，返回原语句 System.out.println(QuerySqlLimiter.rewriteForPrestoSql(sql,500)); //超出限制，修改成最大限制 System.out.println(QuerySqlLimiter.rewriteForPrestoSql(sql,50)); sql = &quot;select * from (select a.id as userid,a.name as username from a left join b on a.id = b.userid order by a.id limit &quot; + &quot;10000) limit 0,300&quot;; //未超出限制，返回原语句 System.out.println(QuerySqlLimiter.rewriteForPrestoSql(sql,500)); //超出限制，修改成最大限制 System.out.println(QuerySqlLimiter.rewriteForPrestoSql(sql,50)); &#125;&#125; 输出： select from (select a.id as userid,a.name as username from a left join b on a.id = b.userid order by a.id limit 10000) limit 300select from (select a.id as userid,a.name as username from a left join b on a.id = b.userid order by a.id limit 10000) limit 50select from (select a.id as userid,a.name as username from a left join b on a.id = b.userid order by a.id limit 10000) limit 0,300select from (select a.id as userid,a.name as username from a left join b on a.id = b.userid order by a.id limit 10000) limit 0, 50 2017-08-02更新 在上线过后的小段时间内线上服务器出现了栈溢出java.lang.StackOverflowError的情况。 注意StackOverflowError（A note about the StackOverflowError） 有时regex包中的Pattern类会抛出StackOverflowError。这是已知的bug #5050507的表现，它自从Java 1.4就存在于java.util.regex包中。这个bug仍然存在，因为它是“won’t fix”的状态。这个错误的出现是因为，Pattern类把一个正则表达式编译为一个用来寻找匹配的小程序。这个程序被递归调用，有时太多的递归就会导致该错误的出现。更多细节请参考bug描述。看起来大部分是在使用选择（alternation）的出现。 如果你碰到了这个错误，尝试重写正则表达式，或者分为几个子表达式，然后分别单独执行。后者有时设置会提高性能。 同时CPU开始飙高到了90%，NFA的回缩确实被验证了。以下SQL是导致SQL飙高的其中一条： select t1.userid,t1.keyword,c.cityname,b.usermobile from( select t.userid,substring(visit_time,1,10) as visit_timeed,current_path, split_part(split_part(url_extract_query(current_url),&#39;kw=&#39;,2),&#39;&amp;&#39;,1) as keyword from hive.bdc_dwd.dw_fact_galog_pv_daily t where acct_day &gt;= &#39;2017-01-01&#39; and acct_day &lt;= &#39;2017-08-01&#39; and ((t.current_domainname=&#39;search.xxx.com&#39; and current_path like &#39;%kw=%&#39;)or(t.current_domainname=&#39;list.xxx.com&#39; and current_path like &#39;%key=%&#39;)) )t1 left join (select user_id,usermobile from hive.bdc_dwd.dw_mb_account_p group by user_id,usermobile)b on t1.userid=b.user_id left join (select user_id,cityname from hive.bdc_dwd.dw_mb_info group by user_id,cityname)c on c.user_id=t1.userid where t1.keyword in(&#39;app开发&#39;,&#39;微信开发&#39;,&#39;软件开发&#39;,&#39;UI设计&#39;,&#39;手机游戏开发&#39;,&#39;商城开发&#39;,&#39;办公系统开发&#39;,&#39;餐饮系统&#39;,&#39;教育&#39;,&#39;直播&#39;,&#39;支付系统&#39;,&#39;酒店系统开发&#39;,&#39;电商系统&#39;) group by t1.userid,t1.keyword,c.cityname,b.usermobile 出于对正则表达式的不精通，这样处理下去可能会有其它的问题出现，所以决定换一种解决思路： 对语句进行整形，去除一些分析干扰，比如将语句进行trim去掉两端空白，去掉语句最后的’;’分号等 检测sql中是否有limit关键字，这个可以使用Java字符串的lastIndexOf，这种确定性搜索还是相当高效的 当没有检索到limit关键字时，就可以直接在语句后添加Limit并加上限制数量了 当有检索到limit关键字后，需要判断limit后是否是形如limit xxx 或limit xxx,xxx的形式，这里需要注意的是这期间可能会有空格，回车换行符，制表符等空白字符。这个时候可以使用正则表达式，这个时候正则的匹配范围就变得很小很小了，效率很高 当上一步匹配到limit的形式后，就可以用以前的办法，对limit进行比较和替换了 最新实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172 /** * SQL查询Limit改写器 * Created by luopeng on 2017/8/2. */public class QuerySqlLimiter &#123; public static final List&lt;Character&gt; SPLIT_CHARS = Arrays.asList(' ', '1', '2', '3', '4', '5', '6', '7', '8', '9', '0',',', '\\n', '\\r', '\\t'); public static final String LIMIT_NUM_REPR_1 = \"^[ \\\\f\\\\n\\\\r\\\\t\\\\v]+(\\\\d+)$\"; public static final String LIMIT_NUM_REPR_2 = \"^[ \\\\f\\\\n\\\\r\\\\t\\\\v]+(\\\\d+)[ \\\\f\\\\n\\\\r\\\\t\\\\v]*,[ \\\\f\\\\n\\\\r\\\\t\\\\v]*(\\\\d+)$\"; private static Pattern p3 = Pattern.compile(LIMIT_NUM_REPR_1); private static Pattern p4 = Pattern.compile(LIMIT_NUM_REPR_2); private static String keyword = \"limit\"; public static String rewriteSql(String querySql, int queryLimit) &#123; String originSql = querySql; querySql = trimDelimiter(querySql).toLowerCase(); int length = querySql.length(); int limitIdx = StringUtils.lastIndexOf(querySql, keyword); if (limitIdx == -1) &#123; //直接添加limit return originSql + \" LIMIT \" + queryLimit; &#125; int limitIdxEnd = limitIdx + keyword.length(); for (int i = limitIdxEnd; i &lt; length - 1; ++i) &#123; char c = querySql.charAt(i); if (!SPLIT_CHARS.contains(c)) &#123; return originSql + \" LIMIT \" + queryLimit; &#125; &#125; //当以上的遍历完成时，证明存在LIMIT结尾，通过substring进一步分析limit数量 String limitStr = StringUtils.substring(querySql, limitIdxEnd); Matcher matcher = p3.matcher(limitStr); if (matcher.matches()) &#123; int limit = Integer.parseInt(matcher.group(1)); if (limit &gt; queryLimit) &#123; return StringUtils.substring(originSql, 0,limitIdx) + \" LIMIT \" + queryLimit; &#125; else &#123; return originSql; &#125; &#125; matcher = p4.matcher(limitStr); if (matcher.matches()) &#123; int offset = Integer.parseInt(matcher.group(1)); int limit = Integer.parseInt(matcher.group(2)); if (limit &gt; queryLimit) &#123; return StringUtils.substring(originSql, 0, limitIdx) + \" LIMIT \" + offset + \",\" + queryLimit; &#125; else &#123; return originSql; &#125; &#125; //直接添加limit return originSql + \" LIMIT \" + queryLimit; &#125; private static String trimDelimiter(String querySql) &#123; querySql = StringUtils.trim(querySql); if (StringUtils.endsWith(querySql, \";\")) &#123; querySql = StringUtils.substring(querySql, 0, querySql.length() - 1); &#125; return querySql; &#125;&#125; 总结以前一直不觉得正则表达式会有什么问题，实际运用中也没有遇到和去理解正则表达式的原理。才导致出现了上面所说的问题，有时候采用比正则表达式更原始更粗暴的方式解决问题可能反而会更加高效，这也许也是自己不精通正则表达式编写的问题，后续需要多多关注。","categories":[{"name":"原创文章","slug":"原创文章","permalink":"http://www.lpnote.com/categories/原创文章/"}],"tags":[{"name":"参与开源","slug":"参与开源","permalink":"http://www.lpnote.com/tags/参与开源/"},{"name":"SQL","slug":"SQL","permalink":"http://www.lpnote.com/tags/SQL/"}]},{"title":"基于Apache Ranger的Presto数据权限控制","slug":"ranger-based-user-data-authority-control-for-presto","date":"2017-07-13T16:00:00.000Z","updated":"2019-02-13T08:59:06.536Z","comments":true,"path":"2017/07/13/ranger-based-user-data-authority-control-for-presto/","link":"","permalink":"http://www.lpnote.com/2017/07/13/ranger-based-user-data-authority-control-for-presto/","excerpt":"","text":"我司大数据项目数据查询引擎使用到了presto，但是对于presto本身来说并没有人员及数据权限粒度的权限控制，我在上面做了一层封装。 Apache Ranger权限控制Apache Ranger是Apache开源的一款Hadoop生态的安全管理框架，它可以管理诸如：HDFS、HBASE、YARN等的权限访问和控制，提供了统一的权限管理后台，可以实施在线的权限变更和访问日志审计。具体怎么使用Ranger进行权限控制及自定义服务的新增，可以参考以前的一篇文章：Ranger自定义插件开发 sql解析引擎使用Druid解析SQL解析引擎最开始使用的是Druid开源SQL解析引擎，相对来说它对于SQL的解析也是比较简单的，在我们的场景下，我们需要控制人员能够访问到大数据仓库的catalog/schema/table/column级，所以我们需要将sql中的涉及到的表和列需要解析出来，而Druid可以满足我们，虽然Druid提供了Oracle/Mysql/Postgresql等传统关系数据库的sql解析，但是并没有一些开源nosql的类sql解析，但像presto这样的开源数据查询引擎本身支持标准的sql语句，所以对于我们来说大部分的sql它是可以进行标准sql解析的，所以我们就采用了它来进行sql解析的工作。 一切其实都进行得比较顺利，期间也出现过一些小问题最后都比较好的解决，其中有一个比较大的问题是发现了一个解析复杂sql语句的bug，该bug我已经提交给温少，详细的issue地址如下：复杂sql解析不正确的问题。该bug主要的问题是在存在union查询时，如果两边的子句中不同的表使用了相同的别名，在最后的解析结果时会出现混乱，表和列的对应关系会出现问题，终其原因是应该是在解析器的工作状态下只存在了一张映射表，而且该表没有命名空间的概念，导致有不同命名空间(即不同子句下)的表及别名产生了相互覆盖和影响。 最新更新： 作者已经修复bug，最新版本中1.1.2中作者已经声称修复了该bug，但是我们已经换作自己的解析器工作，所以并没有及时去验证 使用自研解析Presto数据查询引擎实际使用的是Antlr进行自定义的SQL解析，它本身提供了一些AST的节点信息Java源码，所以我只需要将SQL传入它的解析器得到一个Statement，根据Statement的详细情况遍历其节点及子节点并解析其内容就要吧得到相关的数据表和列，解析过程中我同样根据AST的路径重新生成了一棵树，通过遍历树就可以知道表别名及真实表的对应关系，以及字段和数据表的真实对应关系。 这里值得一提的是，虽然SQL解析的问题解决了，但是我们真正要解决的问题是得到非常准确的SQL中涉及到的表和字段。这里我们需要解决的是诸如：select from a,b这样的字段解析，因为为表所有的字段，这里有两张表，所以解析的时候要注意到两张表的字段都解析出来，还有一个地方要注意的是：select id from a,b， 在该情况下id并不确定它是属于a表还是b表，所以这里对于纯的sql解析还是存在困难的，对于具体权限的解析还要涉及对应于数据源的元数据schema结合来进行判断id字段应该是属于哪张表还是两张表都有，这样才能将id归属到对应表。 权限控制在应用程序中通过对SQL的语法解析，得到了数据表(形如: catalog.schema.table)和数据列的对应关系，而我们在ranger中新定义了一种服务类型，并通过新增ranger插件的形式与应用程序连通。应用程序通过Ranger提供的插件SDK，通过相关的接口将用户名，数据表和数据列传入，Ranger插件结合从Ranger服务器得到的权限及策略进行匹配，如果匹配成功则返回成功响应，应用程序收到权限允许的响应后就放行用户的查询，如果权限验证失败，则提示用户查询权限被禁止。 总结Presto本身并未提供用户SQL查询权限的控制，我们实现的方式相对来说比较简单，我们自身提供了一层WEB层用于用户进行数据查询，所以所有的权限控制都做在了这WEB层，只要这WEB层控制住了用户的权限，那么在执行引擎之前就可以拦截非法的SQL执行请求。","categories":[{"name":"原创文章","slug":"原创文章","permalink":"http://www.lpnote.com/categories/原创文章/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.lpnote.com/tags/大数据/"},{"name":"bug","slug":"bug","permalink":"http://www.lpnote.com/tags/bug/"},{"name":"druid","slug":"druid","permalink":"http://www.lpnote.com/tags/druid/"}]},{"title":"自动清理Solr Doc & Ranger Audits","slug":"solr-ttl-auto-purging-solr-documents-ranger-audits","date":"2017-07-11T16:00:00.000Z","updated":"2019-02-13T08:59:06.536Z","comments":true,"path":"2017/07/11/solr-ttl-auto-purging-solr-documents-ranger-audits/","link":"","permalink":"http://www.lpnote.com/2017/07/11/solr-ttl-auto-purging-solr-documents-ranger-audits/","excerpt":"本文原文：Solr TTL - Auto-Purging Solr Documents &amp; Ranger Audits 背景最近公司的使用的大数据项目中频繁出现以下的报错: ERROR [org.apache.ranger.audit.queue.AuditBatchQueue0] o.a.s.c.solrj.impl.CloudSolrClient:924- Request to collection ranger_audits failed due to (400) org.apache.solr.client.solrj.impl.HttpSolrClient$RemoteSolrException: Error from server at http://172.18.2.6:8886/solr/ranger_audits_shard1_replica1: Exception writing document id bc8d53c6-0878-450c-a9b0-1b8d34c55e1d to the index; possible analysis error: number of documents in the index cannot exceed 2147483519, retry? 0 搜索了一下原因，大致是说solr分片的索引量不能超过最大值（2的32次方）。由于线上solr是单机版本，所以数据量上已经超了，为了解决这个问题，我搜索到了下面这篇文章。 这篇文章将讲述如何在Solr文档上使用TTL(time to live)来自动清理过期文档。 这篇文章我们将关注如何自动地从Solr集合中删除文档，关于solr文档TTL的问题如下： 节点磁盘满 公司策略要求删除旧的审计日志(audit logs) 自动清理等 SOLR-5795介绍了一个新的UpdateProcesser叫做DocExpirationUpdateProcessorFactory,它允许我们在solr文档上添加一个过期时间，并且确保过期的文档能够自动的被清除。","text":"本文原文：Solr TTL - Auto-Purging Solr Documents &amp; Ranger Audits 背景最近公司的使用的大数据项目中频繁出现以下的报错: ERROR [org.apache.ranger.audit.queue.AuditBatchQueue0] o.a.s.c.solrj.impl.CloudSolrClient:924- Request to collection ranger_audits failed due to (400) org.apache.solr.client.solrj.impl.HttpSolrClient$RemoteSolrException: Error from server at http://172.18.2.6:8886/solr/ranger_audits_shard1_replica1: Exception writing document id bc8d53c6-0878-450c-a9b0-1b8d34c55e1d to the index; possible analysis error: number of documents in the index cannot exceed 2147483519, retry? 0 搜索了一下原因，大致是说solr分片的索引量不能超过最大值（2的32次方）。由于线上solr是单机版本，所以数据量上已经超了，为了解决这个问题，我搜索到了下面这篇文章。 这篇文章将讲述如何在Solr文档上使用TTL(time to live)来自动清理过期文档。 这篇文章我们将关注如何自动地从Solr集合中删除文档，关于solr文档TTL的问题如下： 节点磁盘满 公司策略要求删除旧的审计日志(audit logs) 自动清理等 SOLR-5795介绍了一个新的UpdateProcesser叫做DocExpirationUpdateProcessorFactory,它允许我们在solr文档上添加一个过期时间，并且确保过期的文档能够自动的被清除。 如何做？每个被索引的文档都有一个表示文档过期时间的字段（_ttle_一般情况下是这个字段，当然你也可以修改这个字段的名字），该字段标识了该文档将在什么时候过期。这个字段是由文档被索引的时间的相对时间确定的，_ttle_设置了文档的生命周期，例如：+10DAYS,+2WEEKS,+4HOURS等等。 例如： 当前时间为: 2016-10-26 20:14:00ttle定义为：+2HOURS那么过期时间将会是2016-10-26 22:14:00 deleteByQuery将会根据autoDeletePeriodSeconds的配置来触发，例如：86400，那么后台的一个线程将会将每天的方式执行删除操作(1天等于86400秒)。 这些deleteByQuery将会删除所有的过期时间小于当前时间的文档。 如果你想要自定义删除程序，你可以使用autoDeleteChainName来配置你自己的updateRequestProcessorChain,这个配置是对所有的删除生效的。 一旦删除程序完成，一个软提交将被触发，那么所有的过期的文档将不会出现在查询结果中了。 通常情况下的Solr首先，来看看一个使用ttl的普通例子，在此我们使用一个叫做moveis的solr集合，并且我们希望在这个集合中的电影保存时间不超过10天。 找到solrCloud的一个节点（不用关心哪个节点，但是该节点上需要有zkcli客户端） 创建一个solr的初始化配置： 1234mkdir /opt/lucidworks-hdpsearch/solr_collectionsmkdir /opt/lucidworks-hdpsearch/solr_collections/filmschown -R solr:solr /opt/lucidworks-hdpsearch/solr_collectionscp -R /opt/lucidworks-hdpsearch/solr/server/solr/configsets/basic_configs/conf /opt/lucidworks-hdpsearch/solr_collections/films 调整schema.xml此例中文件在/opt/lucidworks-hdpsearch/solr_collections/films/conf下面 在schema.xml中添加以下字段的定义： 1234&lt;field name=\"directed_by\" type=\"string\" indexed=\"true\" stored=\"true\" multiValued=\"true\"/&gt;&lt;field name=\"name\" type=\"text_general\" indexed=\"true\" stored=\"true\"/&gt;&lt;field name=\"initial_release_date\" type=\"string\" indexed=\"true\" stored=\"true\"/&gt;&lt;field name=\"genre\" type=\"string\" indexed=\"true\" stored=\"true\" multiValued=\"true\"/&gt; 同时添加以下字段用于自动清理 12&lt;field name=\"_ttl_\" type=\"string\" indexed=\"true\" multiValued=\"false\" stored=\"true\" /&gt;&lt;field name=\"expire_at\" type=\"date\" multiValued=\"false\" indexed=\"true\" stored=\"true\" /&gt; _ttl_表示这个文档应该保存多长时间（例如：+10DAYS）expire_at表示计算好的过期时间（索引时间+ttl） 调整solrconfig.xml为了让过期时间能被自动计算，我们需要添加DocExpirationUpdateProcessorFactory 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758&lt;updateRequestProcessorChain name=\"add-unknown-fields-to-the-schema\"&gt; &lt;processor class=\"solr.DefaultValueUpdateProcessorFactory\"&gt; &lt;str name=\"fieldName\"&gt;_ttl_&lt;/str&gt; &lt;str name=\"value\"&gt;+14DAYS&lt;/str&gt; &lt;/processor&gt; &lt;processor class=\"solr.processor.DocExpirationUpdateProcessorFactory\"&gt; &lt;int name=\"autoDeletePeriodSeconds\"&gt;300&lt;/int&gt; &lt;str name=\"ttlFieldName\"&gt;_ttl_&lt;/str&gt; &lt;str name=\"expirationFieldName\"&gt;expire_at&lt;/str&gt; &lt;/processor&gt; &lt;processor class=\"solr.RemoveBlankFieldUpdateProcessorFactory\"/&gt; &lt;processor class=\"solr.ParseBooleanFieldUpdateProcessorFactory\"/&gt; &lt;processor class=\"solr.ParseLongFieldUpdateProcessorFactory\"/&gt; &lt;processor class=\"solr.ParseDoubleFieldUpdateProcessorFactory\"/&gt; &lt;processor class=\"solr.ParseDateFieldUpdateProcessorFactory\"&gt; &lt;arr name=\"format\"&gt; &lt;str&gt;yyyy-MM-dd'T'HH:mm:ss.SSSZ&lt;/str&gt; &lt;str&gt;yyyy-MM-dd'T'HH:mm:ss,SSSZ&lt;/str&gt; &lt;str&gt;yyyy-MM-dd'T'HH:mm:ss.SSS&lt;/str&gt; &lt;str&gt;yyyy-MM-dd'T'HH:mm:ss,SSS&lt;/str&gt; &lt;str&gt;yyyy-MM-dd'T'HH:mm:ssZ&lt;/str&gt; &lt;str&gt;yyyy-MM-dd'T'HH:mm:ss&lt;/str&gt; &lt;str&gt;yyyy-MM-dd'T'HH:mmZ&lt;/str&gt; &lt;str&gt;yyyy-MM-dd'T'HH:mm&lt;/str&gt; &lt;str&gt;yyyy-MM-dd HH:mm:ss.SSSZ&lt;/str&gt; &lt;str&gt;yyyy-MM-dd HH:mm:ss,SSSZ&lt;/str&gt; &lt;str&gt;yyyy-MM-dd HH:mm:ss.SSS&lt;/str&gt; &lt;str&gt;yyyy-MM-dd HH:mm:ss,SSS&lt;/str&gt; &lt;str&gt;yyyy-MM-dd HH:mm:ssZ&lt;/str&gt; &lt;str&gt;yyyy-MM-dd HH:mm:ss&lt;/str&gt; &lt;str&gt;yyyy-MM-dd HH:mmZ&lt;/str&gt; &lt;str&gt;yyyy-MM-dd HH:mm&lt;/str&gt; &lt;str&gt;yyyy-MM-dd&lt;/str&gt; &lt;/arr&gt; &lt;/processor&gt; &lt;processor class=\"solr.AddSchemaFieldsUpdateProcessorFactory\"&gt; &lt;str name=\"defaultFieldType\"&gt;text_general&lt;/str&gt; &lt;lst name=\"typeMapping\"&gt; &lt;str name=\"valueClass\"&gt;java.lang.Boolean&lt;/str&gt; &lt;str name=\"fieldType\"&gt;booleans&lt;/str&gt; &lt;/lst&gt; &lt;lst name=\"typeMapping\"&gt; &lt;str name=\"valueClass\"&gt;java.util.Date&lt;/str&gt; &lt;str name=\"fieldType\"&gt;tdates&lt;/str&gt; &lt;/lst&gt; &lt;lst name=\"typeMapping\"&gt; &lt;str name=\"valueClass\"&gt;java.lang.Long&lt;/str&gt; &lt;str name=\"valueClass\"&gt;java.lang.Integer&lt;/str&gt; &lt;str name=\"fieldType\"&gt;tlongs&lt;/str&gt; &lt;/lst&gt; &lt;lst name=\"typeMapping\"&gt; &lt;str name=\"valueClass\"&gt;java.lang.Number&lt;/str&gt; &lt;str name=\"fieldType\"&gt;tdoubles&lt;/str&gt; &lt;/lst&gt; &lt;/processor&gt; &lt;processor class=\"solr.LogUpdateProcessorFactory\"/&gt; &lt;processor class=\"solr.RunUpdateProcessorFactory\"/&gt; &lt;/updateRequestProcessorChain&gt; 同时要确保处理链在索引的每次更新都触发： 123456&lt;initParamspath=\"/update/**,/query,/select,/tvrh,/elevate,/spell\"&gt; &lt;lst name=\"defaults\"&gt; &lt;str name=\"df\"&gt;text&lt;/str&gt; &lt;str name=\"update.chain\"&gt;add-unknown-fields-to-the-schema&lt;/str&gt; &lt;/lst&gt;&lt;/initParams&gt; 上传配置，创建集合，并索引示例数据。 12345/opt/lucidworks-hdpsearch/solr/server/scripts/cloud-scripts/zkcli.sh -zkhost horton0.example.com:2181/solr -cmd upconfig -confname films -confdir /opt/lucidworks-hdpsearch/solr_collections/films/confcurl --negotiate -u : &quot;http://horton0.example.com:8983/solr/admin/collections?action=CREATE&amp;name=films&amp;numShards=1&quot;curl --negotiate -u : &apos; http://horton0.example.com:8983/solr/films/update/json?commit=true&apos; --data-binary @/opt/lucidworks-hdpsearch/solr/example/films/films.json -H &apos;Content-type:application/json&apos; 查询单个文档：1curl --negotiate -u : http://horton0.example.com:8983/solr/films/select?q=*&amp;start=0&amp;rows=1&amp;wt=json 结果如下： 123456789101112131415161718192021&#123; \"id\":\"/en/45_2006\", \"directed_by\":[ \"Gary Lennon\" ], \"initial_release_date\":\"2006-11-30\", \"genre\":[ \"Black comedy\", \"Thriller\", \"Psychological thriller\", \"Indie film\", \"Action Film\", \"Crime Thriller\", \"Crime Fiction\", \"Drama\" ], \"name\":\".45\", \"_ttl_\":\"+10DAYS\", \"expire_at\":\"2016-11-06T05:46:46.565Z\", \"_version_\":1549320539674247200&#125; Ranger审计日志的管理Ranger的审计日志可以保存在自定义的SolrCloud中，也可以保存在由Ambari Infra提供的SolrCloud中。 Ambari Infra是一项新服务，包括其自己的Solr实例，例如 存储Ranger审核或Atlas详细信息。 由于HDP 2.5审计日志已经正式由数据库转移到Solr。 在Ranger审核中，Solr（以及DB）只是一个短期存储，基本上它仅用于Ranger Admin UI中显示的审计信息。 审计长期存档应存储在HDFS或类似的内容中。 默认情况下，Ranger Solr Audit Collection附带预配置的TTL，因此Solr中的所有Ranger Audits将在90天后立即被删除。 如果您只想将审核日志存储30天或一周，会发生什么？ 看看下面的段落:) 全新安装-Solr审计日志处于关闭如果您以前没有使用过Solr Audits，还没有通过Ambari启用Ranger Audits to Solr，那么很容易调整TTL配置。 转到您的Ranger Admin节点并执行以下命令： 1sed -i &apos;s/+90DAYS/+30DAYS/g&apos; /usr/hdp/2.5.0.0-1245/ranger-admin/contrib/solr_for_audit_setup/conf/solrconfig.xml 之后，您可以访问Ambari并启用Ranger Solr Audits，将要创建的集合将使用新的设置。 审计样例： 1234567891011121314151617181920212223&#123; \"id\":\"5519e650-440b-4c14-ace5-c1b79ee9f3d5-47734\", \"access\":\"READ_EXECUTE\", \"enforcer\":\"hadoop-acl\", \"repo\":\"bigdata_hadoop\", \"reqUser\":\"mapred\", \"resource\":\"/mr-history/tmp\", \"cliIP\":\"127.0.0.1\", \"logType\":\"RangerAudit\", \"result\":1, \"policy\":-1, \"repoType\":1, \"resType\":\"path\", \"reason\":\"/mr-history/tmp\", \"action\":\"read\", \"evtTime\":\"2016-10-26T05:14:21.686Z\", \"seq_num\":71556, \"event_count\":1, \"event_dur_ms\":0, \"_ttl_\":\"+30DAYS\", \"_expire_at_\":\"2016-11-25T05:14:23.107Z\", \"_version_\":1549227904852820000&#125; 过期时间已经自动变成了30天。 已经存在的一个Solr集群-Solr审计日志处于开启如果您已经将Ranger Audit日志启用到Solr并且已经在Solr集合中收集了大量文档，则可以通过以下步骤调整TTL。 但是，重要的是要记住，这不影响旧文档，只会影响新文档。 转到托管Solr实例的Ambari Infra节点（再次，具有zkcli客户端的任何节点） 从zookeeper上下载solrconfig.xml1/usr/lib/ambari-infra-solr/server/scripts/cloud-scripts/zkcli.sh --zkhost horton0.example.com:2181 -cmd getfile /infra-solr/configs/ranger_audits/solrconfig.xml solrconfig.xml 编辑solrconfig.xml文件1sed -i &apos;s/+90DAYS/+14DAYS/g&apos; solrconfig.xml 上传solrconfig.xml文件到zookeeper上1/usr/lib/ambari-infra-solr/server/scripts/cloud-scripts/zkcli.sh --zkhost horton0.example.com:2181 -cmd putfile /infra-solr/configs/ranger_audits/solrconfig.xml solrconfig.xml 重新加载配置1curl -v --negotiate -u : &quot;http://horton0.example.com:8983/solr/admin/collections?action=RELOAD&amp;name=ranger_audits&quot; 审计日志示例： 1234567891011121314151617181920212223&#123; \"id\":\"5519e650-440b-4c14-ace5-c1b79ee9f3d5-47742\", \"access\":\"READ_EXECUTE\", \"enforcer\":\"hadoop-acl\", \"repo\":\"bigdata_hadoop\", \"reqUser\":\"mapred\", \"resource\":\"/mr-history/tmp\", \"cliIP\":\"127.0.0.1\", \"logType\":\"RangerAudit\", \"result\":1, \"policy\":-1, \"repoType\":1, \"resType\":\"path\", \"reason\":\"/mr-history/tmp\", \"action\":\"read\", \"evtTime\":\"2016-10-26T05:16:21.674Z\", \"seq_num\":71568, \"event_count\":1, \"event_dur_ms\":0, \"_ttl_\":\"+14DAYS\", \"_expire_at_\":\"2016-11-09T05:16:23.118Z\", \"_version_\":1549228030682988500&#125; 清空集合中所有文档如果要从Solr集合中删除所有文档，以下命令可能会有所帮助： 1curl -v --negotiate -u : &quot;http://horton0.example.com:8983/solr/films/update?commit=true&quot; -H &quot;Content-Type: text/xml&quot; --data-binary &quot;&lt;delete&gt;&lt;query&gt;*:*&lt;/query&gt;&lt;/delete&gt;&quot; 或者通过浏览器操作：1http://horton0.example.com:8983/solr/films/update?commit=true&amp;stream.body=&lt;delete&gt;&lt;query&gt;*:*&lt;/query&gt;&lt;/delete&gt; 参考链接 https://lucene.apache.org/solr/5_3_0/solr-core/org/apache/solr/update/processor/DocExpirationUpdateProcessorFactory.html https://cwiki.apache.org/confluence/display/solr/Update+Request+Processors","categories":[{"name":"翻译文章","slug":"翻译文章","permalink":"http://www.lpnote.com/categories/翻译文章/"}],"tags":[{"name":"ranger","slug":"ranger","permalink":"http://www.lpnote.com/tags/ranger/"},{"name":"大数据","slug":"大数据","permalink":"http://www.lpnote.com/tags/大数据/"},{"name":"solr","slug":"solr","permalink":"http://www.lpnote.com/tags/solr/"}]},{"title":"Java并发：AbstractQueuedAsynchronizer解析（二）","slug":"java-concurrency-abstract-queued-asynchronizer-part-two","date":"2017-06-28T16:00:00.000Z","updated":"2019-02-13T08:59:06.532Z","comments":true,"path":"2017/06/28/java-concurrency-abstract-queued-asynchronizer-part-two/","link":"","permalink":"http://www.lpnote.com/2017/06/28/java-concurrency-abstract-queued-asynchronizer-part-two/","excerpt":"在上一篇文章中，主要讲解了AQS的大体结构和用法，在本篇文章中则主要讲述AQS中等待队列的原理及实现。希望通过对AQS源码的解读加深自己去AQS的原理理解以及对AQS使用的熟练度。","text":"在上一篇文章中，主要讲解了AQS的大体结构和用法，在本篇文章中则主要讲述AQS中等待队列的原理及实现。希望通过对AQS源码的解读加深自己去AQS的原理理解以及对AQS使用的熟练度。 Node类介绍AbstractQueuedAsynchronizer.Node该类是CLH（Craig, Landin, and Hagersten）锁队列的一个变种。CLH锁通常用于自旋锁。对于阻塞的同步器，我们也使用相同的策略来掌握其上一个节点的线程的控制信息。每个节点都有一个status字段用来记录该节点的线程是否应该被阻塞。每当一个节点被释放后，它的下游节点将被唤醒。队列中的每个节点也将用作一个具有特定通知的监视器，该监视器保存一个等待线程。状态字段不控制线程是否是授予锁等。队列中的第一个线程将会尝试获取锁，但是只保证有权利去竞争锁，并不保证一定会获取成功。所以当线程被唤醒后重新竞争也意味着它们可能会被重新进入等待队列。 实现原理对于CLH队列的入队，你需要将新节点以原子的方式拼接到最后。对于出队，你只需要设置head指针就可以了。 队列的形式如下： 123 +------+ prev +-----+ +-----+head | | &lt;---- | | &lt;---- | | tail +------+ +-----+ +-----+ 插入一个新节点到CLH队列只需要在队列尾部tail进行单个原子操作，所以有一个简单的原子点（耗费很少的时间）从无排队到排队。同样的，出队只涉及head指针的更新。然而，节点需要更多的消耗来确定他们的后继者，部分原因是由于超时和中断而可能的取消。 prev指针（在原来的CLH锁中未使用）主要用来处理取消。如果一个节点被取消了，那么它的后继者需要重新连接到一个没有被取消的前驱节点。关于自旋锁的解释，可以参看Scott和Scherer的论文 我们同样使用一个next指针来实现阻塞机制。每个节点都保存了它自己的线程id，所以前驱节点可以根据next指针知道应该唤醒哪一个线程。后继者必须要避免和新入队的节点竞争next指针的设置。必要时通过从原子向后检查来解决这个问题，当节点的后继显示为空时更新tail。（或者说，不同的是，next指针是一个优化手段，这样我们通常不需要反向扫描。） 取消采用了一种比较保守的做法。由于我们必须轮询其它节点是否被取消，我们可以忽略取消的节点在我们前面还是后面，这取决于我们将后继节点接驳到一个没有被取消的前驱节点上，由这个前驱节点来承担后继节点的唤醒职责。 CLH队列需要一个虚拟头节点才能开始。 但是，我们不会在构造中创造它们，因为如果从来没有竞争，那么所做的这些工作都是浪费。 相反，我们只在第一次争用发生时设置头和尾指针。 等待条件的线程使用相同的节点，但是使用额外的指针。只需要使用普通的链表队列（非并发队列）来保存节点，因为它们是在排它情况下进行访问的。在等待时，将节点插入到等待队列。在唤醒时，节点被转换到待运行队列，节点的status值被用来标识该节点处于什么队列之中。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135static final class Node &#123; /** Marker to indicate a node is waiting in shared mode */ static final Node SHARED = new Node(); /** Marker to indicate a node is waiting in exclusive mode */ static final Node EXCLUSIVE = null; /** waitStatus value to indicate thread has cancelled */ static final int CANCELLED = 1; /** waitStatus value to indicate successor's thread needs unparking */ static final int SIGNAL = -1; /** waitStatus value to indicate thread is waiting on condition */ static final int CONDITION = -2; /** * waitStatus value to indicate the next acquireShared should * unconditionally propagate */ static final int PROPAGATE = -3; /** * Status field, taking on only the values: * SIGNAL: The successor of this node is (or will soon be) * blocked (via park), so the current node must * unpark its successor when it releases or * cancels. To avoid races, acquire methods must * first indicate they need a signal, * then retry the atomic acquire, and then, * on failure, block. * CANCELLED: This node is cancelled due to timeout or interrupt. * Nodes never leave this state. In particular, * a thread with cancelled node never again blocks. * CONDITION: This node is currently on a condition queue. * It will not be used as a sync queue node * until transferred, at which time the status * will be set to 0. (Use of this value here has * nothing to do with the other uses of the * field, but simplifies mechanics.) * PROPAGATE: A releaseShared should be propagated to other * nodes. This is set (for head node only) in * doReleaseShared to ensure propagation * continues, even if other operations have * since intervened. * 0: None of the above * * The values are arranged numerically to simplify use. * Non-negative values mean that a node doesn't need to * signal. So, most code doesn't need to check for particular * values, just for sign. * * The field is initialized to 0 for normal sync nodes, and * CONDITION for condition nodes. It is modified using CAS * (or when possible, unconditional volatile writes). */ volatile int waitStatus; /** * Link to predecessor node that current node/thread relies on * for checking waitStatus. Assigned during enqueuing, and nulled * out (for sake of GC) only upon dequeuing. Also, upon * cancellation of a predecessor, we short-circuit while * finding a non-cancelled one, which will always exist * because the head node is never cancelled: A node becomes * head only as a result of successful acquire. A * cancelled thread never succeeds in acquiring, and a thread only * cancels itself, not any other node. */ volatile Node prev; /** * Link to the successor node that the current node/thread * unparks upon release. Assigned during enqueuing, adjusted * when bypassing cancelled predecessors, and nulled out (for * sake of GC) when dequeued. The enq operation does not * assign next field of a predecessor until after attachment, * so seeing a null next field does not necessarily mean that * node is at end of queue. However, if a next field appears * to be null, we can scan prev's from the tail to * double-check. The next field of cancelled nodes is set to * point to the node itself instead of null, to make life * easier for isOnSyncQueue. */ volatile Node next; /** * The thread that enqueued this node. Initialized on * construction and nulled out after use. */ volatile Thread thread; /** * Link to next node waiting on condition, or the special * value SHARED. Because condition queues are accessed only * when holding in exclusive mode, we just need a simple * linked queue to hold nodes while they are waiting on * conditions. They are then transferred to the queue to * re-acquire. And because conditions can only be exclusive, * we save a field by using special value to indicate shared * mode. */ Node nextWaiter; /** * Returns true if node is waiting in shared mode. */ final boolean isShared() &#123; return nextWaiter == SHARED; &#125; /** * Returns previous node, or throws NullPointerException if null. * Use when predecessor cannot be null. The null check could * be elided, but is present to help the VM. * * @return the predecessor of this node */ final Node predecessor() throws NullPointerException &#123; Node p = prev; if (p == null) throw new NullPointerException(); else return p; &#125; Node() &#123; // Used to establish initial head or SHARED marker &#125; Node(Thread thread, Node mode) &#123; // Used by addWaiter this.nextWaiter = mode; this.thread = thread; &#125; Node(Thread thread, int waitStatus) &#123; // Used by Condition this.waitStatus = waitStatus; this.thread = thread; &#125; &#125; 在以上代码中： prev指针被用于检查waitStatus，该指针在入队的时候被设置，然后在出队的时候被置空。同时，在前驱节点被取消时，我们可以快速的通过遍历prev找到新的非取消节点作为新的前驱节点。非取消状态的前驱节点是一定存在的，因为head节点永远不会被取消：原因是一个节点成为head节点的前提是该节点成功获取到锁。一个被取消节点的线程永远不会获取到锁，并且线程只能取消属于它自己的节点，并不能取消其它不属于它的节点。 next指针被用于release释放锁的阶段。该指针在入队，前驱节点被取消时被设置，以及在出队时被置空（有助于GC回收）。入队操作直到attachment后才设置前驱节点的next字段，所以看到一个空的next字段并不一定意味着该节点在队列的结尾。 但是，如果next字段看起来是空的，我们可以从尾部反向扫描prev，以进行双重检查。被取消节点的next字段被设置为指向节点本身而不是null，这更有利于AQS的isOnSyncQueue方法。 isOnSyncQueue方法：12345678910111213141516171819202122232425262728293031323334353637/** * Returns true if a node, always one that was initially placed on * a condition queue, is now waiting to reacquire on sync queue. * @param node the node * @return true if is reacquiring */final boolean isOnSyncQueue(Node node) &#123; if (node.waitStatus == Node.CONDITION || node.prev == null) return false; if (node.next != null) // If has successor, it must be on queue return true; /* * node.prev can be non-null, but not yet on queue because * the CAS to place it on queue can fail. So we have to * traverse from tail to make sure it actually made it. It * will always be near the tail in calls to this method, and * unless the CAS failed (which is unlikely), it will be * there, so we hardly ever traverse much. */ return findNodeFromTail(node);&#125;/** * Returns true if node is on sync queue by searching backwards from tail. * Called only when needed by isOnSyncQueue. * @return true if present */private boolean findNodeFromTail(Node node) &#123; Node t = tail; for (;;) &#123; if (t == node) return true; if (t == null) return false; t = t.prev; &#125;&#125; 总结AQS中Node类提供了一个双向链表结构，分别使用prev和next指针构成双向指针。 prev指针在入队时设置，在出队时置空；next指针在入队，前驱节点被取消被设置，在出队时被置空。由该类构造成了AQS的等待队列，并在AQS中发挥重要的作用。","categories":[{"name":"原创文章","slug":"原创文章","permalink":"http://www.lpnote.com/categories/原创文章/"}],"tags":[{"name":"多线程","slug":"多线程","permalink":"http://www.lpnote.com/tags/多线程/"},{"name":"并发","slug":"并发","permalink":"http://www.lpnote.com/tags/并发/"},{"name":"源码解读","slug":"源码解读","permalink":"http://www.lpnote.com/tags/源码解读/"}]},{"title":"Java并发：AbstractQueuedSynchronizer解析（一）","slug":"java-concurrency-abstract-queued-synchronizer-part-one","date":"2017-06-23T16:00:00.000Z","updated":"2019-02-13T08:59:06.532Z","comments":true,"path":"2017/06/23/java-concurrency-abstract-queued-synchronizer-part-one/","link":"","permalink":"http://www.lpnote.com/2017/06/23/java-concurrency-abstract-queued-synchronizer-part-one/","excerpt":"注：本文基于JDK 8，全文中所有的叙述都是基于该版本。 概述AbstractQueuedSynchronizer类是JUC类库的核心实现，它是实现Java并发核心库众多并发工具的基础，基于它及它的衍生品的并发核心包括ReentrantReadWriteLock,ArrayBlockingQueue,CopyOnWriteArrayList,CountDownLatch,CyclicBarrier等等。它为实现同步锁以及相关的基于FIFO等待队列的同步器（如：semaphores,events等）提供了一个统一框架。该类被设计用来为绝大多数基于一个原子int型状态值的同步器提供有用的基础设施。子类必须实现protected方法来改变这个状态值，该状态值决定了对象是被acquired还是released。该类的其它方法提供了所有的队列和阻塞管理。子类可以维护其它的状态字段，但是只有通过使用getState,setState,compareAndSetState方法才能被同步追踪。","text":"注：本文基于JDK 8，全文中所有的叙述都是基于该版本。 概述AbstractQueuedSynchronizer类是JUC类库的核心实现，它是实现Java并发核心库众多并发工具的基础，基于它及它的衍生品的并发核心包括ReentrantReadWriteLock,ArrayBlockingQueue,CopyOnWriteArrayList,CountDownLatch,CyclicBarrier等等。它为实现同步锁以及相关的基于FIFO等待队列的同步器（如：semaphores,events等）提供了一个统一框架。该类被设计用来为绝大多数基于一个原子int型状态值的同步器提供有用的基础设施。子类必须实现protected方法来改变这个状态值，该状态值决定了对象是被acquired还是released。该类的其它方法提供了所有的队列和阻塞管理。子类可以维护其它的状态字段，但是只有通过使用getState,setState,compareAndSetState方法才能被同步追踪。 子类应该被定义为非公开的内部帮助类来实现它闭包类的同步属性。AbstractQueuedSynchronizer未实现任何同步接口。相反，它定义了一些方法比如acquireInterruptibly被用于具体的锁或者同步器来实现它们的public方法。 该类提供了两种模式：排它和共享，当处于排它模式时，其它线程的尝试请求(attempted acquires)将不会成功，而对于共享模式来说，其它线程的请求则会成功（但不是必须）。该类并不理解这些不同点当一个共享模式获取成功时，下一个等待纯种（如果存在）也必须确定是否可以也获取到。不同的模式都共享相同的FIFO队列，通常，子类只实现这两种模式中的一种，但是这两者都可以在一个例子中发挥作用，比如ReadwriteLock。只支持排它或者共享模式的子类，不用去实现那些不支持模式的方法，因为也不会用到这些方法。 该类定义了一个嵌套的ConditionObject类，该类实现了Condition接口并被用于子类方法的排它模式。该类提供了检查和监控的方法以便于使用到AbstractQueuedSynchronizer的这些类可以很方便的使用。 该类的序列化只会存储一个维护状态的原子int值，所以对于反序列化来说，反序列化对象中线程等待队列将会为空，当然子类也可以自己定义一个readObject方法用于自定义类的状态恢复。 用法使用该类作为同步器的一个基础，需要实现以下方法（这些方法可以使用getState,setState,compareAndSetState作为检测和修改同步状态的方法）： tryAcquire tryRelease tryAcquireShared tryReleaseShared isHeldExlusively 以上这些方法默认在AQS中被实现为抛出UnsupportedOperationException，这些方法的实现必须是内部线程安全的，并且应该是快速的且不被阻塞的。这些方法是唯一该类的方法，其实方法都被定义成final，因为它们的逻辑不能被继承子类修改。你也许也会发现该类从AbstractOwnableSynchronizer类继承下来的一些方法对于保持对排它同步器的线程追踪非常有帮助，同时也鼓励使用这些方法来加强对持有锁的线程的监控。 虽然该类是基于内部的一个FIFO队列，但是它不会自动的采用，排它同步器的核心采用以下形式： 获取锁（伪代码）：1234while (!tryAcquire(arg)) &#123; enqueue thread if it is not already queued; possibly block current thread;&#125; 释放锁（伪代码）：12if (tryRelease(arg)) unblock the first queued thread; 因为获取锁时检查是在入队之前，所以新的获取锁的线程会被放到等待队列的最前面。然而如果你愿意，你也可以重新定义tryAcquire或者tryAcquireShared来改写这样的规则来提供一个公平的FIFO顺序。在特殊情况下，大部分公平同步器会在hasQueuedPredecessors返回true的情况下将tryAcquire方法定义为返回false。 对于默认的取锁方式（也称之为贪婪），吞吐量和扩展性通常情况都是最高的。然而这里并不保证公平，也不保证没有饥饿的出现，更早入队的线程是可以再次争抢锁的，并且拥有和新进线程一样的机会。而且通常意义上的自旋是在被阻塞之前它们可能会多次执行tryAcquire，这些可能会穿插在其它计算之间。自旋对于排它锁持有的时间通常很短时将发挥非常大的用处，然而对于持有锁时间比较长的情况下，这样就会造成非常大的计算浪费。如果需要，你也可以提前检测hasContended或者hasQueuedThreads这些快速路径来确认锁不会被竞争。 该类提供了一个有效的易扩展的工具，凡是使用int型状态作为同步状态同时使用一个FIFO队列的的同步器都可以使用。如果这些都不能满足你的话，你也可以使用java.util.concurrent.atomic包，你自己定义的java.util.Queue实现类以及LockSupport阻塞支持来自己实现一个同步器。 例子下面这个例子是一个不可重入的排它锁实现，它使用int型的0值表示未被加锁，1值表示被加锁。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556class Mutex implements Lock, java.io.Serializable &#123; // Our internal helper class private static class Sync extends AbstractQueuedSynchronizer &#123; // Reports whether in locked state protected boolean isHeldExclusively() &#123; return getState() == 1; &#125; // Acquires the lock if state is zero public boolean tryAcquire(int acquires) &#123; assert acquires == 1; // Otherwise unused if (compareAndSetState(0, 1)) &#123; setExclusiveOwnerThread(Thread.currentThread()); return true; &#125; return false; &#125; // Releases the lock by setting state to zero protected boolean tryRelease(int releases) &#123; assert releases == 1; // Otherwise unused if (getState() == 0) throw new IllegalMonitorStateException(); setExclusiveOwnerThread(null); setState(0); return true; &#125; // Provides a Condition Condition newCondition() &#123; return new ConditionObject(); &#125; // Deserializes properly private void readObject(ObjectInputStream s) throws IOException, ClassNotFoundException &#123; s.defaultReadObject(); setState(0); // reset to unlocked state &#125; &#125; // The sync object does all the hard work. We just forward to it. private final Sync sync = new Sync(); public void lock() &#123; sync.acquire(1); &#125; public boolean tryLock() &#123; return sync.tryAcquire(1); &#125; public void unlock() &#123; sync.release(1); &#125; public Condition newCondition() &#123; return sync.newCondition(); &#125; public boolean isLocked() &#123; return sync.isHeldExclusively(); &#125; public boolean hasQueuedThreads() &#123; return sync.hasQueuedThreads(); &#125; public void lockInterruptibly() throws InterruptedException &#123; sync.acquireInterruptibly(1); &#125; public boolean tryLock(long timeout, TimeUnit unit) throws InterruptedException &#123; return sync.tryAcquireNanos(1, unit.toNanos(timeout)); &#125; &#125; 下面这个例子是一个类似CountDownLatch的Latch。 12345678910111213141516171819202122class BooleanLatch &#123; private static class Sync extends AbstractQueuedSynchronizer &#123; boolean isSignalled() &#123; return getState() != 0; &#125; protected int tryAcquireShared(int ignore) &#123; return isSignalled() ? 1 : -1; &#125; protected boolean tryReleaseShared(int ignore) &#123; setState(1); return true; &#125; &#125; private final Sync sync = new Sync(); public boolean isSignalled() &#123; return sync.isSignalled(); &#125; public void signal() &#123; sync.releaseShared(1); &#125; public void await() throws InterruptedException &#123; sync.acquireSharedInterruptibly(1); &#125; &#125;","categories":[{"name":"原创文章","slug":"原创文章","permalink":"http://www.lpnote.com/categories/原创文章/"}],"tags":[{"name":"多线程","slug":"多线程","permalink":"http://www.lpnote.com/tags/多线程/"},{"name":"并发","slug":"并发","permalink":"http://www.lpnote.com/tags/并发/"},{"name":"源码解读","slug":"源码解读","permalink":"http://www.lpnote.com/tags/源码解读/"}]},{"title":"关于Git合并多个提交的做法：rebase","slug":"usage-of-rebase-commits-in-git","date":"2017-06-14T16:00:00.000Z","updated":"2019-02-13T08:59:06.552Z","comments":true,"path":"2017/06/14/usage-of-rebase-commits-in-git/","link":"","permalink":"http://www.lpnote.com/2017/06/14/usage-of-rebase-commits-in-git/","excerpt":"在使用Git作为版本管理的过程中，在一个分支上开发很久了以后，当你回顾以前的提交时，会发现以前的提交非常杂乱而且提交的日志标注不清时，这个时候你就会非常想重新整理一下你的提交了。将多个提交日志合并成一个提交，而这个操作Git提供了这样的方法： git rebase -i","text":"在使用Git作为版本管理的过程中，在一个分支上开发很久了以后，当你回顾以前的提交时，会发现以前的提交非常杂乱而且提交的日志标注不清时，这个时候你就会非常想重新整理一下你的提交了。将多个提交日志合并成一个提交，而这个操作Git提供了这样的方法： git rebase -i 一个简单的例子： 对于一个已经存在的Git仓库，我们向其中添加一个文件：123456789$touch a.txt$git add a.txt$git commit -m \"add a.txt\"$touch b.txt$git add b.txt$git commit -m \"add b.txt\"$git add c.txt$git commit -m \"add c.txt\"$git push 那么提交历史是这样的： 1234567891011121314151617commit f03350c8981c2df08d9443a9befb6f47aeedceb5 (HEAD -&gt; test, origin/test)Author: lpwork &lt;lpwork@foxmail.com&gt;Date: Mon Jun 15 11:58:37 2017 +0800 add c.txtcommit 239c69ef6e5ac8a2716bd7f561a6b6aa8fd52074Author: lpwork &lt;lpwork@foxmail.com&gt;Date: Mon Jun 15 11:58:17 2017 +0800 add b.txtcommit 5008e5acb3b437276e3f1796de09071e1ccd7532Author: lpwork &lt;lpwork@foxmail.com&gt;Date: Mon Jun 15 11:57:33 2017 +0800 add a.txt 假如我们想要合并提交文件c.txt和b.txt的两次提交，我们的做法是： 取得合并提交的前一个提交的id，这里我们取得的是5008e5acb3b437276e3f1796de09071e1ccd7532 执行git rebase 1git rebase -i 5008e5acb3b437276e3f1796de09071e1ccd7532 显示如下：123456789101112131415161718192021pick 239c69e add b.txtpick f03350c add c.txt# Rebase 5008e5a..f03350c onto 5008e5a (2 commands)## Commands:# p, pick = use commit# r, reword = use commit, but edit the commit message# e, edit = use commit, but stop for amending# s, squash = use commit, but meld into previous commit# f, fixup = like &quot;squash&quot;, but discard this commit&apos;s log message# x, exec = run command (the rest of the line) using shell# d, drop = remove commit## These lines can be re-ordered; they are executed from top to bottom.## If you remove a line here THAT COMMIT WILL BE LOST.## However, if you remove everything, the rebase will be aborted.## Note that empty commits are commented out 这里我们选择保留如下： 123456789101112131415161718192021pick 239c69e add b.txt and c.txtsquash f03350c add c.txt# Rebase 5008e5a..f03350c onto 5008e5a (2 commands)## Commands:# p, pick = use commit# r, reword = use commit, but edit the commit message# e, edit = use commit, but stop for amending# s, squash = use commit, but meld into previous commit# f, fixup = like &quot;squash&quot;, but discard this commit&apos;s log message# x, exec = run command (the rest of the line) using shell# d, drop = remove commit## These lines can be re-ordered; they are executed from top to bottom.## If you remove a line here THAT COMMIT WILL BE LOST.## However, if you remove everything, the rebase will be aborted.## Note that empty commits are commented out :wq保存之后，得到如下： 12345678910111213141516171819202122232425# This is a combination of 2 commits.# This is the 1st commit message:add b.txt# This is the commit message #2:add c.txt# Please enter the commit message for your changes. Lines starting# with &apos;#&apos; will be ignored, and an empty message aborts the commit.## Date: Mon Aug 21 16:43:14 2017 +0800## interactive rebase in progress; onto 5008e5a# Last commands done (2 commands done):# reword 239c69e add b.txt and c.txt# squash f03350c add c.txt# No commands remaining.# You are currently rebasing branch &apos;test2&apos; on &apos;5008e5a&apos;.## Changes to be committed:# new file: b.txt# new file: c.txt# 这里就是我们要修改的合并的提交日志了。我们修改成如下：123456789101112131415161718add b.txt and c.txt# Please enter the commit message for your changes. Lines starting# with &apos;#&apos; will be ignored, and an empty message aborts the commit.## Date: Mon Aug 21 16:43:14 2017 +0800## interactive rebase in progress; onto 5008e5a# Last commands done (2 commands done):# reword 239c69e add b.txt and c.txt# squash f03350c add c.txt# No commands remaining.# You are currently rebasing branch &apos;test2&apos; on &apos;5008e5a&apos;.## Changes to be committed:# new file: b.txt# new file: c.txt# 保存并提交。再次查看git提交日志时：1234567891011commit 14d341cd6eddf4e8b04502bcf261d9becfbedd55 (HEAD -&gt; test, origin/test)Author: lpwork &lt;lpwork@foxmail.com&gt;Date: Mon Jun 15 12:23:14 2017 +0800 add b.txt and c.txtcommit 5008e5acb3b437276e3f1796de09071e1ccd7532Author: lpwork &lt;lpwork@foxmail.com&gt;Date: Mon Jun 15 11:57:33 2017 +0800 add a.txt 注意此时我们的rebase还只是在我们自己的本地进行了合并，最后还需要push到远程仓库中。 注意： 这里需要说明的是，在很大部分情况下，在本地进行了rebase操作后，push远程仓库都有可能会造成冲突而无法提交。这里的一个做法是使用强制push git push --force。注意，强制push可能会造成别人的提交丢失，请确保你在push前合并完别人的push结果git pull。 通过这样的方式，可以让你重新整理杂乱无章的提交历史，让提交历史更加清晰，同时可以清除一些无用的提交信息，提高历史查询的效率。 最后需要关注一下rebase提供的一些可选项： 1234567891011121314151617## Commands:# p, pick = use commit# r, reword = use commit, but edit the commit message# e, edit = use commit, but stop for amending# s, squash = use commit, but meld into previous commit# f, fixup = like &quot;squash&quot;, but discard this commit&apos;s log message# x, exec = run command (the rest of the line) using shell# d, drop = remove commit## These lines can be re-ordered; they are executed from top to bottom.## If you remove a line here THAT COMMIT WILL BE LOST.## However, if you remove everything, the rebase will be aborted.## Note that empty commits are commented out","categories":[{"name":"原创文章","slug":"原创文章","permalink":"http://www.lpnote.com/categories/原创文章/"}],"tags":[{"name":"git","slug":"git","permalink":"http://www.lpnote.com/tags/git/"}]},{"title":"Presto独立服务发现(Discovery Service)","slug":"discovery-service-install-of-presto","date":"2017-05-31T16:00:00.000Z","updated":"2019-02-13T08:59:06.520Z","comments":true,"path":"2017/05/31/discovery-service-install-of-presto/","link":"","permalink":"http://www.lpnote.com/2017/05/31/discovery-service-install-of-presto/","excerpt":"Presto的运行机制Presto的运行机制如下： 不管是coordinator还是worker配置项中都有一项discovery.uri,这个是一个比较核心的东西，简单来说就是服务发现的地址。 coordinator和worker都会将自身注册到这个服务发现地址上，供彼此发现对方，coordinator可以通过个发现服务知道有多少worker节点，而worker节点可以通过这个发现服务知道coordinator是谁，这样做的好处是coordinator和worker做到了完全的解耦，彼此都不需要在启动时配置对方，而是通过第三方服务来发现对方。","text":"Presto的运行机制Presto的运行机制如下： 不管是coordinator还是worker配置项中都有一项discovery.uri,这个是一个比较核心的东西，简单来说就是服务发现的地址。 coordinator和worker都会将自身注册到这个服务发现地址上，供彼此发现对方，coordinator可以通过个发现服务知道有多少worker节点，而worker节点可以通过这个发现服务知道coordinator是谁，这样做的好处是coordinator和worker做到了完全的解耦，彼此都不需要在启动时配置对方，而是通过第三方服务来发现对方。 Presto On Yarn的问题在默认的情况下这个发现服务是内嵌在coordinator中的，也就是coordinator在启动的时候会启动一个内嵌的发现服务，在这种情况下，coordinator将自身注册给自身的发现服务，而worker则将发现服务的地址配置成coordinator的发现服务地址，此时coordinator同时充当presto协调者和服务发现的提供者。 以上这种情况在一般的情况下可以良好的运行，但是当我们将presto服务迁移到Presto On Yarn时就会遇到一些问题： presto on yarn是一种动态的运行策略，在yarn上面，哪个节点运行presto的coordinator和worker是不确定的，这会给外部调用presto的程序带来困扰 外部的程序和presto的交流一般是通过presto提供的客户端来调用，而它的客户端需要事先知道presto的coordinator地址，在presto on yarn的情况下，coordinator的地址是不确定的，有可能会发生变化。 这种情况下的处理方案是：将presto的服务发现方案外置，将presto的服务发现服务独立于presto的coordinator运行，将presto的coordinator和worker中的discovery.uri配置成外部独立的发现服务地址，在外部提供具有HA的服务发现，提供稳定的发现服务。 Presto的服务发现是基于airlift的服务发现做的实现，airlift的服务发现可以在这里查看实现和源码，不过它基本是处于无文档的状态，所以理解要多花些功夫。 airlift的服务发现的总体思路是基于http提供一个提供服务发现的HA集群，集群之间通过http通信，通过数据同步方式，提供最终一致性的保证。 这里我们就来说说airlift的服务发现服务的HA安装。 Airlift Discovery安装安装步骤 下载源码git clone https://github.com/airlift/discovery.git 编译源码mvn clean package -DskipTests=true 环境安装将target目录下的discovery-server--SNAPSHOT.tar.gz安装包copy至安装机器上进行解压安装环境配置 解压后在解压目录新建etc目录，并在etc目录下新建以下配置文件 config.properties jvm.config log.properties node.properties service-inventory.json 配置文件config.properties文件为主配置文件，主要配置该discovery服务的主要配置信息，如运行环境，服务端口，节点id等信息，配置信息一般情况如下： 1234node.environment=testhttp-server.http.port=8411node.id=597A741E-9968-40E2-BB4D-7AF26DE18689service-inventory.uri=file://&lt;installation-location-of-your-discovery-service&gt;/etc/service-inventory.json node.environment指定运行环境http-server.http.port指定服务运行的端口node.id指定该节点的idservice-inventory.uri指定了该集群拥有的所有节点信息 jvm.config文件主要配置服务jvm的配置信息，该配置和presto的配置文件的jvm配置类似，一般情况按如下信息自行进行调整： 12345678-server-Xmx2G-XX:+UseG1GC-XX:G1HeapRegionSize=32M-XX:+UseGCOverheadLimit-XX:+ExplicitGCInvokesConcurrent-XX:+HeapDumpOnOutOfMemoryError-XX:OnOutOfMemoryError=kill -9 %p log.properties主要记录的日志级别调整，这里不再叙述node.properties主要记录的是节点相关的配置，类似于config.properties配置，但是不同点在于config.properties强调集群共有的特性，而node.properites强调节点间相同配置项的不同配置值区别service-inventory.json这是一个比较重要的文件，里面记录了整个集群的信息，discovery集群利用这个配置文件获取集群的所有信息，知道集群中所有部署的情况及如何与其它节点进行通信。它的配置如下： 12345678910111213141516171819202122232425262728 &#123; &quot;environment&quot;: &quot;test&quot;, &quot;services&quot;: [ &#123; &quot;id&quot;: &quot;C8A9EE64-0476-452C-8638-8E72F3EE3CA6&quot;, &quot;nodeId&quot;: &quot;597A741E-9968-40E2-BB4D-7AF26DE18689&quot;, &quot;type&quot;: &quot;discovery&quot;, &quot;pool&quot;: &quot;general&quot;, &quot;location&quot;: &quot;/172.17.31.245&quot;, &quot;state&quot;: &quot;RUNNING&quot;, &quot;properties&quot;: &#123; &quot;http&quot;: &quot;http://172.17.31.245:8411&quot; &#125; &#125;, &#123; &quot;id&quot;: &quot;370AF416-5F44-47D3-BFB6-D93A92676D49&quot;, &quot;nodeId&quot;: &quot;0BA42FDB-5DBA-4A2C-BE26-9596B7B4368E&quot;, &quot;type&quot;: &quot;discovery&quot;, &quot;pool&quot;: &quot;general&quot;, &quot;location&quot;: &quot;/172.17.31.246&quot;, &quot;state&quot;: &quot;RUNNING&quot;, &quot;properties&quot;: &#123; &quot;http&quot;: &quot;http://172.17.31.246:8411&quot; &#125; &#125; ]&#125; 以上面的配置中，集群中有两个节点，并指出了两个节点的节点id信息，以及他们的通信地址properties.http等信息，有了这份信息，集群中的各节点就知道如何同其它节点进行数据交互与同步了。 运行集群 在集群每个节点的安装目录下bin目录中运行: ./launcher start进行服务的启动， ./launcher stop 进行服务的停止 ./launcher restart 进行服务的重启 验证服务 当服务运行成功后，可以通过浏览器进行访问，若配置的端口为8411，则访问发现服务的地址为：http://localhost:8411/v1/service这个地址将返回所有注册到这个发现服务的服务的列表 高可用 因多台机器共同组成了发现服务，发现服务有最终一致性保障，所以只需要访问其中一台就可以，但是为了高可用，可以在发现服务前端加入NGINX作流量分担与负载解决高可用的问题 Presto节点信息注册到发现服务 将Presto的节点信息注册到发现服务非常简单，上面也说过了，Presto节点之前是通过自身位集群的coordinator节点充当服务发现者提供服务的，现在只需要将discovery.uri的配置换成外置的airlift服务发现服务地址就可以了用了。在这个示例中我将配置值修改成了’172.17.31.245:8411’,因为是测试环境，不需要过于要求的HA场景，所以我只配置了服务发现集群中的一个节点。 Presto的客户端集成 因Presto的客户端调用需要知道coordinator，而现在Presot On Yarn上了过后，coordinator的地址是不定的，且是注册到服务发现上的，对于Presto客户端想知道明确的coordinator地址需要做一些改变：将调用presto客户端前要先得到coordinator，而要得到coordinator可以通过服务发现获取，看了下airlift这个框架，它自身提供了服务发现的客户端的功能，但是看了有点晕眩，大致思路是实现一个http接口去定期轮询服务发现地址，得到服务地址(coordinator)就可以了，于是我自己实现了一个简易版本的，通过一个服务发现的网关地址，应用启动后通过后台线程每隔10s去轮询一次该服务发现网关，得到更新的coordirnator地址，更新本报的缓存，所有获取coordinator地址都从本地缓存中获取，避免每次的服务发现网关轮询。 目前运行情况 目前运行情况良好，充分解决了Presto On Yarn后的coordinator随时可变的情况，应用能够根据coordinator的变化随时适应变化（10s延时）及时调整，避免因coordinator的变化导致的查询应用不可用问题。 参考文档： https://prestodb.io https://prestodb.io/presto-yarn https://github.com/airlift/airlift https://github.com/airlift/discovery","categories":[{"name":"原创文章","slug":"原创文章","permalink":"http://www.lpnote.com/categories/原创文章/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.lpnote.com/tags/大数据/"},{"name":"presto","slug":"presto","permalink":"http://www.lpnote.com/tags/presto/"}]},{"title":"基于YARN-Based集群的presto自动安装(Presto On YARN)","slug":"automated-installation-on-a-yarn-based-cluster","date":"2017-05-23T16:00:00.000Z","updated":"2019-02-13T08:59:06.516Z","comments":true,"path":"2017/05/23/automated-installation-on-a-yarn-based-cluster/","link":"","permalink":"http://www.lpnote.com/2017/05/23/automated-installation-on-a-yarn-based-cluster/","excerpt":"如果你正计划使用HDP的发行版，那么你可以使用Ambari和Apache Slider来执行基于YARN的Presto的自动安装和集成，在安装过程中，Apache Slider和Presto的包都会被安装。 部署Presto到基于YARN的集群安装部署的前提是假设你有一些关于Presto的基础知识以及了解它的一些配置文件。所有的例子引用都来至于：https://github.com/prestodb/presto-yarn/ 前提条件 基于HDP 2.2+ 或者 CDH 5.4+ 的集群 Apache Slider 0.80.0（可以从这个地址下载） JDK 1.8 Zookeeper openssl &gt;= 1.0.1e-16 Ambari 2.1","text":"如果你正计划使用HDP的发行版，那么你可以使用Ambari和Apache Slider来执行基于YARN的Presto的自动安装和集成，在安装过程中，Apache Slider和Presto的包都会被安装。 部署Presto到基于YARN的集群安装部署的前提是假设你有一些关于Presto的基础知识以及了解它的一些配置文件。所有的例子引用都来至于：https://github.com/prestodb/presto-yarn/ 前提条件 基于HDP 2.2+ 或者 CDH 5.4+ 的集群 Apache Slider 0.80.0（可以从这个地址下载） JDK 1.8 Zookeeper openssl &gt;= 1.0.1e-16 Ambari 2.1 Presto安装目录结构当你使用Ambari Slider View在一个YARN集群上安装Presto的时候，Presto的安装目录不同于标准的目录，这里会有一些区别。 如果你使用Slider Scripts或者使用Ambari slider view安装Presto到YARN集群，Presto将会通过使用Presto tarball包安装（并不是rpm包）。安装发生在YARN应用被启动时并且你可以在你的YARN nodemanager节点上找到Presto server的安装目录，该目录是由yarn.nodemanager.local-dirs该参数指定的。 比如你的yarn.nodemanager.local-dirs参数指定为/mnt/hadoop/nm-local-dirs，并且app_user为yarn，那么你就会发现Presto被安装在了/mnt/hadoop-hdfs/nm-local-dir/usercache/yarn/appcache/application_/container_/app/install/presto-server-，这个路径的第一部分（直到container_id）在Slider中被称作AGENT_WORK_ROOT，那么这么来说，Presto就是被安装在AGENT_WORK_ROOT/app/install/presto-server-&lt;version&gt; 这里。 对于正常情况下的Presto安装，会将presto的catalog、plugin以及lib目录安装在presto安装的主目录下。同样的在这里，catalog目录会是在AGENT_WORK_ROOT/app/install/presto-server-&lt;version&gt;/etc/catalog，plugin和lib目录会是分别在AGENT_WORK_ROOT/app/install/presto-server-&lt;version&gt;/plugin和AGENT_WORK_ROOT/app/install/presto-server-&lt;version&gt;/lib，用于启动Presto服务的脚本会是在AGENT_WORK_ROOT/app/install/presto-server-&lt;version&gt;/bin. Presto的日志目录是基于你配置的data directory目录下的， 如果你在appConfig.json配置成/var/lib/presto/data，那么你就会得到presto的日志目录/var/lib/presto/data/var/log/ Presto安装配置选项在安装过程中，Ambari Slider View允许你设置Presto运行的必要参数。 使用Ambari Slider View来安装Presto到YARN集群上Ambari支持通过Slider View部署Slider应用包并提供Slider的集成。 Slider View for Ambari允许你通过Ambari WEB控制台部署和管理Slider应用。 使用Ambari Slider View安装Presto到YARN集群的步骤如下： 1、安装Ambari，如果没有安装，这里有一些教程 2、下载Apache Slider3、复制Presto应用包presto-yarn-package-&lt;version&gt;-&lt;presto-version&gt;.zip到/var/lib/ambari-server/resources/apps/(你的ambari服务节点上)4、重启Ambari服务器5、重新登录Ambari服务器6、配置Ambari的中间过程略过…7、配置和自定义服务，并安装它们，Slider的最小服务集是：HDFS,YARN,Zookeeper。当然你也必须选择安装Slider。8、对于Slider客户端安装，你需要更新它的配置如果你不是使用默认配置安装的Hadoop和Zookeeper。因此slider-env.sh应该需要指出你的JAVA_HOME和HADOOP_CONF_DIR export JAVA_HOME=/usr/lib/jvm/java export HADOOP_CONF_DIR=/etc/hadoop/conf9、对于Zookeeper，如果你使用了一个不同的区别于默认的/usr/lib/zookeeper的目录： 那么请在slider-client节添加一个自定义属性zk.home,值是你的zookeeper路径。 如果zookeeper不是使用的默认端口2181，那么你还需要指定slider.zookeeper.quorum,形式为node:port。 10、当所的有服务都安装完毕并且运行起来后，你就可以在Ambari中配置Slider来创建和管理你的应用了。 点击Admin（左上角）-&gt; Manage Ambari 从左侧面板中选择Views 创建Slider View，填入必要的字段。 ambari.server.url格式为：http://&lt;ambari-server-url&gt;:8080/api/v1/clusters/&lt;clustername&gt;，其中clustername是你Ambari集群的名字。 选择右上角的Views控制按钮 选择你在上一步创建的实例（例如：‘Slider’） 点击Create App来创建一个新的Presto YARN应用 11、提供Presto服务详细配置，默认情况下，UI会从*-default.json文件中计算读取，这些文件都存在于你的presto-yarn-package-*.zip文件中。12、应用名必须是小写的，例如：presto113、你可以设置一些配置值，比如你想给presto设置一个connector，那么只需要更新global.catalog的属性值就可以了，下面这个链接是对各个配置值的解释。Presto Configuration Options for YARN-Based Clusters14、为Slider准备HDFS。HDFS的用户目录应该和你的配置文件中设置的global.app_user字段内容一致。假如app_user被设置成yarn，那么操作就会像下面这样：su hdfs hdfs dfs -mkdir -p /user/yarnsu hdfs hdfs dfs -chown yarn:yarn /user/yarn15、将配置中的global.presto_server_port从8080改成其它值，比如8089，因为8080可能已经被Ambari或者其它服务占用。总之是需要找到一个可用的端口。16、提前在每个节点上创建好presto的本地目录（目录配置在appConfig-default.json中，例如：/var/lib/presto/），并且该目录的拥有者必须是global.app_user的配置值，不然Slider会因为权限问题不能成功启动Presto服务mkdir -p /var/lib/presto/datachown -R yarn:hadoop /var/lib/presto/data17、如果你想添加一些额外的配置属性，可以使用自定义属性那节，额外的属性目前支持如下： site.global.plugin site.global.additional_config_properties site.global.additional_node_properties以上的参数格式请参考(Presto Configuration Options for YARN-Based Clusters)[https://prestodb.io/presto-yarn/installation-yarn-configuration-options.html] 18、点击完成。这步等效于Slider的bin/slider脚本中’package install’以及’create’。 如果操作成功，你就会看到YARN应用被成功拉起，你可以操作如下： 点击app launched，查看Slider view的状态 点击Quick Links，这会带你到YARN的WEBUI中 19、如果部署失败了，就需要检查一下任务的历史日志了，或者看下节点的本地日志，可以参考(Debugging and Logging for YARN-Based Clusters)[https://prestodb.io/presto-yarn/installation-yarn-debugging-logging.html]20、你可以在UI界面上管理应用的生命周期（如：start,stop,flex,destroy等） 额外的配置选项当你安装完Presto和Slider后，你可以重新配置Presto的配置项或者添加更多的配置 在Slider View中重新配置Presto在你启动Presto后，你也可以更新它的配置，例如，你想添加一个新的connector。1、在Slider View的实例界面上点击Actions2、停止正在运行的Presto应用3、点击Destory来删除掉在Slider中已经存在的实例4、点击Create App按钮重新创建一个新实例，并使用新的配置值 高级配置选项 配置内存、CPU、以及YARN的CGroups 失败策略 YARN Lable更多信息，参见Advanced Configuration Options for YARN-Based Clusters 调试和日志更多信息，参见：Debugging and Logging for YARN-Based Clusters 参考链接 http://slider.incubator.apache.org/docs/getting_started.html http://docs.hortonworks.com/HDPDocuments/Ambari-2.0.1.0/bk_Installing_HDP_AMB/content/ch_Installing_Ambari.html","categories":[{"name":"翻译文章","slug":"翻译文章","permalink":"http://www.lpnote.com/categories/翻译文章/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.lpnote.com/tags/大数据/"},{"name":"presto","slug":"presto","permalink":"http://www.lpnote.com/tags/presto/"},{"name":"yarn","slug":"yarn","permalink":"http://www.lpnote.com/tags/yarn/"}]},{"title":"Fastjson关于autoType is not support问题解析","slug":"autotype-not-support-with-fastjson","date":"2017-05-11T16:00:00.000Z","updated":"2019-02-13T08:59:06.516Z","comments":true,"path":"2017/05/11/autotype-not-support-with-fastjson/","link":"","permalink":"http://www.lpnote.com/2017/05/11/autotype-not-support-with-fastjson/","excerpt":"在做大数据查询系统的过程中，需要对Presto对正在进行的查询进行管理，允许用户进行kill。今天有用户反馈线上的任务无法kill，通过查看线上日志得知在解析json的时候报了一个错：123456com.alibaba.fastjson.JSONException: autoType is not support. output at com.alibaba.fastjson.parser.ParserConfig.checkAutoType(ParserConfig.java:888) at com.alibaba.fastjson.parser.DefaultJSONParser.parseObject(DefaultJSONParser.java:325) at com.alibaba.fastjson.parser.DefaultJSONParser.parseObject(DefaultJSONParser.java:520) at com.alibaba.fastjson.parser.DefaultJSONParser.parse(DefaultJSONParser.java:1335) ... 从上图可以看出错误代码出在了：1QueryVO query = JSON.parseObject(result, QueryVO.class); 根据错误堆栈，错误发生在323行：","text":"在做大数据查询系统的过程中，需要对Presto对正在进行的查询进行管理，允许用户进行kill。今天有用户反馈线上的任务无法kill，通过查看线上日志得知在解析json的时候报了一个错：123456com.alibaba.fastjson.JSONException: autoType is not support. output at com.alibaba.fastjson.parser.ParserConfig.checkAutoType(ParserConfig.java:888) at com.alibaba.fastjson.parser.DefaultJSONParser.parseObject(DefaultJSONParser.java:325) at com.alibaba.fastjson.parser.DefaultJSONParser.parseObject(DefaultJSONParser.java:520) at com.alibaba.fastjson.parser.DefaultJSONParser.parse(DefaultJSONParser.java:1335) ... 从上图可以看出错误代码出在了：1QueryVO query = JSON.parseObject(result, QueryVO.class); 根据错误堆栈，错误发生在323行： 即在这里的判断如果json的key的内容等于‘@type’，并且并没有禁用DisableSpecialKeyDetect这个关键字检测，则会进行特殊类型反序列化处理，也就是325行，在这里fastjson自己定义了一个特殊的关键字‘@type’用于保留反序列化时类型信息，而我们返回的json内容中恰好就含有这样的关键字，fastjson当成了自己的预定义解析类型进行解析，故会报出刚才的错误。 根据以上代码的提示，可以在进行JSON解析的时候，将Feature.DisableSpecialKeyDetect传入解析器中，设置禁用关键字解析。 1QueryVO query = JSON.parseObject(result, QueryVO.class, Feature.DisableSpecialKeyDetect); 如此便解决了问题。","categories":[{"name":"原创文章","slug":"原创文章","permalink":"http://www.lpnote.com/categories/原创文章/"}],"tags":[{"name":"问题解析","slug":"问题解析","permalink":"http://www.lpnote.com/tags/问题解析/"},{"name":"fastjson","slug":"fastjson","permalink":"http://www.lpnote.com/tags/fastjson/"}]},{"title":"Hive获取查询日志的问题解析","slug":"hive-query-log-problem-analysis","date":"2017-05-04T16:00:00.000Z","updated":"2019-02-13T08:59:06.528Z","comments":true,"path":"2017/05/04/hive-query-log-problem-analysis/","link":"","permalink":"http://www.lpnote.com/2017/05/04/hive-query-log-problem-analysis/","excerpt":"需求背景最近这段时间一直在做数据查询系统的需求，最近接到一个需求：因为HIVE查询一般需要比较久的查询时间，这期间查询人员需要知道查询的进度，需要在界面上进行进度的展示。 探路过程我们查询系统连接HIVE使用的是标准的JDBC接口，在标准的JDBC接口中并没有提供这样的一个获取查询日志的接口。翻阅了很多的资料后发现其实在HIVE Server的Thrift接口中是有提供这样的接口的： 123456789101112131415161718192021222324252627282930313233343536373839404142public List&lt;String&gt; getQueryLog(boolean incremental, int fetchSize) throws SQLException, ClosedOrCancelledStatementException &#123; checkConnection(\"getQueryLog\"); if (isCancelled) &#123; throw new ClosedOrCancelledStatementException(\"Method getQueryLog() failed. The \" + \"statement has been closed or cancelled.\"); &#125; List&lt;String&gt; logs = new ArrayList&lt;String&gt;(); TFetchResultsResp tFetchResultsResp = null; try &#123; if (stmtHandle != null) &#123; TFetchResultsReq tFetchResultsReq = new TFetchResultsReq(stmtHandle, getFetchOrientation(incremental), fetchSize); tFetchResultsReq.setFetchType((short)1); tFetchResultsResp = client.FetchResults(tFetchResultsReq); Utils.verifySuccessWithInfo(tFetchResultsResp.getStatus()); &#125; else &#123; if (isQueryClosed) &#123; throw new ClosedOrCancelledStatementException(\"Method getQueryLog() failed. The \" + \"statement has been closed or cancelled.\"); &#125; if (isExecuteStatementFailed) &#123; throw new SQLException(\"Method getQueryLog() failed. Because the stmtHandle in \" + \"HiveStatement is null and the statement execution might fail.\"); &#125; else &#123; return logs; &#125; &#125; &#125; catch (SQLException e) &#123; throw e; &#125; catch (Exception e) &#123; throw new SQLException(\"Error when getting query log: \" + e, e); &#125; RowSet rowSet = RowSetFactory.create(tFetchResultsResp.getResults(), connection.getProtocol()); for (Object[] row : rowSet) &#123; logs.add(String.valueOf(row[0])); &#125; return logs; &#125; 以上取至HIVE的JDBC接口实现HiveStatement这个类。这个类是标准java.sql.Statement的实现，但是getQueryLog这个方法并不是标准的JDBC方法，因为在我们的程序中运行的就是HIVE查询，所以我们可以在程序中进行强转得到HiveStatement这个类并调用这个方法获取到查询日志。 getQueryLog 这个方法中用到了整个HiveStatement中的一些变量，所以我们要进行HIVE查询日志的获取必须要对HiveStatement对象进行关联，同时一边在执行HIVE查询，一边还要从另一个线程中获取HIVE查询的日志过程。","text":"需求背景最近这段时间一直在做数据查询系统的需求，最近接到一个需求：因为HIVE查询一般需要比较久的查询时间，这期间查询人员需要知道查询的进度，需要在界面上进行进度的展示。 探路过程我们查询系统连接HIVE使用的是标准的JDBC接口，在标准的JDBC接口中并没有提供这样的一个获取查询日志的接口。翻阅了很多的资料后发现其实在HIVE Server的Thrift接口中是有提供这样的接口的： 123456789101112131415161718192021222324252627282930313233343536373839404142public List&lt;String&gt; getQueryLog(boolean incremental, int fetchSize) throws SQLException, ClosedOrCancelledStatementException &#123; checkConnection(\"getQueryLog\"); if (isCancelled) &#123; throw new ClosedOrCancelledStatementException(\"Method getQueryLog() failed. The \" + \"statement has been closed or cancelled.\"); &#125; List&lt;String&gt; logs = new ArrayList&lt;String&gt;(); TFetchResultsResp tFetchResultsResp = null; try &#123; if (stmtHandle != null) &#123; TFetchResultsReq tFetchResultsReq = new TFetchResultsReq(stmtHandle, getFetchOrientation(incremental), fetchSize); tFetchResultsReq.setFetchType((short)1); tFetchResultsResp = client.FetchResults(tFetchResultsReq); Utils.verifySuccessWithInfo(tFetchResultsResp.getStatus()); &#125; else &#123; if (isQueryClosed) &#123; throw new ClosedOrCancelledStatementException(\"Method getQueryLog() failed. The \" + \"statement has been closed or cancelled.\"); &#125; if (isExecuteStatementFailed) &#123; throw new SQLException(\"Method getQueryLog() failed. Because the stmtHandle in \" + \"HiveStatement is null and the statement execution might fail.\"); &#125; else &#123; return logs; &#125; &#125; &#125; catch (SQLException e) &#123; throw e; &#125; catch (Exception e) &#123; throw new SQLException(\"Error when getting query log: \" + e, e); &#125; RowSet rowSet = RowSetFactory.create(tFetchResultsResp.getResults(), connection.getProtocol()); for (Object[] row : rowSet) &#123; logs.add(String.valueOf(row[0])); &#125; return logs; &#125; 以上取至HIVE的JDBC接口实现HiveStatement这个类。这个类是标准java.sql.Statement的实现，但是getQueryLog这个方法并不是标准的JDBC方法，因为在我们的程序中运行的就是HIVE查询，所以我们可以在程序中进行强转得到HiveStatement这个类并调用这个方法获取到查询日志。 getQueryLog 这个方法中用到了整个HiveStatement中的一些变量，所以我们要进行HIVE查询日志的获取必须要对HiveStatement对象进行关联，同时一边在执行HIVE查询，一边还要从另一个线程中获取HIVE查询的日志过程。 设计思路 前端查询页面在进行查询提交时同时生成一个UUID类似的唯一查询ID一并提交到查询后台 程序接到HIVE查询请求后，将HIVE查询请求通过标准的JDBC的方式进行提交，需要注意的这期间通过HiveConnection获取到的Statement对象需要被缓存到自己创建的一个HiveStatementHolderService类里并和第1步生成的唯一ID关联，以便于上面提到的日志线程池进行日志查询 为了不影响HIVE查询线程，HIVE的执行日志查询放到另一个线程(池)中进行 通过定时调度轮询，日志查询线程通过HiveStatement的getQueryLogs查询到日志后将日志写入集中缓存如redis有序集合中，key为查询ID，同时为了redis内存回收可以设置一个过期时间 查询页面在提交HIVE查询后，通过定时轮询的方式，携带查询提交时的查询ID轮询HiveStatementHolderService服务，HiveStatementHolderService服务根据查询ID到对应的redis中取得对应的日志序列集合，并返回给查询展示端 将HIVE查询结束后，将Statement从HiveStatementHolderService中移除掉 暴露的问题一切都感觉很美好，但是现实呢？当我深入到HiveStatement内部，我发现了问题：HiveStatement并不是一个线程安全的类！也就是说这个类的实例在多线程环境下使用并不安全，可能会造成多线程访问出现数据上的问题或者报错，具体原因就是该类的各个方法，以及各个判断中都使用了类的局部属性，而这些属性的获取和设值并没有经过线程同步，所以可能会存在线程不同步的一些问题。基于这个问题我也google了一下，发现网上也有相关的issue： HIVE-16451 Thanks for finding this out Peter Vary. Although I didn’t quite get how the patch fixes the race condition. The way I understand the issue is that there is a Logging thread and the thread executing the HiveStatement. Both these threads are accessing isLogBeingGenerated, isCancelled, isQueryClosed flags in the same HiveStatement object. None of these getters and setters are thread safe. I think there could be more undiscovered race-conditions in this execution path. 提到了HiveStatement的线程安全问题 HIVE-16517 BeeLine, and Commands classes shares one HiveStatement between multiple threads for querying the logs, and running the queries.We can not make the HiveStatement thread safe, but we should at least make sure that calling getQueryLog will not cause problems if it is called parallel with any of the followings: cancel, close, execute, executeAsync, executeQuery, executeUpdate, getUpdateCount and more interestingly for the HiveQueryResultSet.next too. 上面更是提到了queryLog的获取存在线程不同步的问题 HIVE-15940最后这个ISSUE提出了后续可能的一个解决方案：Merge the query log operation as part of the getOperationStatus which also gets the Progress update.将查询日志作为getOperationStatus调用的一部分，具体的怎么设计估计还得等官方的具体实现了。 总结目前看来，此方案并没有完美，HiveStatement存在线程安全问题，不过我们应该可以暂时忍受一些线程不同步带来的很多问题，毕竟只是一个日志显示的问题，哪怕出错，报出什么异常，我们也可以暂时用粗暴的方式来解决：一旦出现异常就直接把该Statement的日志获取给停掉。","categories":[{"name":"原创文章","slug":"原创文章","permalink":"http://www.lpnote.com/categories/原创文章/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.lpnote.com/tags/大数据/"},{"name":"bug","slug":"bug","permalink":"http://www.lpnote.com/tags/bug/"},{"name":"HIVE","slug":"HIVE","permalink":"http://www.lpnote.com/tags/HIVE/"}]},{"title":"SpringBoot文件上传解析","slug":"spring-boot-fileupload-problem-analysis","date":"2017-05-02T16:00:00.000Z","updated":"2019-02-13T08:59:06.536Z","comments":true,"path":"2017/05/02/spring-boot-fileupload-problem-analysis/","link":"","permalink":"http://www.lpnote.com/2017/05/02/spring-boot-fileupload-problem-analysis/","excerpt":"在以往的开发过程中，Spring体系的文件上传一直使用的是commons-fileupload，在我们的项目中也是一样的，这两天在做公司的大数据查询平台，其中在做大文件上传时遇到了一些问题，记录如下： 问题现象在开发环境、测试环境中，我们的环境是直接部署的jetty，也就是所有的访问直接经由浏览器后到达jetty服务器，运行良好。 但是当程序上到预发布环境时，文件上传出现了问题，文件上传不成功。服务器返回了一个错误，大体意思就是文件上传的请求体大于服务器端的最大设置，从响应的header看是nginx响应回来的，初步定位到是nginx文件大小上传受限，运维帮助修改了nginx配置后，这个错误不再出现。然而又出现了新的问题，Java程序在上传文件的过程中内存溢出了！ 都知道在springmvc中，上传文件的话如果太大会写入临时磁盘文件来避免内存溢出的问题。当然我们的程序中也设置了： 以上的配置信息的解析是： 12文件上传的最大大小为256MHTTP请求的最大大小为257M，因为HTTP请求包含了文件上传，故要大于文件上传大小。","text":"在以往的开发过程中，Spring体系的文件上传一直使用的是commons-fileupload，在我们的项目中也是一样的，这两天在做公司的大数据查询平台，其中在做大文件上传时遇到了一些问题，记录如下： 问题现象在开发环境、测试环境中，我们的环境是直接部署的jetty，也就是所有的访问直接经由浏览器后到达jetty服务器，运行良好。 但是当程序上到预发布环境时，文件上传出现了问题，文件上传不成功。服务器返回了一个错误，大体意思就是文件上传的请求体大于服务器端的最大设置，从响应的header看是nginx响应回来的，初步定位到是nginx文件大小上传受限，运维帮助修改了nginx配置后，这个错误不再出现。然而又出现了新的问题，Java程序在上传文件的过程中内存溢出了！ 都知道在springmvc中，上传文件的话如果太大会写入临时磁盘文件来避免内存溢出的问题。当然我们的程序中也设置了： 以上的配置信息的解析是： 12文件上传的最大大小为256MHTTP请求的最大大小为257M，因为HTTP请求包含了文件上传，故要大于文件上传大小。 问题分析一直以为该配置是commons-fileupload的组件的配置，但是从报错的信息来看，似乎并没有写入到文件中，而是不断地在内存中写入堆积，导致内存不断被占用导致最终的内存溢出。 而且从报错的地方看是调用了jetty的Multipart的解析器，而并没有看到有使用到commons-fileupload的文件上传解析，出于解决问题的必要，我在项目中强行依赖了jetty的包，然后在IDE中打开了内存溢出部位的源码： 上图中write方法负责文件上传请求解析过程中数据的管道写入，这个方法的大体意思是：12如果设置了最大的文件上传大小，并且读取的实际大小大于了最大限制，则抛出异常如果设置了文件上传大小因子fileSizeThreshold(大于0),并且读取的实际大小大于fileSizeThreshold，并且文件为空，那么就初始化一个文件。 比较吸引人的是有一个“createFile”函数的调用，我在整个类中找了一遍，发现有几个地方有调用： 在parse方法中有创建MultiPart类的调用： 以及MultiPart内部类的open方法： isWriteFilesWithFilenames() 是一个属性值判断，在MultiPartInputStreamParser类的初始化过程中并没有对此进行设值，故可以判断该处返回false，反推可以知道在这里并没有调用createFile()这个函数。同时对_out对象进行了初始化，默认是初始化为一个ByteArrayOutputStream2的内存缓冲。 那么调用createFile()这个函数的地方就只剩下write方法了。 我们再来看看createFile这个方法的实现： 在这里方法里面，对parser的_file文件对象进行了初始化，同时该该文件包装了带缓冲的输出流bos对象，如果之前有在_out中的缓冲数据，则将缓冲数据写入到bos对象中，最后，最关键的一步：将_out对象原有的引用替换为bos对象，即将输出流指向了文件输出。 柳暗花明到这里，整个文件上传的线条都非常明显了： 1、Jetty检测到文件上传标识时(multipart)，实例化MultiPartInputStreamParser，并调用parse方法进行解析。2、parse方法进行一系列的逻辑处理，并初始化MultiPart，调用MultiPart对象的open方法，open方法先将_out输出流先初始化为内存缓冲流。3、parse方法完成一系列初始化后，进行上传数据读取，并调用MultiPart对象的写入方法write，并在write方法中进行了逻辑判断与处理：当内存缓冲达到最大值时，改为写入文件的方式（防止内存占用过大） 那么现在最大的问题是内存溢出了，那么最大的可能性就是write的时候并没有引导进入createFile这个调用，而是一直在向内存缓冲中写入数据，导致的内存溢出。 1if (MultiPartInputStreamParser.this._config.getFileSizeThreshold() &gt; 0 &amp;&amp; _size + length &gt; MultiPartInputStreamParser.this._config.getFileSizeThreshold() &amp;&amp; _file==null) 这个条件中唯一的可能性就是MultipartConfig中并没有对fileSizeThreshold进行设值。 到此问题已经浮现，那么现实中就是要找到jetty的配置，并进行设值处理。这个值的配置我其实找了好久，包括翻看了jetty的etc目录下的所有文件。 最后发现，其实这是Servlet 3.0的文件上传规范：http://www.blogjava.net/yongboy/archive/2011/01/15/346202.html 应用进行设置后，所有符合Servlet 3.0规范的WEB服务器会自动加载这个配置。但是我想Jetty中应该也是可以配置的，具体的细节就没有去考究了。 锦上添花而springboot是直接支持修改这个参数的： 峰回路转我翻看了springboot的文档时，文档上的错误注释误导了我： 上图红圈中的文字表述： file-size-threshold指明了上传内容会被写入到文件中的最大因子，默认为0，这表示上传内容会被立即写入到磁盘文件中。 我之前的配置中并没有配置file-size-threshold，然而默认值可以认为是都是写入文件。这并不符合我现在看到的现象，最后在我怀疑的精神下，我试着将该配置修改为file-size-threshold=1MB后，发到预发布环境验证，居然一切OK了。 也由此反证官方代码注释文档存在错误引导，并且实际上的Jetty代码中也体现了当设置为0时并不会创建文件，而一直写入内存缓冲导致内存溢出的问题。 为此我也给官方提交了一个issue，等待官方的答复。 20170504 更新官方回复了，确实存在这样的问题： This is somewhat complicated as the behaviour with a value of zero varies by container: Tomcat will write to disk immediatelyJetty will never write to diskUndertow ignores the file size threshold entirelySo the javadoc is right if you’re using Tomcat, but wrong if you’re using Jetty or Undertow. 所以也警示我们，虽然大厂的开源东西比较可靠，但是也要有怀疑精神！ 还有值得一说的就是，我最初在使用springboot的时候，引入了commons-fileupload包，我一直以为springboot使用的是这个文件上传包，直到内存溢出错误发生时，我才发现这个包根本没有用到。而springboot使用到了的是web容器Servlet 3.0自带的文件上传解析。这个配置从springboot的配置项：spring.http.multipart.enabled（默认为true）可以设置开启或者关闭。在没有显式设置的情况下，这个选项是默认开启的，也就有了我上面的问题的出现。 延伸思考那么问题又来了，如何让springboot使用commons-fileupload组件进行文件上传呢。这里我就不多讲了，通过网上我已经搜索到了比较多的资料： 1、https://github.com/bobbylight/file-upload-example2、http://stackoverflow.com/questions/32782026/springboot-large-streaming-file-upload-using-apache-commons-fileupload 总结产生问题的原因很多，自己也有原因，妄自认为和springmvc的文件上传没有区别。归根到底还是对springboot的文档和机制不熟悉。对springboot的使用还没有太深入，这个有待加强。 文件上传几大限制 负载均衡器（反向代理）限制： 一些负载均衡器或者反向代理服务器存在请求大小最大限制 应用服务器限制： 一些WEB应用服务器存在请求最大大小限制 应用程序限制： 应用程序中进行了上传大小限制 SpringBoot文件上传SpringBoot默认自己开启了Servlet3.0规范的文件上传，不再需要commons-fileupload等第三方文件上传包同时配置项如下：1234spring.http.enabled=true （默认为true）spring.http.multipart.max-request-size= #这里设置最大请求大小，该大小必须大于max-file-sizespring.http.multipart.max-file-size= #这里设置最大的文件上传大小spring.http.multipart.file-size-threshold= #这里设置当文件大小超过多大后，由内存缓冲改由写入磁盘文件 SpringBoot使用CommonsFileUpload使用方式见文末所说。1spring.http.enabled=false 并引入Commons Fileupload包，并进行相关初始化。 关于怀疑精神任何时候都要对开源精神充满敬畏，同时少不了怀疑探索精神！","categories":[{"name":"原创文章","slug":"原创文章","permalink":"http://www.lpnote.com/categories/原创文章/"}],"tags":[{"name":"问题解析","slug":"问题解析","permalink":"http://www.lpnote.com/tags/问题解析/"},{"name":"参与开源","slug":"参与开源","permalink":"http://www.lpnote.com/tags/参与开源/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://www.lpnote.com/tags/SpringBoot/"},{"name":"Spring","slug":"Spring","permalink":"http://www.lpnote.com/tags/Spring/"}]},{"title":"java输出csv文件中文乱码的问题","slug":"java-csv-encoding-problem","date":"2017-04-19T16:00:00.000Z","updated":"2019-02-13T08:59:06.532Z","comments":true,"path":"2017/04/19/java-csv-encoding-problem/","link":"","permalink":"http://www.lpnote.com/2017/04/19/java-csv-encoding-problem/","excerpt":"","text":"在开发程序的时候，通过java程序输出csv文件，采用utf-8编码，然后用excel打开，发现文件中的中文全部乱码了。 在网上搜索了各种手工解决办法，无外乎就是将文件打开另存为“ANSI”格式。 然而真正程序能解决的是如下： 12//在文件中增加BOM，详细说明可以Google,该处的byte[] 可以针对不同编码进行修改out.write(new byte[] &#123; (byte) 0xEF, (byte) 0xBB,(byte) 0xBF &#125;); 在文件的输出流中增加BOM信息，中文乱码得以解决。","categories":[{"name":"转载文章","slug":"转载文章","permalink":"http://www.lpnote.com/categories/转载文章/"}],"tags":[{"name":"最佳实践","slug":"最佳实践","permalink":"http://www.lpnote.com/tags/最佳实践/"},{"name":"基础问题","slug":"基础问题","permalink":"http://www.lpnote.com/tags/基础问题/"}]},{"title":"mac下某些软件内复制粘贴失效的问题","slug":"mac-quick-key","date":"2017-04-11T16:00:00.000Z","updated":"2019-02-13T08:59:06.532Z","comments":true,"path":"2017/04/11/mac-quick-key/","link":"","permalink":"http://www.lpnote.com/2017/04/11/mac-quick-key/","excerpt":"","text":"之前一直使用一款叫做switchhosts！的host管理软件，但是唯一的问题就是在软件里面复制粘贴失效，问了作者，作者说没问题。小白的我折腾了好久才知道了方法: 快捷键冲突：系统偏好设置－键盘－快捷键－恢复成默认","categories":[{"name":"原创文章","slug":"原创文章","permalink":"http://www.lpnote.com/categories/原创文章/"}],"tags":[{"name":"mac","slug":"mac","permalink":"http://www.lpnote.com/tags/mac/"}]},{"title":"Apache Ranger在HDFS中的最佳实践","slug":"best-practices-in-hdfs-authorization-with-apache-ranger","date":"2017-04-10T16:00:00.000Z","updated":"2019-02-13T08:59:06.520Z","comments":true,"path":"2017/04/10/best-practices-in-hdfs-authorization-with-apache-ranger/","link":"","permalink":"http://www.lpnote.com/2017/04/10/best-practices-in-hdfs-authorization-with-apache-ranger/","excerpt":"HDFS对于任何Hadoop大数据平台来说都是核心组成部分，为了加强对Hadoop平台的数据保护，将安全控制深入到HDFS层是非常有必要的。HDFS本身提供了Kerberos认证，并且提供了基于POSIX风格的权限和HDFS——ACL控制，当然它也可以使用基于Apache Ranger的权限控制体系。 Apache Ranger (http://hortonworks.com/hadoop/ranger/) 是一个集中式的Hadoop体系的安全管理解决方案，它提供给管理者在HDFS或者其它Hadoop体系组件上创建和应用安全策略的功能。 Ranger是怎么在HDFS上工作的？为了在HDP发行版中加强安全性，建议安装和配置Kerberos, Apache Knox和Apache Ranger。 Apache Ranger提供了一个和HDFS原生权限相匹配适应的授权模型。 HDFS Ranger插件会首先检测是否存在对应的授权策略对应用户授权，如果存在那么用户权限检测通过。如果没有这样的策略，那么Ranger插件会启用HDFS原生的权限体系进行权限检查（POSIX or HDFS ACL）。这种模型在Ranger中适用于HDFS和YARN服务。","text":"HDFS对于任何Hadoop大数据平台来说都是核心组成部分，为了加强对Hadoop平台的数据保护，将安全控制深入到HDFS层是非常有必要的。HDFS本身提供了Kerberos认证，并且提供了基于POSIX风格的权限和HDFS——ACL控制，当然它也可以使用基于Apache Ranger的权限控制体系。 Apache Ranger (http://hortonworks.com/hadoop/ranger/) 是一个集中式的Hadoop体系的安全管理解决方案，它提供给管理者在HDFS或者其它Hadoop体系组件上创建和应用安全策略的功能。 Ranger是怎么在HDFS上工作的？为了在HDP发行版中加强安全性，建议安装和配置Kerberos, Apache Knox和Apache Ranger。 Apache Ranger提供了一个和HDFS原生权限相匹配适应的授权模型。 HDFS Ranger插件会首先检测是否存在对应的授权策略对应用户授权，如果存在那么用户权限检测通过。如果没有这样的策略，那么Ranger插件会启用HDFS原生的权限体系进行权限检查（POSIX or HDFS ACL）。这种模型在Ranger中适用于HDFS和YARN服务。 对于其它服务，比如Hive或者HBase，Ranger是作为唯一的有效授权依据，在HDP发行版中，使用Ambari配置HDFS使用Ranger的路径如下：Ambari → Ranger → HDFS config → Advanced ranger-hdfs-security 基于Ranger的这种授权模型，用户可以非常安全的将Ranger添加到已经存在的大数据集群中，而该集群可能之前一直使用的是基于POSIX的权限体系。建议对于所有的部署，这个选项都设置成true。 Ranger的用户界面可以让管理者非常容易地找到用户的授权关系（Ranger policy or native HDFS） 用户可以方便的查看审计内容（路径为：Ranger→ Audit），如果在界面上“Access Enforcer”列的内容为“Ranger-acl”，那说明Ranger的策略被应用到了用户身上。如果“Access Enforcer”列的内容为“Hadoop-acl”,表示该访问是由HDFS原生的POSIX权限和HDFS ACL提供的。 Having a federated authorization model may create a challenge for security administrators looking to plan a security model for HDFS. 如何确保安全当Ranger和Hadoop都安装完后，建议管理员按下面的步骤进行配置： Change HDFS umask to 077 Identify directory which can be managed by Ranger policies Identify directories which need to be managed by HDFS native permissions Enable Ranger policy to audit all records 改变HDFS掩码为077，确定哪些目录由Ranger授权管理，哪些目录由HDFS原生权限控制。启用Ranger的审计功能 下面是详细步骤： Change HDFS umask to 077 from 022. This will prevent any new files or folders to be accessed by anyone other than the owner管理员可以在Ambari中这样操作： 在HDFS中默认的掩码为022，在这种情况下，所有的用户都具有所有HDFS文件系统文件和文件夹的读取权限。 你可以通过以下命令进行检查： $ hdfs dfs -ls /appsFound 3 itemsdrwxrwxrwx – falcon hdfs 0 2015-11-30 08:02 /apps/falcondrwxr-xr-x – hdfs hdfs 0 2015-11-30 07:56 /apps/hbasedrwxr-xr-x – hdfs hdfs 0 2015-11-30 08:01 /apps/hive 指定哪些目录由Ranger授权 建议这些目录由Ranger来进行管理和授权（/app/hive,/apps/Hbase以及一些自定义的数据目录） HDFS本身的授权模型对于这些需求来说显得捉襟见肘。 可以使用chmod修改默认权限，例如： $ hdfs dfs -chmod -R 000 /apps/hive$ hdfs dfs -chown -R hdfs:hdfs /apps/hive$ hdfs dfs -ls /apps/hiveFound 1 itemsd——— – hdfs hdfs 0 2015-11-30 08:01 /apps/hive/warehouse Ranger可以给用户进行显式的授权，例如： 管理员可以照着这个图对其它目录进行用户授权，你可以通过以下方式进行授权验证： Connect to HiveServer2 using beeline Create a table create table employee( id int, name String, ssn String); Go to ranger, and check the HDFS access audit. The enforcer should be ‘ranger-acl’ 指定哪些目录由HDFS原生权限控制 建议让HDFS原生权限管理/tmp和/user目录。这些目录通常被各种应用使用于创建用户级的目录。这里你也需要设置/user目录的权限为“700”: $ hdfs dfs -ls /userFound 4 itemsdrwxrwx— – ambari-qa hdfs 0 2015-11-30 07:56 /user/ambari-qadrwxr-xr-x – hcat hdfs 0 2015-11-30 08:01 /user/hcatdrwxr-xr-x – hive hdfs 0 2015-11-30 08:01 /user/hivedrwxrwxr-x – oozie hdfs 0 2015-11-30 08:02 /user/oozie $ hdfs dfs -chmod -R 700 /user/*$ hdfs dfs -ls /userFound 4 itemsdrwx—— – ambari-qa hdfs 0 2015-11-30 07:56 /user/ambari-qadrwx—— – hcat hdfs 0 2015-11-30 08:01 /user/hcatdrwx—— – hive hdfs 0 2015-11-30 08:01 /user/hivedrwx—— – oozie hdfs 0 2015-11-30 08:02 /user/oozie 确保所有的HDFS数据操作都是被审计的当Ranger是通过Ambari安装时，它会创建一个默认的策略，该策略允许所有的目录和文件访问并开启审计功能。这个策略同时也应用于Ambari冒烟测试用户“ambari-qa”用来验证HDFS服务的可用性。如果管理员禁用或者删除了该策略，那么需要创建一个类似的策略来允许审计所有的文件和目录。 总结保证HDFS的安全性是保证Hadoop安全性的起点。 Ranger为HDFS提供了一个集中统一管理安全策略的接口。建议管理员合理使用Ranger以及HDFS本身的权限机制来全程覆盖HDFS的授权管理。 本文英文原文：https://hortonworks.com/blog/best-practices-in-hdfs-authorization-with-apache-ranger/","categories":[{"name":"翻译文章","slug":"翻译文章","permalink":"http://www.lpnote.com/categories/翻译文章/"}],"tags":[{"name":"ranger","slug":"ranger","permalink":"http://www.lpnote.com/tags/ranger/"},{"name":"hdfs","slug":"hdfs","permalink":"http://www.lpnote.com/tags/hdfs/"},{"name":"大数据","slug":"大数据","permalink":"http://www.lpnote.com/tags/大数据/"},{"name":"最佳实践","slug":"最佳实践","permalink":"http://www.lpnote.com/tags/最佳实践/"},{"name":"hortonworks","slug":"hortonworks","permalink":"http://www.lpnote.com/tags/hortonworks/"}]},{"title":"Ranger自定义插件开发","slug":"how-to-add-a-custom-plugin-in-ranger","date":"2017-01-23T06:58:20.000Z","updated":"2019-02-13T08:59:06.528Z","comments":true,"path":"2017/01/23/how-to-add-a-custom-plugin-in-ranger/","link":"","permalink":"http://www.lpnote.com/2017/01/23/how-to-add-a-custom-plugin-in-ranger/","excerpt":"英文链接：https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=53741207 Ranger简介Apache Ranger为Hadoop体系提供了统一的安全体系，包括细致的访问控制和统一的审计。 Apache Ranger 0.4版本为很多服务提供了授权和审核，这些服务包括： HDFS, HBase, Hive, Knox和Storm。如果要添加更多的服务支持就需要多个模块的修改，包括UI，API，数据库schema等等。 Apache Ranger 0.5版本支持了一个统一的模型来更好地支持新组件的接入，而这些接入并不需要改变Ranger的代码。这篇文章主要阐述了自定义Ranger组件的编程模型以及步骤。 创建自定义授权插件这节将提供的是一个创建授权插件的高阶视图。 更多的每一步细节都会在后面的步骤中说明。 定义服务类型(Service-type) 创建一个JSON格式的文件，包含以下内容： 资源,比如：database,table,column等 访问类型,比如：select, update, create, drop等 连接服务的配置,比如：JDBC URL,JDBC driver,credentials等 加载JSON文件到Ranger中","text":"英文链接：https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=53741207 Ranger简介Apache Ranger为Hadoop体系提供了统一的安全体系，包括细致的访问控制和统一的审计。 Apache Ranger 0.4版本为很多服务提供了授权和审核，这些服务包括： HDFS, HBase, Hive, Knox和Storm。如果要添加更多的服务支持就需要多个模块的修改，包括UI，API，数据库schema等等。 Apache Ranger 0.5版本支持了一个统一的模型来更好地支持新组件的接入，而这些接入并不需要改变Ranger的代码。这篇文章主要阐述了自定义Ranger组件的编程模型以及步骤。 创建自定义授权插件这节将提供的是一个创建授权插件的高阶视图。 更多的每一步细节都会在后面的步骤中说明。 定义服务类型(Service-type) 创建一个JSON格式的文件，包含以下内容： 资源,比如：database,table,column等 访问类型,比如：select, update, create, drop等 连接服务的配置,比如：JDBC URL,JDBC driver,credentials等 加载JSON文件到Ranger中 Ranger授权插件开发服务初始化 创建一个静态的（或者全局的）RangerBasePlugin实例（或者继承至它的一个实例），并提供一个方便后面授权使用的引用 调用该实例的init()方法。这个步骤将会初始化policy-engine，它会从Ranger Admin拉取安全策略，并启动一个后台线程用于周期性的从Ranger Admin更新安全策略。 在插件实例中注册一个审计处理器，比如：RangerDefaultAuditHandler。 插件将会使用这个审计处理器生成资源访问的审计日志。 资源授权访问 创建一个RangerAccessRequest实现的实例，该实例拥有资源、访问类型、用户等需要被授权的细节逻辑，这个实现类可以参考RangerAccessRequestImpl。 调用前面创建的插件实例方法isAccessAllowed()。 根据返回的结果决定允许还是拒绝操作。 资源查找 继承RangerBaseService类，实现lookupResource()和validateConfig()方法。 为这个类提供一个在服务定义中的名字。 将包含该实现的类的类库（jar包文件）以及该类库依赖的其它类库拷贝到Ranger Admin的ranger-plugins/&lt;service-type&gt;目录 安装插件到服务中要想访问授权生效，Ranger插件必须先进行安装和配置。 请参看查看相关文档，如何注册一个授权（authorizer）。 服务类型服务类型定义一个服务的资源，如：资源访问类型（read/write/create/delete/submit/…）, 连接服务的配置信息（url/username/password/…）,在策略计算时的自定义的一些条件（IP range等）。这些都需要定义在JSON文件中，最新的服务类型定义格式请查看最新的文档服务定义格式 例子-YARN服务类型定义123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172&#123; \"name\": \"yarn\", \"implClass\": \"org.apache.ranger.services.yarn.RangerServiceYarn\", \"label\": \"YARN\", \"description\": \"YARN\", \"guid\": \"5b710438-edcf-4e20-834c-a9a267b5b963\", \"resources\": [ &#123; \"name\": \"queue\", \"type\": \"string\", \"level\": 10, \"mandatory\": true, \"lookupSupported\": true, \"recursiveSupported\": true, \"matcher\": \"org.apache.ranger.plugin.resourcematcher.RangerPathResourceMatcher\", \"matcherOptions\": &#123;\"wildCard\":true, \"ignoreCase\":true, \"pathSeparatorChar\":\".\"&#125;, \"label\": \"Queue\", \"description\": \"Queue\" &#125; ], \"accessTypes\": [ &#123; \"name\": \"submit-app\", \"label\": \"submit-app\" &#125;, &#123; \"name\": \"admin-queue\", \"label\": \"admin-queue\" &#125; ], \"configs\": [ &#123; \"name\": \"username\", \"type\": \"string\", \"mandatory\": true, \"label\": \"Username\" &#125;, &#123; \"name\": \"password\", \"type\": \"password\", \"mandatory\": true, \"label\": \"Password\" &#125;, &#123; \"name\": \"yarn.url\", \"type\": \"string\", \"mandatory\": true, \"label\": \"YARN REST URL\" &#125;, &#123; \"name\": \"commonNameForCertificate\", \"type\": \"string\", \"mandatory\": false, \"label\": \"Common Name for Certificate\" &#125; ], \"policyConditions\": [ &#123; \"name\": \"ip-range\", \"evaluator\": \"org.apache.ranger.plugin.conditionevaluator.RangerIpMatcher\", \"label\": \"IP Address Range\", \"description\": \"IP Address Range\" &#125; ]&#125; 注册服务类型服务类型注册必须使用Ranger Admin提供的RESTFUL API来进行。 服务类型一旦注册成功，Ranger Admin就会提供一个创建服务的UI页面(在以前的发行版中叫做repositories)以及该服务的策略。 Ranger插件使用服务定义和策略来确定一个访问请求应该被允许还是被拒绝。 Ranger Admin提供的REST API可以通过curl这个小命令行工具调用:1curl -u admin:admin -X POST -H &quot;Accept: application/json&quot; -H &quot;Content-Type: application/json&quot; –d @ranger-servicedef-yarn.json http://ranger-admin-host:port/service/plugins/definitions Ranger插件开发Ranger AuthorizerRanger服务授权体系主要是通过以下方式实现的：提供一个lib，该lib实现了服务钩子(hook)拦截资源访问，并调用Ranger API获得授权以及记录审计日志。 当在某个服务里面安装ranger插件时，这些钩子会被自动注册到服务中。在这节中，我们会深入YARN ranger插件的实现细节（YARN服务类型定义已经在前面一节中做过了）。 在服务的初始化过程中，一个静态（或者全局的）RangerBasePlugin实例应该被创建出来， 该实例的引用也应该被提供出来，以方便后续访问授权请求的使用。 在初始化过程中，插件将会从本地缓存中加载策略（如果本地存在的话），并启动一个策略更新器用于从Ranger Admin中拉取更新后的策略。 YARN服务需要授权服务实现YarnAuthorizationProvider接口。 Ranger YARN插件实现了init()和checkPermission()方法用于提供授权和YARN队列的访问审计。1234567891011121314151617181920212223242526public class RangerYarnAuthorizer extends YarnAuthorizationProvider &#123; private static RangerBasePlugin plugin = null; @Override public void init(Configuration conf) &#123; plugin = new RangerBasePlugin(\"yarn\", \"yarn\"); plugin.init(); // this will initialize policy engine and policy refresher plugin.setDefaultAuditHandler(new RangerDefaultAuditHandler()); &#125; @Override public boolean checkPermission(AccessType accessType, PrivilegedEntity entity, UserGroupInformation ugi) &#123; RangerAccessRequestImpl request = new RangerAccessRequestImpl(); RangerResourceImpl resource = new RangerResourceImpl(); resource.setValue(\"queue\", entity.getName()); request.setResource(resource); request.setAccessType(getRangerAccessType(accessType)); request.setUser(ugi.getShortUserName()); request.setUserGroups(Sets.newHashSet(ugi.getGroupNames())); request.setAccessTime(new Date()); request.setClientIPAddress(getRemoteIp()); RangerAccessResult result = plugin.isAccessAllowed(request); return result == null ? false : result.getIsAllowed(); &#125;&#125; 资源查找在Ranger Admin中构建策略的时候，用户会输入需要保护的资源的名字。为了让用户使用更方便，Ranager Admin提供了一自动完成特性，该特性会根据输入的内容查询服务中可用的匹配资源。 lookup的实现是针对被访问的资源中的服务。它涉及服务提供的API以及检索可用的资源。为了完成autocomplete特性，Ranger Admin要求插件提供RangerBaseService的一个具体实现。这个实现类必须在Ranger的服务类型中注册，并且要保证该类库已经放置到了Ranger Admin的类路径下。 123456789public class RangerServiceYarn extends RangerBaseService &#123; public HashMap&lt;String, Object&gt; validateConfig() throws Exception &#123; // TODO: connect to YARN resource manager; throw Exception on failure &#125; public List&lt;String&gt; lookupResource(ResourceLookupContext context) throws Exception &#123; // TODO: retrieve the resource list from YARN resource manager using REST API &#125;&#125; 安装和配置插件以下的实现了Ranger插件的jar文件必须保证已经放置在了服务的类路径下（如YARN）： ranger-plugins-audit-&lt;version&gt;.jar ranger-plugins-common-&lt;version&gt;.jar ranger-plugins-cred-&lt;version&gt;.jar Ranger插件在初始化的时候会读取以下文件，也请确保以下文件已经放置在了服务的类路径下： ranger-&lt;serviceType&gt;-audit.xml ranger-&lt;serviceType&gt;-security.xml ranger-policymgr-ssl.xml Ranger插件需要以下配置才能正常运行，这些配置属性通常在ranger-&lt;serviceType&gt;-security.xml中。 配置 默认值 备注 ranger.plugin.&lt;serviceType&gt;.service.name No default value. This configuration must be provided. Name of the service containing policies for the plugin ranger.plugin.&lt;serviceType&gt;.policy.source.impl org.apache.ranger.admin.client.RangerAdminRESTClient Name of the class used to retrieve policies. ranger.plugin.&lt;serviceType&gt;.policy.rest.url No default value. URL to Ranger Admin ranger.plugin.&lt;serviceType&gt;.policy.rest.ssl.config.file No default value. This configuration must be provided if SSL is enabled between plugin and Ranger Admin. Path to the file containing SSL details to contact Ranger Admin ranger.plugin.&lt;serviceType&gt;.policy.cache.dir No default value. If no valid value is specified, local caching of policies will not be done. Directory where Ranger policies are cached after successful retrieval from the source ranger.plugin.&lt;serviceType&gt;.policy.pollIntervalMs 30000 How often to poll for changes in policies?","categories":[{"name":"翻译文章","slug":"翻译文章","permalink":"http://www.lpnote.com/categories/翻译文章/"}],"tags":[{"name":"ranger","slug":"ranger","permalink":"http://www.lpnote.com/tags/ranger/"},{"name":"大数据","slug":"大数据","permalink":"http://www.lpnote.com/tags/大数据/"}]},{"title":"PrestoDB自定义函数开发指南","slug":"user-guide-of-prestodb-udf","date":"2017-01-19T09:51:04.000Z","updated":"2019-02-13T08:59:06.552Z","comments":true,"path":"2017/01/19/user-guide-of-prestodb-udf/","link":"","permalink":"http://www.lpnote.com/2017/01/19/user-guide-of-prestodb-udf/","excerpt":"Presto 自定义函数开发（译）本文是基于Presto 0.160版本翻译,最新版的内容请参看最新版文档 英文原文: https://prestodb.io/docs/current/develop/functions.html Presto是一个开源的分布式SQL查询引擎，适用于交互式分析查询，数据量支持GB到PB字节。 Presto的设计和编写完全是为了解决像Facebook这样规模的商业数据仓库的交互式分析和处理速度的问题。 Presto本身自带了一些功能函数，对于我们在做SQL查询时非常方便，但是有些时候这些内建的功能函数并不能完全满足我们的需求，又或者是我们逻辑中有一些共性的部分，需要将这些共性的部分提炼出来形成公共的函数库。Presto为我们提供了这套自定义函数的机制。 插件实现该函数框架用于实现SQL功能函数。Presto包含了一定数量的内建函数支持。 为了实现更多的功能函数,你可以自己写一个插件,实现getFunctions(),让这个插件实现更多的功能函数: 123456789101112131415public class ExampleFunctionsPlugin implements Plugin&#123; @Override public Set&lt;Class&lt;?&gt;&gt; getFunctions() &#123; return ImmutableSet.&lt;Class&lt;?&gt;&gt;builder() .add(ExampleNullFunction.class) .add(IsNullFunction.class) .add(IsEqualOrNullFunction.class) .add(ExampleStringFunction.class) .add(ExampleAverageFunction.class) .build(); &#125;&#125; 注意: ImmutableSet是一个Guava类库的一个工具类, getFunctions()方法包含了本掼中所有的我们将要实现的功能函数。 如果想要查看完整的代码例子,可以查看presto的功能模块presto-ml(用于机器学习)或者presto-teradata-functions(Teradata-compatible functions), 以上两个包都是presto源代码包中。","text":"Presto 自定义函数开发（译）本文是基于Presto 0.160版本翻译,最新版的内容请参看最新版文档 英文原文: https://prestodb.io/docs/current/develop/functions.html Presto是一个开源的分布式SQL查询引擎，适用于交互式分析查询，数据量支持GB到PB字节。 Presto的设计和编写完全是为了解决像Facebook这样规模的商业数据仓库的交互式分析和处理速度的问题。 Presto本身自带了一些功能函数，对于我们在做SQL查询时非常方便，但是有些时候这些内建的功能函数并不能完全满足我们的需求，又或者是我们逻辑中有一些共性的部分，需要将这些共性的部分提炼出来形成公共的函数库。Presto为我们提供了这套自定义函数的机制。 插件实现该函数框架用于实现SQL功能函数。Presto包含了一定数量的内建函数支持。 为了实现更多的功能函数,你可以自己写一个插件,实现getFunctions(),让这个插件实现更多的功能函数: 123456789101112131415public class ExampleFunctionsPlugin implements Plugin&#123; @Override public Set&lt;Class&lt;?&gt;&gt; getFunctions() &#123; return ImmutableSet.&lt;Class&lt;?&gt;&gt;builder() .add(ExampleNullFunction.class) .add(IsNullFunction.class) .add(IsEqualOrNullFunction.class) .add(ExampleStringFunction.class) .add(ExampleAverageFunction.class) .build(); &#125;&#125; 注意: ImmutableSet是一个Guava类库的一个工具类, getFunctions()方法包含了本掼中所有的我们将要实现的功能函数。 如果想要查看完整的代码例子,可以查看presto的功能模块presto-ml(用于机器学习)或者presto-teradata-functions(Teradata-compatible functions), 以上两个包都是presto源代码包中。 标量函数实现函数框架使用注解来表示相关的函数信息,如: 名称、描述、返回类型和参数类型。 以下的例子是一个实现is_null的功能函数例子:12345678910public class ExampleNullFunction&#123; @ScalarFunction(\"is_null\") @Description(\"Returns TRUE if the argument is NULL\") @SqlType(StandardTypes.BOOLEAN) public static boolean isNull(@SqlNullable @SqlType(StandardTypes.VARCHAR) Slice string) &#123; return (string == null); &#125;&#125; 该函数is_null只需要一个VARCHAR类型的参数,并且返回一个BOOLEAN类型的结果,判断给定的参数是否为NULL。 这里需要注意的是提供给函数的参数是一个Slice类型。VARCHAR使用Slice,而不使用String（原生的容器类型）,这个类型是对byte[]的包装。 @SqlType: @SqlType注解用于申明返回类型和参数类型。注意Java代码的参数和返回类型必须和注释中申明的类型一致。 @SqlNullable: @SqlNullable注解表明参数可以为空。 如果没有这个注解, 任何一个参数为NULL,那么该函数都会返回NULL。当类型有对应的原始类型时,比如:BigintType,如果要使用@SqlNullable请使用包装类型, 当参数不为NULL时,如果函数想要返回NULL,则必须在函数上申明@SqlNullable。 参数化标量函数拥有类型参数的标量函数会增加额外的复杂度。下面我们将演示如果将上面的例子能够适应任何类型:123456789101112131415161718192021222324252627@ScalarFunction(name = \"is_null\")@Description(\"Returns TRUE if the argument is NULL\")public final class IsNullFunction&#123; @TypeParameter(\"T\") @SqlType(StandardTypes.BOOLEAN) public static boolean isNullSlice(@SqlNullable @SqlType(\"T\") Slice value) &#123; return (value == null); &#125; @TypeParameter(\"T\") @SqlType(StandardTypes.BOOLEAN) public static boolean isNullLong(@SqlNullable @SqlType(\"T\") Long value) &#123; return (value == null); &#125; @TypeParameter(\"T\") @SqlType(StandardTypes.BOOLEAN) public static boolean isNullDouble(@SqlNullable @SqlType(\"T\") Double value) &#123; return (value == null); &#125; // ...and so on for each native container type&#125; @TypeParameter: @TypeParameter注解用于申明哪些类型可以被用于@SqlType注解或者函数的返回类型。 它也可以用于注解一个参数的类型Type. 在运行时,引擎会将正确的类型绑定到该参数上。 @OperatorDependency 可以用于申明一个参数需要另一个额外的功能函数用于操作。 例如,下面的功能函数只会绑定到拥有一个equals方法的类型上: 12345678910111213141516171819202122@ScalarFunction(name = \"is_equal_or_null\")@Description(\"Returns TRUE if arguments are equal or both NULL\")public final class IsEqualOrNullFunction&#123; @TypeParameter(\"T\") @SqlType(StandardTypes.BOOLEAN) public static boolean isEqualOrNullSlice( @OperatorDependency(operator = OperatorType.EQUAL, returnType = StandardTypes.BOOLEAN, argumentTypes = &#123;\"T\", \"T\"&#125;) MethodHandle equals, @SqlNullable @SqlType(\"T\") Slice value1, @SqlNullable @SqlType(\"T\") Slice value2) &#123; if (value1 == null &amp;&amp; value2 == null) &#123; return true; &#125; if (value1 == null || value2 == null) &#123; return false; &#125; return (boolean) equals.invokeExact(value1, value2); &#125; // ...and so on for each native container type&#125; 另一个标量函数的例子 下面的lowercaser函数需要一个VARCHAR参数,并且返回VARCHAR的结果, 目的是转换参数字符到小写形式: 1234567891011public class ExampleStringFunction&#123; @ScalarFunction(\"lowercaser\") @Description(\"converts the string to alternating case\") @SqlType(StandardTypes.VARCHAR) public static Slice lowercaser(@SqlType(StandardTypes.VARCHAR) Slice slice) &#123; String argument = slice.toStringUtf8(); return Slices.utf8Slice(argument.toLowerCase()); &#125;&#125; 对于常见的字符串函数（包括转换小写）, Slice类型也提供了直接基于底层byte[]的实现来提供更好的性能。 这个功能函数没有使用@SqlNullable注解,意味着如果参数为NULL,那么返回结果会自动变成NULL（函数不会被调用）。 聚合函数实现聚合函数使用和标量函数相似的框架,但是更加复杂一点。 AccumulatorState: 所有的聚合函数聚集input rows到一个state对象中。这个对象必须实现AccumulatorState这个接口。对于简单聚合, 仅仅需求继承AccumulatorState来创建一个带getter和setter方法的新接口就可以了, 框架会帮你 生成实现代码和序列化器。 如果你需要一个更复杂的state对象, 你需要实现AccumulatorStateFactory和AccumulatorStateSerializer, 并通过AccumulatorStateMetadata这个注解来标注。下面的代码实现了聚合函数avg_double用于计算double类型的列: 123456789101112131415161718192021222324252627282930@AggregationFunction(\"avg_double\")public class AverageAggregation&#123; @InputFunction public static void input(LongAndDoubleState state, @SqlType(StandardTypes.DOUBLE) double value) &#123; state.setLong(state.getLong() + 1); state.setDouble(state.getDouble() + value); &#125; @CombineFunction public static void combine(LongAndDoubleState state, LongAndDoubleState otherState) &#123; state.setLong(state.getLong() + otherState.getLong()); state.setDouble(state.getDouble() + otherState.getDouble()); &#125; @OutputFunction(StandardTypes.DOUBLE) public static void output(LongAndDoubleState state, BlockBuilder out) &#123; long count = state.getLong(); if (count == 0) &#123; out.appendNull(); &#125; else &#123; double value = state.getDouble(); DOUBLE.writeDouble(out, value / count); &#125; &#125;&#125; 有两部分: 每一行的sum和总行数. LongAndDoubleState一个继承至AccumulatorState的接口: 1234567891011public interface LongAndDoubleState extends AccumulatorState&#123; long getLong(); void setLong(long value); double getDouble(); void setDouble(double value);&#125; 就像上面看到的, 对于简单的AccumulatorState对象, 只需要简单定义一个接口,并写上getter和setter方法,后面的事就由框架帮你实现了。 深入了解各种注解对于聚合函数开发的用途: @InputFunction: @InputFunction注解申明哪个function应该来接收输入的rows并且将它们存储在AccumulatorState对象里. 这有点类似于标量函数（你必须给参数指定@SqlType）, 不同的于上面标量的例子（Slice用于存储VARCHAR）,原始类型double用于参数输入, 在这个例子中, 输入函数(input function)简单地跟踪记录的行数（通过setLong()函数）和 总值(通过setDouble()函数)。 @CombineFunction: @CombineFunction注解用于申明哪个function用于合并两个state对象。该函数用于合并分区的聚合state,它将两个state对象合并到第一个（在上面的例子中, 就是把它们相加）。 @OutputFunction: @OutputFunction是聚合操作最后一个被调用的function,它携带最终的state对象(所有的分区state的结果)并且将这个结果写入到BlockBuilder。 序列化是在哪里发生的呢? 并且什么是GroupedAccumulatorState? @InputFunction和@CombineFunction通常运行在不同的worker机器上, 所以state对象被聚合框架序列化并且在worker机器之间进行传输。GroupedAccumulatorState被用于GROUP BY聚合, 并且框架会自动为你生成实现,你不需要指定一个AccumulatorStateFactory","categories":[{"name":"翻译文章","slug":"翻译文章","permalink":"http://www.lpnote.com/categories/翻译文章/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.lpnote.com/tags/大数据/"},{"name":"presto","slug":"presto","permalink":"http://www.lpnote.com/tags/presto/"}]},{"title":"kafka顺序消息","slug":"sequence-message-in-kafka","date":"2017-01-17T04:49:43.000Z","updated":"2019-02-13T08:59:06.536Z","comments":true,"path":"2017/01/17/sequence-message-in-kafka/","link":"","permalink":"http://www.lpnote.com/2017/01/17/sequence-message-in-kafka/","excerpt":"","text":"顺序消息包括以下两方面： 全局顺序 局部顺序 全局顺序全局顺序就目前的应用范围来讲，可以列举出来的也就限于binlog日志传输，如mysql binlog日志传输要求全局的顺序，不能有任何的乱序。这种的解决办法通常是最为保守的方式： 全局使用一个生产者 全局使用一个消费者（并严格到一个消费线程） 全局使用一个分区（当然不同的表可以使用不同的分区或者topic实现隔离与扩展） 局部顺序其实在大部分业务场景下，只需要保证消息局部有序即可，什么是局部有序？局部有序是指在某个业务功能场景下保证消息的发送和接收顺序是一致的。如：订单场景，要求订单的创建、付款、发货、收货、完成消息在同一订单下是有序发生的，即消费者在接收消息时需要保证在接收到订单发货前一定收到了订单创建和付款消息。 针对这种场景的处理思路是：针对部分消息有序（message.key相同的message要保证消费顺序）场景，可以在producer往kafka插入数据时控制，同一key分发到同一partition上面。因为每个partition是固定分配给某个消费者线程进行消费的，所以对于在同一个分区的消息来说，是严格有序的（在kafka 0.10.x以前的版本中，kafka因消费者重启或者宕机可能会导致分区的重新分配消费，可能会导致乱序的发生，0.10.x版本进行了优化，减少重新分配的可能性）。 注意事项消息重试对顺序消息的影响对于一个有着先后顺序的消息A、B，正常情况下应该是A先发送完成后再发送B，但是在异常情况下，在A发送失败的情况下，B发送成功，而A由于重试机制在B发送完成之后重试发送成功了。这时对于本身顺序为AB的消息顺序变成了BA 消息producer发送逻辑的控制消息producer在发送消息的时候，对于同一个broker连接是存在多个未确认的消息在同时发送的，也就是存在上面场景说到的情况，虽然A和B消息是顺序的，但是由于存在未知的确认关系，有可能存在A发送失败，B发送成功，A需要重试的时候顺序关系就变成了BA。简之一句就是在发送B时A的发送状态是未知的。针对以上的问题，严格的顺序消费还需要以下参数支持：max.in.flight.requests.per.connection这个参数官方文档的解释是： The maximum number of unacknowledged requests the client will send on a single connection before blocking. Note that if this setting is set to be greater than 1 and there are failed sends, there is a risk of message re-ordering due to retries (i.e., if retries are enabled). 大体意思是： 在发送阻塞前对于每个连接，正在发送但是发送状态未知的最大消息数量。如果设置大于1，那么就有可能存在有发送失败的情况下，因为重试发送导致的消息乱序问题。所以我们应该将其设置为1，保证在后一条消息发送前，前一条的消息状态已经是可知的。","categories":[{"name":"原创文章","slug":"原创文章","permalink":"http://www.lpnote.com/categories/原创文章/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.lpnote.com/tags/大数据/"},{"name":"kafka","slug":"kafka","permalink":"http://www.lpnote.com/tags/kafka/"}]},{"title":"metrics系列之quickstart","slug":"dropwizard-metrics-series-quickstart","date":"2017-01-15T16:00:00.000Z","updated":"2019-02-13T08:59:06.520Z","comments":true,"path":"2017/01/15/dropwizard-metrics-series-quickstart/","link":"","permalink":"http://www.lpnote.com/2017/01/15/dropwizard-metrics-series-quickstart/","excerpt":"dropwizard/metrics Metrics是一个Java库，它能在你的生产环境中，为你的代码中提供无与伦比的洞察力。 Metrics提供了一个强大的工具包，用于测量生产环境中关键组件的行为。 它提供了常见模块支持，如：Jetty，Logback，Log4j，Apache HttpClient，Ehcache，JDBI，Jersey和报告后端（如Ganglia和Graphite）的公共库的模块，Metrics为您提供全栈可见性。 QuickStarted下面是我们开始一个quickstart的步骤 Setting Up Maven需要加入maven依赖：12345&lt;dependency&gt; &lt;groupId&gt;io.dropwizard.metrics&lt;/groupId&gt; &lt;artifactId&gt;metrics-core&lt;/artifactId&gt; &lt;version&gt;$&#123;metrics.version&#125;&lt;/version&gt;&lt;/dependency&gt; 注意：你要确保metrics.version属性已经在你的pom文件中被定义，当前的版本是3.1.0 好的，现在是时候开始为你的程序提供指标化的数据了","text":"dropwizard/metrics Metrics是一个Java库，它能在你的生产环境中，为你的代码中提供无与伦比的洞察力。 Metrics提供了一个强大的工具包，用于测量生产环境中关键组件的行为。 它提供了常见模块支持，如：Jetty，Logback，Log4j，Apache HttpClient，Ehcache，JDBI，Jersey和报告后端（如Ganglia和Graphite）的公共库的模块，Metrics为您提供全栈可见性。 QuickStarted下面是我们开始一个quickstart的步骤 Setting Up Maven需要加入maven依赖：12345&lt;dependency&gt; &lt;groupId&gt;io.dropwizard.metrics&lt;/groupId&gt; &lt;artifactId&gt;metrics-core&lt;/artifactId&gt; &lt;version&gt;$&#123;metrics.version&#125;&lt;/version&gt;&lt;/dependency&gt; 注意：你要确保metrics.version属性已经在你的pom文件中被定义，当前的版本是3.1.0 好的，现在是时候开始为你的程序提供指标化的数据了 Meters一个Meter负责测量一类事件的速率（比如：请求速率/ requests per sencond）, 它提供了1分钟，5分钟，15分钟的平均值 123456private final Meter requests = metrics.meter(\"requests\");public void handleRequest(Request request, Response response) &#123; requests.mark(); // etc&#125; 这个meter将会测量请求速率，单位为requests per seconds Console Reporter就如听起来的一样，它会将结果汇报给标准输出，这个报告会每隔1秒钟打印一次12345ConsoleReporter reporter = ConsoleReporter.forRegistry(metrics) .convertRatesTo(TimeUnit.SECONDS) .convertDurationsTo(TimeUnit.MILLISECONDS) .build(); reporter.start(1, TimeUnit.SECONDS); so，完整的示例看起来是这个样子的：12345678910111213141516171819202122232425262728package sample; import com.codahale.metrics.*; import java.util.concurrent.TimeUnit; public class GetStarted &#123; static final MetricRegistry metrics = new MetricRegistry(); public static void main(String args[]) &#123; startReport(); Meter requests = metrics.meter(\"requests\"); requests.mark(); wait5Seconds(); &#125; static void startReport() &#123; ConsoleReporter reporter = ConsoleReporter.forRegistry(metrics) .convertRatesTo(TimeUnit.SECONDS) .convertDurationsTo(TimeUnit.MILLISECONDS) .build(); reporter.start(1, TimeUnit.SECONDS); &#125; static void wait5Seconds() &#123; try &#123; Thread.sleep(5*1000); &#125; catch(InterruptedException e) &#123;&#125; &#125;&#125; pom文件的定义：123456789101112131415161718192021&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;somegroup&lt;/groupId&gt; &lt;artifactId&gt;sample&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;Example project for Metrics&lt;/name&gt; &lt;properties&gt; &lt;metrics.version&gt;3.1.0&lt;/metrics.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;io.dropwizard.metrics&lt;/groupId&gt; &lt;artifactId&gt;metrics-core&lt;/artifactId&gt; &lt;version&gt;$&#123;metrics.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 运行:1mvn package exec:java -Dexec.mainClass=sample.GetStarted The RegistryMetricsRegistry是整个项目的核心类，它是所有测量的容量，可以通过如下方式创建它：1final MetricRegistry metrics = new MetricRegistry(); 你可能想把这个实例集成到你的应用生命周期中（比如使用你的依赖注入），这没有任何问题,但是建议还是静态实例化是一个最佳实践。 Gauges一个gauges代表了一个瞬时度量值，例如：我们想要度量一个队列中的pending jobs数据量：1234567891011121314public class QueueManager &#123; private final Queue queue; public QueueManager(MetricRegistry metrics, String name) &#123; this.queue = new Queue(); metrics.register(MetricRegistry.name(QueueManager.class, name, \"size\"), new Gauge&lt;Integer&gt;() &#123; @Override public Integer getValue() &#123; return queue.size(); &#125; &#125;); &#125;&#125; 当这个gauge被度量时，它将返回这个队列中的job数量 每个metric在注册表(registry)中都有一个唯一的名字，这个名字一般使用.号分隔，如：things.count，又或者:”com.example.Thing.latency”。MetricRegistry有一个静态的方法用于构造这些命名：1MetricRegistry.name(QueueManager.class, \"jobs\", \"size\") 以上的例子会返回: “com.example.QueueManager.jobs.size” 对于大多数queue或者像queue一样的数据结构，通常不希望只返回队列的大小，大多数java.util和java.util.concurrent包的size()实现都是O(n)的复杂度，这表示你的gauge将会比较慢（潜在地可能存在加锁） Counter一个Counter实际上也是一个gauge，它是关于AtomicLong的一个泛型实例。你可以自增或者自减它的值，例如，我们可以以不同的方式来度量队列中的pending job：1234567891011private final Counter pendingJobs = metrics.counter(name(QueueManager.class, \"pending-jobs\"));public void addJob(Job job) &#123; pendingJobs.inc(); queue.offer(job);&#125;public Job takeJob() &#123; pendingJobs.dec(); return queue.take();&#125; 每次该counter被计算时，它将会返回这个队列中的job数量 就上面看到的，counters的API是不同的，#counter(String) 取代了#register(String,Metrics)，然而你也可以使用register，然后创建一个你自己的Counter实例，但是要说一下的是，#counter(String) 帮你做了这一切，并且允许你通过该名称进行引用复用。 HistogramsHistogram是一个流式数据的统计分布，除了比如：最大值，最小值，平均值等，它还支持中位数，75%，90%，95%，98%，99%和99.9%的百分线123456private final Histogram responseSizes = metrics.histogram(name(RequestHandler.class, \"response-sizes\"));public void handleRequest(Request request, Response response) &#123; // etc responseSizes.update(response.getContent().length);&#125; 该histogram将会度量响应的大小 Timer一个Timer提供了某个程序的耗时和调用速率1234567891011private final Timer responses = metrics.timer(name(RequestHandler.class, \"responses\"));public String handleRequest(Request request, Response response) &#123; final Timer.Context context = responses.time(); try &#123; // etc; return \"OK\"; &#125; finally &#123; context.stop(); &#125;&#125; 该timer会度量该段代码的调用速率以及调用的时耗。 Health ChecksMetrics也提供了通过metrics-healthchecks模块集中式健康检测的能力首先，创建一个健康检测注册表的实例：1final HealthCheckRegistry healthChecks = new HealthCheckRegistry(); 其次，实现一个健康检测的子类:12345678910111213141516public class DatabaseHealthCheck extends HealthCheck &#123; private final Database database; public DatabaseHealthCheck(Database database) &#123; this.database = database; &#125; @Override public HealthCheck.Result check() throws Exception &#123; if (database.isConnected()) &#123; return HealthCheck.Result.healthy(); &#125; else &#123; return HealthCheck.Result.unhealthy(\"Cannot connect to \" + database.getUrl()); &#125; &#125;&#125; 最后注册它：1healthChecks.register(\"postgres\", new DatabaseHealthCheck(database)); 运行健康检测：123456789101112final Map&lt;String, HealthCheck.Result&gt; results = healthChecks.runHealthChecks();for (Entry&lt;String, HealthCheck.Result&gt; entry : results.entrySet()) &#123; if (entry.getValue().isHealthy()) &#123; System.out.println(entry.getKey() + \" is healthy\"); &#125; else &#123; System.err.println(entry.getKey() + \" is UNHEALTHY: \" + entry.getValue().getMessage()); final Throwable e = entry.getValue().getError(); if (e != null) &#123; e.printStackTrace(); &#125; &#125;&#125; Metrics comes with a pre-built health check: ThreadDeadlockHealthCheck, which uses Java’s built-in thread deadlock detection to determine if any threads are deadlocked. Reporting Via JMX通过JMX汇报指标12final JmxReporter reporter = JmxReporter.forRegistry(registry).build();reporter.start(); 当上面的reporter开启后，你就可以通过JConsole或者VisualVM进行MBean查看了： Reporting Via HTTP提供一个Servlet，该Servlet提供了所有metrics的json表现形式。它同样会运行健康检测，打印thread dump，提供简单的ping pong响应用于负载均衡。（当然也有分开的单独的Servlet提供单独的功能，例如：MetricsServlet, HealthCheckServlet, ThreadDumpServlet, and PingServlet） 如果需要使用到上面的东西，需要以下依赖：12345&lt;dependency&gt; &lt;groupId&gt;io.dropwizard.metrics&lt;/groupId&gt; &lt;artifactId&gt;metrics-servlets&lt;/artifactId&gt; &lt;version&gt;$&#123;metrics.version&#125;&lt;/version&gt;&lt;/dependency&gt; Other Reporting除了以上的JMX和HTTP Reporter以外，还有以下一些Reporter: STDOUT, using ConsoleReporter from metrics-core CSV files, using CsvReporter from metrics-core SLF4J loggers, using Slf4jReporter from metrics-core Ganglia, using GangliaReporter from metrics-ganglia Graphite, using GraphiteReporter from metrics-graphite","categories":[{"name":"翻译文章","slug":"翻译文章","permalink":"http://www.lpnote.com/categories/翻译文章/"}],"tags":[{"name":"metrics","slug":"metrics","permalink":"http://www.lpnote.com/tags/metrics/"}]},{"title":"kafka消息可靠性","slug":"reliability-of-kafka-message","date":"2017-01-15T12:09:04.000Z","updated":"2019-02-13T08:59:06.536Z","comments":true,"path":"2017/01/15/reliability-of-kafka-message/","link":"","permalink":"http://www.lpnote.com/2017/01/15/reliability-of-kafka-message/","excerpt":"kafka消息可靠性，这里涉及几个方面： 发送可靠性 接收可靠性 存储可靠性 发送可靠性kafka新版client(0.10.x)使用java重新实现。使用的是异步方式发送消息，即消息提交给KafkaProducer的send方法后，实际上是将该消息放入了它本身的一个后台发送队列，然后再有一个后台线程不断地从队列中取出消息进行发送，发送成功后会回调send方法的callback（如果没有，就不用回调了）。 从以上的流程来看，kafka客户端的发送流程是一个异步化的流程，kafka客户端会累积一定量的消息后统一组装成一个批量消息发出，这个的触发条件是： 消息量达到了batch.size的大小或者等待批量的时间超过了linger.ms时间。 此外还要注意一下发送方消息的堆积问题，当程序的发送速率大于发送到broker的速率时，会产生消费在发送方堆积，堆积的策略控制主要由参数buffer.memory 以及max.block.ms。buffer.memory设置了可使用的buffer内存，max.block.ms是指在buffer满的情况下可以阻塞多长时间，超过这个时间则抛出异常。 消息失败重试设置失败重试的次数为一个很大的数值,如Integer.MAX_VALUE，对应properties的设置为： 配置 默认值 建议值 retries 0 Integer.MAX_VALUE 消息异步转同步对于消息异步转同步：使用future.get()等待消息发送返回结果,如：12Future&lt;RecordMetadata&gt; future = producer.send(new ProducerRecord&lt;String, String&gt;(\"test.testTopic\", \"key\",\"value\"));RecordMetadata metadata = future.get(); //等待发送结果返回 这种用法可能会导致性能下降比较厉害，也可以通过send(message,callback)的方式，在消息发送失败时通过callback记录失败并处理 顺序消息kafka默认情况下是批量发送，批量发送存在消息积累再发送的过程，为了达到消息send后立刻发送到broker的要求，对应properties设置： 配置 默认值 建议值 max.in.flight.requests.per.connection 5 1 其中max.in.flight.requests.per.connection以及retries主要应用于顺序消息场景，顺序场景中需要设置为：max.in.flight.requests.per.connection = 1","text":"kafka消息可靠性，这里涉及几个方面： 发送可靠性 接收可靠性 存储可靠性 发送可靠性kafka新版client(0.10.x)使用java重新实现。使用的是异步方式发送消息，即消息提交给KafkaProducer的send方法后，实际上是将该消息放入了它本身的一个后台发送队列，然后再有一个后台线程不断地从队列中取出消息进行发送，发送成功后会回调send方法的callback（如果没有，就不用回调了）。 从以上的流程来看，kafka客户端的发送流程是一个异步化的流程，kafka客户端会累积一定量的消息后统一组装成一个批量消息发出，这个的触发条件是： 消息量达到了batch.size的大小或者等待批量的时间超过了linger.ms时间。 此外还要注意一下发送方消息的堆积问题，当程序的发送速率大于发送到broker的速率时，会产生消费在发送方堆积，堆积的策略控制主要由参数buffer.memory 以及max.block.ms。buffer.memory设置了可使用的buffer内存，max.block.ms是指在buffer满的情况下可以阻塞多长时间，超过这个时间则抛出异常。 消息失败重试设置失败重试的次数为一个很大的数值,如Integer.MAX_VALUE，对应properties的设置为： 配置 默认值 建议值 retries 0 Integer.MAX_VALUE 消息异步转同步对于消息异步转同步：使用future.get()等待消息发送返回结果,如：12Future&lt;RecordMetadata&gt; future = producer.send(new ProducerRecord&lt;String, String&gt;(\"test.testTopic\", \"key\",\"value\"));RecordMetadata metadata = future.get(); //等待发送结果返回 这种用法可能会导致性能下降比较厉害，也可以通过send(message,callback)的方式，在消息发送失败时通过callback记录失败并处理 顺序消息kafka默认情况下是批量发送，批量发送存在消息积累再发送的过程，为了达到消息send后立刻发送到broker的要求，对应properties设置： 配置 默认值 建议值 max.in.flight.requests.per.connection 5 1 其中max.in.flight.requests.per.connection以及retries主要应用于顺序消息场景，顺序场景中需要设置为：max.in.flight.requests.per.connection = 1 综合以上配置示例：1234567891011121314Properties props = new Properties();props.put(\"bootstrap.servers\", \"localhost:9092\");props.put(\"acks\", \"1\"); //这里是只要求leader响应就OK，更高的要求则应该设置成\"all\"props.put(\"retries\", Integer.MAX_VALUE);props.put(\"max.in.flight.requests.per.connection\",1);props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); //这里是key的序列化类props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");//这里是value的序列化类Producer&lt;String, String&gt; producer = new KafkaProducer&lt;String,String&gt;(props);for (int i = 0; i &lt; 1000000; i++) &#123; Future&lt;RecordMetadata&gt; future = producer.send(new ProducerRecord&lt;String, String&gt;(\"test.testTopic\",\"key\",\"value\")); RecordMetadata metadata = future.get(); //关键的这一步，同步等待发送完成&#125;producer.close(); 接收可靠性新版的java客户端（0.10.0.0）已经变更接收线程为单线程接收处理。同时客户端默认情况下是自动提交offset，这样可能存在消息丢失的可能性，比如客户端接收到一批消息并进行处理，在处理过程中达到了客户端offset定时提交的时间点，这批数据的offset被提交，但是可能这批数据的处理还没有结束，甚至这些数据可能还存在一些数据处理不了或者处理出错，甚至出现宕机的可能性，这时未处理的消息将会丢失，因为offset已经提交，下次读取会从新的offset处读取。所以要保证消息的可靠接收，需要将enable.auto.commit设置为false，防止程序自动提交，应该由应用程序处理完成后手动提交。示例：123456789101112131415161718192021Properties props = new Properties();props.put(\"bootstrap.servers\", \"localhost:9092\");props.put(\"group.id\", \"testsub\");props.put(\"enable.auto.commit\", \"false\");props.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\");props.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\");KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;String,String&gt;(props);consumer.subscribe(Arrays.asList(\"test.testTopic\",\"testsub\"));while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); Map&lt;TopicPartition,OffsetAndMetadata&gt; commitMap = new HashMap(10); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; logger.info(record.value()); //模拟消息 commitMap.put(new TopicPartition(record.topic(),record.partition()),new OffsetAndMetadata(record.offset()+1)); &#125; if(commitMap.isEmpty())&#123; continue; &#125; consumer.commitSync(commitMap);&#125; 存储可靠性刷盘时机broker的刷盘时机主要是以下两个参数控制：log.flush.interval.ms 日志刷盘的时间间隔，每隔多少时间将消息刷到磁盘上log.flush.interval.messages 日志刷盘的消息量，每积累多少条消息将消息刷到磁盘上 副本数在创建消息Topic的时候需要指定消息的副本数 replicas一般建议设置成3保证消息的可靠，再结合客户端发送方的ack参数，当ack参数设置为0表示不等待broker响应就发送下一条消息，当ack设置为1则表示需要等待leader响应，当ack设置为all则表示需要等待所有的replicas ISR都响应后才返回响应，其中all是最高可靠级别了，但是同时也降低了吞吐率。","categories":[{"name":"原创文章","slug":"原创文章","permalink":"http://www.lpnote.com/categories/原创文章/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.lpnote.com/tags/大数据/"},{"name":"kafka","slug":"kafka","permalink":"http://www.lpnote.com/tags/kafka/"}]},{"title":"kafka-manager 0.10 试玩","slug":"kafka-manager-play-with-0.10","date":"2017-01-11T16:00:00.000Z","updated":"2019-02-13T08:59:06.532Z","comments":true,"path":"2017/01/11/kafka-manager-play-with-0.10/","link":"","permalink":"http://www.lpnote.com/2017/01/11/kafka-manager-play-with-0.10/","excerpt":"项目简介kafka-manager项目提供了kafka集群的常用管理和监控功能，官方描述如下： A tool for managing Apache Kafka.It supports the following: Manage multiple clusters Easy inspection of cluster state (topics, consumers, offsets, brokers, replica distribution, partition distribution) Run preferred replica election Generate partition assignments with option to select brokers to use Run reassignment of partition (based on generated assignments) Create a topic with optional topic configs (0.8.1.1 has different configs than 0.8.2+) Delete topic (only supported on 0.8.2+ and remember set delete.topic.enable=true in broker config) Topic list now indicates topics marked for deletion (only supported on 0.8.2+) Batch generate partition assignments for multiple topics with option to select brokers to use Batch run reassignment of partition for multiple topics Add partitions to existing topic Update config for existing topic Optionally enable JMX polling for broker level and topic level metrics. Optionally filter out consumers that do not have ids/ owners/ &amp; offsets/ directories in zookeeper. Kafka-manger项目：http://www.github.com/yahoo/kafka-manager 该项目目前尚未支持kafka 0.10.x 系列版本。不过在其的PR中有关于支持0.10版本的PR：https://github.com/yahoo/kafka-manager/pull/282","text":"项目简介kafka-manager项目提供了kafka集群的常用管理和监控功能，官方描述如下： A tool for managing Apache Kafka.It supports the following: Manage multiple clusters Easy inspection of cluster state (topics, consumers, offsets, brokers, replica distribution, partition distribution) Run preferred replica election Generate partition assignments with option to select brokers to use Run reassignment of partition (based on generated assignments) Create a topic with optional topic configs (0.8.1.1 has different configs than 0.8.2+) Delete topic (only supported on 0.8.2+ and remember set delete.topic.enable=true in broker config) Topic list now indicates topics marked for deletion (only supported on 0.8.2+) Batch generate partition assignments for multiple topics with option to select brokers to use Batch run reassignment of partition for multiple topics Add partitions to existing topic Update config for existing topic Optionally enable JMX polling for broker level and topic level metrics. Optionally filter out consumers that do not have ids/ owners/ &amp; offsets/ directories in zookeeper. Kafka-manger项目：http://www.github.com/yahoo/kafka-manager 该项目目前尚未支持kafka 0.10.x 系列版本。不过在其的PR中有关于支持0.10版本的PR：https://github.com/yahoo/kafka-manager/pull/282 本地合并步骤git clone https://github.com/yahoo/kafka-manager.gitgit fetch origin pull/282/head:0.10.0git checkout 0.10.0 代码编译cd kafka-manager./sbt clean dist如果是全新的编译环境，编译时间会非常久（主要原因是要初始化sbt编译系统的各种，比如下载ivy2依赖的jar包等）编译完成后在目录 target/universal目录下会出现zip文件即为编译打包成功的部署包。 部署解压到服务器某目录，只需要配置conf/application.conf文件内容即可：kafka-manager.zkhosts=”172.19.11.197:2181”以上ZK地址根据实际情况修改成合适的地址。 启动bin目录启动，./kafka-manger kafka-manger特性研究 功能模块 功能点 是否具备 备注 集群管理 添加集群 √ 修改集群 √ 启/禁用集群 √ 删除集群 √ 集群列表 √ Broker管理 Broker列表 √ 指标 Broker详情 √ 消息总量/Topic列表 Topic管理 Topic创建 √ Topic列表 √ 指标 Topic详情 √ Topic删除 √ 分区调整 √ 分区信息 √ 订阅管理 Consumer列表 √ 需要JMX开启支持 Consumer详情 √ 权限管理 登录 √ 提供basic auth，后台可配置用户名、密码登录 Broker 指标 Message in /sec Bytes in /sec Bytes out /sec Bytes rejected /sec Failed fetch request /sec Failed produce request /sec 需要Broker开启JMX Topic 指标 Replication （副本数） Number of Partitions (分区数) Sum of partition offsets (offset大小，需要开启JMX支持） Total number of Brokers （Broker总数） Number of Brokers for Topic （Topic所占Broker数） Preferred Replicas % （） Brokers Skewed % （Broker 均衡率） Brokers Spread % （Broker 扩散率） Under-replicated % （处于同步状态的比率） 需要JMX开启支持 风险项 0.10版支持 master上尚未合并0.10版本的PR，需要自己合并测试后可用 附一些界面截图：","categories":[{"name":"原创文章","slug":"原创文章","permalink":"http://www.lpnote.com/categories/原创文章/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.lpnote.com/tags/大数据/"},{"name":"kafka","slug":"kafka","permalink":"http://www.lpnote.com/tags/kafka/"}]},{"title":"dubbo服务可用性监控手段","slug":"the-method-to-monitor-availability-of-services-in-dubbo","date":"2016-06-19T16:00:00.000Z","updated":"2019-02-13T08:59:06.544Z","comments":true,"path":"2016/06/19/the-method-to-monitor-availability-of-services-in-dubbo/","link":"","permalink":"http://www.lpnote.com/2016/06/19/the-method-to-monitor-availability-of-services-in-dubbo/","excerpt":"前言随着互联网的发展，网站应用的规模不断扩大，常规的垂直应用架构已无法应对，分布式服务架构以及流动计算架构势在必行，亟需一个治理系统确保架构有条不紊的演进。在采用Dubbo作为服务化框架的过程中需要对服务接口可用性进行监控，程序需要通过监控平台去监控所有业务Dubbo服务接口的可用性。希望的是在1分钟内如果有服务挂掉会有监控报警发出，并且有监控页面显示监控数据，且有数据报表产出某个时间单位内的服务可用性。 服务可用性的定义： 在一段时间内，服务可用时间/（服务可用时间+服务不可用时间）* 100%","text":"前言随着互联网的发展，网站应用的规模不断扩大，常规的垂直应用架构已无法应对，分布式服务架构以及流动计算架构势在必行，亟需一个治理系统确保架构有条不紊的演进。在采用Dubbo作为服务化框架的过程中需要对服务接口可用性进行监控，程序需要通过监控平台去监控所有业务Dubbo服务接口的可用性。希望的是在1分钟内如果有服务挂掉会有监控报警发出，并且有监控页面显示监控数据，且有数据报表产出某个时间单位内的服务可用性。 服务可用性的定义： 在一段时间内，服务可用时间/（服务可用时间+服务不可用时间）* 100% 这里有几个问题需要解决： 监控平台监控怎么与业务平台松耦合？ 首先监控平台不可能去引用所有一大堆的其它业务系统的Dubbo接口的SDK，这样无法保证稳定且无法扩展。 如果有新的业务需要监控，是不是需要新加入该业务的SDK接口包呢？ 如果某个业务平台的SDK升级了，是不是监控平台也需要跟着升级SDK呢？ 所以首先想到的就是监控平台能够在不引入业务平台的SDK的前提下就可以监控到业务平台的服务状况。这里想到的就是使用Dubbo提供的泛化调用，可以在事先不知道业务方SDK的情况下调用业务接口。 监控平台怎么对业务判活？ 如果要求业务方在自己的服务中强行嵌入一个健康检测的接口，对于业务方来说总有种被强奸的感觉。幸好Dubbo也提供了一个回声测试接口，什么是回声测试？就是你发给它什么东西，它返回给你什么东西。就像你对着大山喊了一声“hi”，大山回声一句“hi”一样的道理，证明对方是有能力回复你的，从某种程序上证明对方仍然处于存活状态。 在Dubbo中，泛接口调用方式主要用于客户端没有API接口及模型类元的情况，参数及返回值中的所有POJO均用Map表示，通常用于框架集成，比如：实现一个通用的服务测试框架，可通过GenericService调用所有服务实现。回声测试用于检测服务是否可用，回声测试按照正常请求流程执行，能够测试整个调用是否通畅，可用于监控。所有服务自动实现EchoService接口，只需将任意服务引用强制转型为EchoService，即可使用。 dubbo回声测试1&lt;dubbo:reference id=\"memberService\" interface=\"com.xxx.MemberService\" /&gt; 1234567MemberService memberService = ctx.getBean(\"memberService\"); // 远程服务引用EchoService echoService = (EchoService) memberService; // 强制转型为EchoServiceString status = echoService.$echo(\"OK\"); // 回声测试可用性assert(status.equals(\"OK\")); dubbo泛化调用1&lt;dubbo:reference id=\"barService\" interface=\"com.foo.BarService\" generic=\"true\" /&gt; 12GenericService barService = (GenericService) applicationContext.getBean(\"barService\");Object result = barService.$invoke(\"sayHello\", new String[] &#123; \"java.lang.String\" &#125;, new Object[] &#123; \"World\" &#125;); 123456789101112131415161718192021import com.alibaba.dubbo.rpc.service.GenericService; ... // 引用远程服务 ReferenceConfig&lt;GenericService&gt; reference = new ReferenceConfig&lt;GenericService&gt;(); // 该实例很重量，里面封装了所有与注册中心及服务提供方连接，请缓存reference.setInterface(\"com.xxx.XxxService\"); // 弱类型接口名 reference.setVersion(\"1.0.0\"); reference.setGeneric(true); // 声明为泛化接口 GenericService genericService = reference.get(); // 用com.alibaba.dubbo.rpc.service.GenericService可以替代所有接口引用 // 基本类型以及Date,List,Map等不需要转换，直接调用 Object result = genericService.$invoke(\"sayHello\", new String[] &#123;\"java.lang.String\"&#125;, new Object[] &#123;\"world\"&#125;); // 用Map表示POJO参数，如果返回值为POJO也将自动转成Map Map&lt;String, Object&gt; person = new HashMap&lt;String, Object&gt;(); person.put(\"name\", \"xxx\"); person.put(\"password\", \"yyy\"); Object result = genericService.$invoke(\"findPerson\", new String[]&#123;\"com.xxx.Person\"&#125;, new Object[]&#123;person&#125;); // 如果返回POJO将自动转成Map ... 假设存在POJO如：12345678910111213141516171819202122package com.xxx;public class PersonImpl implements Person &#123; private String name; private String password; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getPassword() &#123; return password; &#125; public void setPassword(String password) &#123; this.password = password; &#125;&#125; 则POJO数据：123Person person = new PersonImpl(); person.setName(\"xxx\"); person.setPassword(\"yyy\"); 可用下面Map表示：1234Map&lt;String, Object&gt; map = new HashMap&lt;String, Object&gt;(); map.put(&quot;class&quot;, &quot;com.xxx.PersonImpl&quot;); // 注意：如果参数类型是接口，或者List等丢失泛型，可通过class属性指定类型。map.put(&quot;name&quot;, &quot;xxx&quot;); map.put(&quot;password&quot;, &quot;yyy&quot;); 以上引用至Dubbo官方文档：泛化引用 回声测试与泛化调用之冲突通过泛化引用与回声测试两者结合，刚好能满足我们的监控需求，监控平台通过注册中心获取所有的服务接口，并通过泛化引用方式引用服务，并调用服务的回声接口测试可用性。 一切看起来都那么美好，但是现实总是那么残酷。对于Dubbo目前的处境来说，泛化引用和回声测试同时使用时会产生不兼容，究其原因是因为：Dubbo的泛化引用调用和回声测试是两个不同的Filter，泛化调用Filter被用于客户端执行，而回声测试被用于服务端，在进行回声测试时并没有对泛化调用进行回应 冲突分析Dubbo回声测试的Filter：12345678910@Activate(group = Constants.PROVIDER, order = -110000)public class EchoFilter implements Filter &#123; public Result invoke(Invoker&lt;?&gt; invoker, Invocation inv) throws RpcException &#123; if(inv.getMethodName().equals(Constants.$ECHO) &amp;&amp; inv.getArguments() != null &amp;&amp; inv.getArguments().length == 1 ) return new RpcResult(inv.getArguments()[0]); return invoker.invoke(inv); &#125;&#125; Dubbo泛化引用调用的Filter：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142@Activate(group = Constants.CONSUMER, value = Constants.GENERIC_KEY, order = 20000)public class GenericImplFilter implements Filter &#123; private static final Logger logger = LoggerFactory.getLogger(GenericImplFilter.class); private static final Class&lt;?&gt;[] GENERIC_PARAMETER_TYPES = new Class&lt;?&gt;[] &#123;String.class, String[].class, Object[].class&#125;; public Result invoke(Invoker&lt;?&gt; invoker, Invocation invocation) throws RpcException &#123; String generic = invoker.getUrl().getParameter(Constants.GENERIC_KEY); if (ProtocolUtils.isGeneric(generic) &amp;&amp; ! Constants.$INVOKE.equals(invocation.getMethodName()) &amp;&amp; invocation instanceof RpcInvocation) &#123; RpcInvocation invocation2 = (RpcInvocation) invocation; String methodName = invocation2.getMethodName(); Class&lt;?&gt;[] parameterTypes = invocation2.getParameterTypes(); Object[] arguments = invocation2.getArguments(); String[] types = new String[parameterTypes.length]; for (int i = 0; i &lt; parameterTypes.length; i ++) &#123; types[i] = ReflectUtils.getName(parameterTypes[i]); &#125; Object[] args; if (ProtocolUtils.isBeanGenericSerialization(generic)) &#123; args = new Object[arguments.length]; for(int i = 0; i &lt; arguments.length; i++) &#123; args[i] = JavaBeanSerializeUtil.serialize(arguments[i], JavaBeanAccessor.METHOD); &#125; &#125; else &#123; args = PojoUtils.generalize(arguments); &#125; invocation2.setMethodName(Constants.$INVOKE); invocation2.setParameterTypes(GENERIC_PARAMETER_TYPES); invocation2.setArguments(new Object[] &#123;methodName, types, args&#125;); Result result = invoker.invoke(invocation2); if (! result.hasException()) &#123; Object value = result.getValue(); try &#123; Method method = invoker.getInterface().getMethod(methodName, parameterTypes); if (ProtocolUtils.isBeanGenericSerialization(generic)) &#123; if (value == null) &#123; return new RpcResult(value); &#125; else if (value instanceof JavaBeanDescriptor) &#123; return new RpcResult(JavaBeanSerializeUtil.deserialize((JavaBeanDescriptor)value)); &#125; else &#123; throw new RpcException( new StringBuilder(64) .append(\"The type of result value is \") .append(value.getClass().getName()) .append(\" other than \") .append(JavaBeanDescriptor.class.getName()) .append(\", and the result is \") .append(value).toString()); &#125; &#125; else &#123; return new RpcResult(PojoUtils.realize(value, method.getReturnType(), method.getGenericReturnType())); &#125; &#125; catch (NoSuchMethodException e) &#123; throw new RpcException(e.getMessage(), e); &#125; &#125; else if (result.getException() instanceof GenericException) &#123; GenericException exception = (GenericException) result.getException(); try &#123; String className = exception.getExceptionClass(); Class&lt;?&gt; clazz = ReflectUtils.forName(className); Throwable targetException = null; Throwable lastException = null; try &#123; targetException = (Throwable) clazz.newInstance(); &#125; catch (Throwable e) &#123; lastException = e; for (Constructor&lt;?&gt; constructor : clazz.getConstructors()) &#123; try &#123; targetException = (Throwable) constructor.newInstance(new Object[constructor.getParameterTypes().length]); break; &#125; catch (Throwable e1) &#123; lastException = e1; &#125; &#125; &#125; if (targetException != null) &#123; try &#123; Field field = Throwable.class.getDeclaredField(\"detailMessage\"); if (! field.isAccessible()) &#123; field.setAccessible(true); &#125; field.set(targetException, exception.getExceptionMessage()); &#125; catch (Throwable e) &#123; logger.warn(e.getMessage(), e); &#125; result = new RpcResult(targetException); &#125; else if (lastException != null) &#123; throw lastException; &#125; &#125; catch (Throwable e) &#123; throw new RpcException(\"Can not deserialize exception \" + exception.getExceptionClass() + \", message: \" + exception.getExceptionMessage(), e); &#125; &#125; return result; &#125; if (invocation.getMethodName().equals(Constants.$INVOKE) &amp;&amp; invocation.getArguments() != null &amp;&amp; invocation.getArguments().length == 3 &amp;&amp; ProtocolUtils.isGeneric(generic)) &#123; Object[] args = (Object[]) invocation.getArguments()[2]; if (ProtocolUtils.isJavaGenericSerialization(generic)) &#123; for (Object arg : args) &#123; if (!(byte[].class == arg.getClass())) &#123; error(byte[].class.getName(), arg.getClass().getName()); &#125; &#125; &#125; else if (ProtocolUtils.isBeanGenericSerialization(generic)) &#123; for(Object arg : args) &#123; if (!(arg instanceof JavaBeanDescriptor)) &#123; error(JavaBeanDescriptor.class.getName(), arg.getClass().getName()); &#125; &#125; &#125; ((RpcInvocation)invocation).setAttachment( Constants.GENERIC_KEY, invoker.getUrl().getParameter(Constants.GENERIC_KEY)); &#125; return invoker.invoke(invocation); &#125; private void error(String expected, String actual) throws RpcException &#123; throw new RpcException( new StringBuilder(32) .append(\"Generic serialization [\") .append(Constants.GENERIC_SERIALIZATION_NATIVE_JAVA) .append(\"] only support message type \") .append(expected) .append(\" and your message type is \") .append(actual).toString()); &#125;&#125; 从以上两个源码中也可以看出，泛化调用中没有对回声测试进行处理。 冲突解决解决冲突有两种方式： 修改回声测试调用Filter，并对泛化调用进行处理 增加新自定义的Provider端Filter，并且排在回声调用Filter之前对泛化调用中的回声测试进行处理返回。 由于Dubbo的扩展性做得非常棒，这里只需要自己定义一个Filter来实现就可以了： GenericEchoFilter.java:123456789101112131415161718192021222324@Activate( group = &#123;\"provider\"&#125;, order = -999999)public class GenericEchoFilter implements Filter &#123; private Logger logger = LoggerFactory.getLogger(GenericEchoFilter.class); public GenericEchoFilter() &#123; &#125; public Result invoke(Invoker&lt;?&gt; invoker, Invocation invocation) throws RpcException &#123; if(invocation.getMethodName().equals(\"$invoke\") &amp;&amp; invocation.getArguments() != null &amp;&amp; invocation.getArguments().length == 3 &amp;&amp; !ProtocolUtils.isGeneric(invoker.getUrl().getParameter(\"generic\"))) &#123; Object[] arguments = invocation.getArguments(); Object realMethod = arguments[0]; String[] argsTypes = (String[])((String[])arguments[1]); Object[] args = (Object[])((Object[])arguments[2]); if(\"$echo\".equals(realMethod) &amp;&amp; argsTypes != null &amp;&amp; argsTypes.length == 1 &amp;&amp; args != null &amp;&amp; args.length == 1) &#123; return new RpcResult(args[0]); &#125; &#125; return invoker.invoke(invocation); &#125;&#125; 注意以上的order属性值要大于回声测试Filter中的order值，这样才能保证先于回声测试就进行响应。 添加了这个后需要在自己的工程或者工具jar包META-INF/dubbo/下新增filter设定文件com.alibaba.dubbo.rpc.Filter，内容如下：1genericecho=com.xxx.dubbo.filter.GenericEchoFilter 这样操作过后，Dubbo在启动时会通过SPI方式自动扫描和加载自定义的Filter，这样我们的自定义Filter就自动生效了。 总结Dubbo是阿里开源的一款精品服务化RPC框架，它的扩展点非常多，有着非常强的可定制化功能。本文中基于监控平台的远程泛化引用调用和回声测试中通过自定义的Filter完美地解决了服务化监控的问题。最多的Dubbo扩展特性还需要更多的探索与研究！","categories":[{"name":"原创文章","slug":"原创文章","permalink":"http://www.lpnote.com/categories/原创文章/"}],"tags":[{"name":"dubbo","slug":"dubbo","permalink":"http://www.lpnote.com/tags/dubbo/"},{"name":"问题分析","slug":"问题分析","permalink":"http://www.lpnote.com/tags/问题分析/"}]},{"title":"IP段转CIDR","slug":"ip-segment-to-CIDR","date":"2016-05-26T01:04:00.000Z","updated":"2019-02-13T08:59:06.532Z","comments":true,"path":"2016/05/26/ip-segment-to-CIDR/","link":"","permalink":"http://www.lpnote.com/2016/05/26/ip-segment-to-CIDR/","excerpt":"","text":"经常使用nginx、apache时候需要对某个ip段进行白名单或黑名单控制，而它的配置是以CIDR的方式进行的，所以此工具可以方便的做到从IP段到CIDR的转换。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071import java.util.ArrayList;import java.util.List; public class Ip2Cidr &#123; public static void main(String[] args) &#123; System.out.println(range2cidrlist(\"10.104.0.12\",\"10.104.0.35\")); &#125; public static List&lt;String&gt; range2cidrlist(String startIp, String endIp) &#123; long start = ipToLong(startIp); long end = ipToLong(endIp); ArrayList&lt;String&gt; pairs = new ArrayList&lt;String&gt;(); while (end &gt;= start) &#123; byte maxsize = 32; while (maxsize &gt; 0) &#123; long mask = CIDR2MASK[maxsize - 1]; long maskedBase = start &amp; mask; if (maskedBase != start) &#123; break; &#125; maxsize--; &#125; double x = Math.log(end - start + 1) / Math.log(2); byte maxdiff = (byte) (32 - Math.floor(x)); if (maxsize &lt; maxdiff) &#123; maxsize = maxdiff; &#125; String ip = longToIP(start); pairs.add(ip + \"/\" + maxsize); start += Math.pow(2, (32 - maxsize)); &#125; return pairs; &#125; public static final int[] CIDR2MASK = new int[] &#123; 0x00000000, 0x80000000, 0xC0000000, 0xE0000000, 0xF0000000, 0xF8000000, 0xFC000000, 0xFE000000, 0xFF000000, 0xFF800000, 0xFFC00000, 0xFFE00000, 0xFFF00000, 0xFFF80000, 0xFFFC0000, 0xFFFE0000, 0xFFFF0000, 0xFFFF8000, 0xFFFFC000, 0xFFFFE000, 0xFFFFF000, 0xFFFFF800, 0xFFFFFC00, 0xFFFFFE00, 0xFFFFFF00, 0xFFFFFF80, 0xFFFFFFC0, 0xFFFFFFE0, 0xFFFFFFF0, 0xFFFFFFF8, 0xFFFFFFFC, 0xFFFFFFFE, 0xFFFFFFFF &#125;; private static long ipToLong(String strIP) &#123; long[] ip = new long[4]; String[] ipSec = strIP.split(\"\\\\.\"); for (int k = 0; k &lt; 4; k++) &#123; ip[k] = Long.valueOf(ipSec[k]); &#125; return (ip[0] &lt;&lt; 24) + (ip[1] &lt;&lt; 16) + (ip[2] &lt;&lt; 8) + ip[3]; &#125; private static String longToIP(long longIP) &#123; StringBuffer sb = new StringBuffer(\"\"); sb.append(String.valueOf(longIP &gt;&gt;&gt; 24)); sb.append(\".\"); sb.append(String.valueOf((longIP &amp; 0x00FFFFFF) &gt;&gt;&gt; 16)); sb.append(\".\"); sb.append(String.valueOf((longIP &amp; 0x0000FFFF) &gt;&gt;&gt; 8)); sb.append(\".\"); sb.append(String.valueOf(longIP &amp; 0x000000FF)); return sb.toString(); &#125;&#125;","categories":[{"name":"转载文章","slug":"转载文章","permalink":"http://www.lpnote.com/categories/转载文章/"}],"tags":[{"name":"apache","slug":"apache","permalink":"http://www.lpnote.com/tags/apache/"},{"name":"nginx","slug":"nginx","permalink":"http://www.lpnote.com/tags/nginx/"}]},{"title":"RPM查询用法（转）","slug":"rpm-usage-for-query","date":"2016-03-05T10:34:00.000Z","updated":"2019-02-13T08:59:06.536Z","comments":true,"path":"2016/03/05/rpm-usage-for-query/","link":"","permalink":"http://www.lpnote.com/2016/03/05/rpm-usage-for-query/","excerpt":"转自：http://blog.csdn.net/fengyifei11228/article/details/6526149 命令格式： rpm {-q|–query} [select-options] [query-options] 对系统中已安装软件的查询 查询系统已安装的软件语法：rpm -q 软件名举例：[root@localhost beinan]# rpm -q gaimgaim-1.3.0-1.fc4 查看系统中所有已经安装的包，要加 -a 参数[root@localhost RPMS]# rpm -qa如果分页查看，再加一个管道 |和more命令； [root@localhost RPMS]# rpm -qa |more在所有已经安装的软件包中查找某个软件，比如说 gaim ；可以用 grep 抽取出来； [root@localhost RPMS]# rpm -qa |grep gaim上面这条的功能和 rpm -q gaim 输出的结果是一样的 查询一个已经安装的文件属于哪个软件包语法 rpm -qf 文件名注：文件名所在的绝对路径要指出举例：[root@localhost RPMS]# rpm -qf /usr/lib/libacl.lalibacl-devel-2.2.23-8 查询已安装软件包都安装到何处语法：rpm -ql 软件名 或 rpm rpmquery -ql 软件名举例：[root@localhost RPMS]# rpm -ql lynx[root@localhost RPMS]# rpmquery -ql lynx 查询一个已安装软件包的信息语法格式： rpm -qi 软件名举例：[root@localhost RPMS]# rpm -qi lynx 查看一下已安装软件的配置文件语法格式：rpm -qc 软件名举例：[root@localhost RPMS]# rpm -qc lynx 查看一个已经安装软件的文档安装位置语法格式： rpm -qd 软件名举例：[root@localhost RPMS]# rpm -qd lynx 查看一下已安装软件所依赖的软件包及文件语法格式： rpm -qR 软件名举例：[root@localhost beinan]# rpm -qR rpm-python 总结：对于一个软件包已经安装，我们可以把一系列的参数组合起来用比如 rpm -qil[root@localhost RPMS]# rpm -qil lynx","text":"转自：http://blog.csdn.net/fengyifei11228/article/details/6526149 命令格式： rpm {-q|–query} [select-options] [query-options] 对系统中已安装软件的查询 查询系统已安装的软件语法：rpm -q 软件名举例：[root@localhost beinan]# rpm -q gaimgaim-1.3.0-1.fc4 查看系统中所有已经安装的包，要加 -a 参数[root@localhost RPMS]# rpm -qa如果分页查看，再加一个管道 |和more命令； [root@localhost RPMS]# rpm -qa |more在所有已经安装的软件包中查找某个软件，比如说 gaim ；可以用 grep 抽取出来； [root@localhost RPMS]# rpm -qa |grep gaim上面这条的功能和 rpm -q gaim 输出的结果是一样的 查询一个已经安装的文件属于哪个软件包语法 rpm -qf 文件名注：文件名所在的绝对路径要指出举例：[root@localhost RPMS]# rpm -qf /usr/lib/libacl.lalibacl-devel-2.2.23-8 查询已安装软件包都安装到何处语法：rpm -ql 软件名 或 rpm rpmquery -ql 软件名举例：[root@localhost RPMS]# rpm -ql lynx[root@localhost RPMS]# rpmquery -ql lynx 查询一个已安装软件包的信息语法格式： rpm -qi 软件名举例：[root@localhost RPMS]# rpm -qi lynx 查看一下已安装软件的配置文件语法格式：rpm -qc 软件名举例：[root@localhost RPMS]# rpm -qc lynx 查看一个已经安装软件的文档安装位置语法格式： rpm -qd 软件名举例：[root@localhost RPMS]# rpm -qd lynx 查看一下已安装软件所依赖的软件包及文件语法格式： rpm -qR 软件名举例：[root@localhost beinan]# rpm -qR rpm-python 总结：对于一个软件包已经安装，我们可以把一系列的参数组合起来用比如 rpm -qil[root@localhost RPMS]# rpm -qil lynx 对于未安装的软件包的查看查看的前提是有一个.rpm 的文件，也就是说对既有软件file.rpm的查看等； 查看一个软件包的用途、版本等信息；语法： rpm -qpi file.rpm举例：[root@localhost RPMS]# rpm -qpi lynx-2.8.5-23.i386.rpm 查看一件软件包所包含的文件；语法： rpm -qpl file.rpm举例：[root@localhost RPMS]# rpm -qpl lynx-2.8.5-23.i386.rpm 查看软件包的文档所在的位置；语法： rpm -qpd file.rpm举例：[root@localhost RPMS]# rpm -qpd lynx-2.8.5-23.i386.rpm 查看一个软件包的配置文件；语法： rpm -qpc file.rpm举例：[root@localhost RPMS]# rpm -qpc lynx-2.8.5-23.i386.rpm 查看一个软件包的依赖关系语法： rpm -qpR file.rpm举例：[root@localhost archives]# rpm -qpR yumex_0.42-3.0.fc4_noarch.rpm/bin/bash/usr/bin/pythonconfig(yumex) = 0.42-3.0.fc4pygtk2pygtk2-libgladerpmlib(CompressedFileNames) &lt;= 3.0.4-1rpmlib(PayloadFilesHavePrefix) &lt;= 4.0-1usermodeyum &gt;= 2.3.2","categories":[{"name":"转载文章","slug":"转载文章","permalink":"http://www.lpnote.com/categories/转载文章/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://www.lpnote.com/tags/linux/"},{"name":"rpm","slug":"rpm","permalink":"http://www.lpnote.com/tags/rpm/"}]},{"title":"Mybatis Invalid bound statement (not found)问题分析","slug":"mybatis-invalid-bound-statement-not-found","date":"2016-03-03T16:00:00.000Z","updated":"2019-02-13T08:59:06.536Z","comments":true,"path":"2016/03/03/mybatis-invalid-bound-statement-not-found/","link":"","permalink":"http://www.lpnote.com/2016/03/03/mybatis-invalid-bound-statement-not-found/","excerpt":"今天又因为精心大意犯一个错，而且以前也已经遇到过，但是没有进行总结1234567891011121314Caused by: org.apache.ibatis.binding.BindingException: Invalid bound statement (not found): com.xxx.xxx.xxx.monitor.mapper.XXXXMapper.loadAllServices at org.apache.ibatis.binding.MapperMethod$SqlCommand.&lt;init&gt;(MapperMethod.java:189) ~[mybatis-3.2.7.jar:3.2.7] at org.apache.ibatis.binding.MapperMethod.&lt;init&gt;(MapperMethod.java:43) ~[mybatis-3.2.7.jar:3.2.7] at org.apache.ibatis.binding.MapperProxy.cachedMapperMethod(MapperProxy.java:58) ~[mybatis-3.2.7.jar:3.2.7] at org.apache.ibatis.binding.MapperProxy.invoke(MapperProxy.java:51) ~[mybatis-3.2.7.jar:3.2.7] at com.sun.proxy.$Proxy35.loadAllServices(Unknown Source) ~[na:na] ... ... at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.7.0_79] at java.lang.reflect.Method.invoke(Method.java:606) ~[na:1.7.0_79] at org.springframework.beans.factory.annotation.InitDestroyAnnotationBeanPostProcessor$LifecycleElement.invoke(InitDestroyAnnotationBeanPostProcessor.java:354) ~[spring-beans-4.2.4.RELEASE.jar:4.2.4.RELEASE] at org.springframework.beans.factory.annotation.InitDestroyAnnotationBeanPostProcessor$LifecycleMetadata.invokeInitMethods(InitDestroyAnnotationBeanPostProcessor.java:305) ~[spring-beans-4.2.4.RELEASE.jar:4.2.4.RELEASE] at org.springframework.beans.factory.annotation.InitDestroyAnnotationBeanPostProcessor.postProcessBeforeInitialization(InitDestroyAnnotationBeanPostProcessor.java:133) ~[spring-beans-4.2.4.RELEASE.jar:4.2.4.RELEASE] ... 74 common frames omitted 查了好多网上资料，其实都没有说到我这个问题的根本上。 后来分析了好一大阵后才发现是maven编译时的配置出问题，加上下面这个配置就好了。","text":"今天又因为精心大意犯一个错，而且以前也已经遇到过，但是没有进行总结1234567891011121314Caused by: org.apache.ibatis.binding.BindingException: Invalid bound statement (not found): com.xxx.xxx.xxx.monitor.mapper.XXXXMapper.loadAllServices at org.apache.ibatis.binding.MapperMethod$SqlCommand.&lt;init&gt;(MapperMethod.java:189) ~[mybatis-3.2.7.jar:3.2.7] at org.apache.ibatis.binding.MapperMethod.&lt;init&gt;(MapperMethod.java:43) ~[mybatis-3.2.7.jar:3.2.7] at org.apache.ibatis.binding.MapperProxy.cachedMapperMethod(MapperProxy.java:58) ~[mybatis-3.2.7.jar:3.2.7] at org.apache.ibatis.binding.MapperProxy.invoke(MapperProxy.java:51) ~[mybatis-3.2.7.jar:3.2.7] at com.sun.proxy.$Proxy35.loadAllServices(Unknown Source) ~[na:na] ... ... at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.7.0_79] at java.lang.reflect.Method.invoke(Method.java:606) ~[na:1.7.0_79] at org.springframework.beans.factory.annotation.InitDestroyAnnotationBeanPostProcessor$LifecycleElement.invoke(InitDestroyAnnotationBeanPostProcessor.java:354) ~[spring-beans-4.2.4.RELEASE.jar:4.2.4.RELEASE] at org.springframework.beans.factory.annotation.InitDestroyAnnotationBeanPostProcessor$LifecycleMetadata.invokeInitMethods(InitDestroyAnnotationBeanPostProcessor.java:305) ~[spring-beans-4.2.4.RELEASE.jar:4.2.4.RELEASE] at org.springframework.beans.factory.annotation.InitDestroyAnnotationBeanPostProcessor.postProcessBeforeInitialization(InitDestroyAnnotationBeanPostProcessor.java:133) ~[spring-beans-4.2.4.RELEASE.jar:4.2.4.RELEASE] ... 74 common frames omitted 查了好多网上资料，其实都没有说到我这个问题的根本上。 后来分析了好一大阵后才发现是maven编译时的配置出问题，加上下面这个配置就好了。 123456789101112131415161718192021&lt;build&gt;...&lt;resources&gt; &lt;resource&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;excludes&gt; &lt;exclude&gt;**/.svn/*&lt;/exclude&gt; &lt;/excludes&gt; &lt;/resource&gt; &lt;resource&gt; &lt;directory&gt;src/main/java&lt;/directory&gt; &lt;excludes&gt; &lt;exclude&gt;**/.svn/*&lt;/exclude&gt; &lt;/excludes&gt; &lt;includes&gt; &lt;include&gt;**/*.xml&lt;/include&gt; &lt;/includes&gt; &lt;/resource&gt;&lt;/resources&gt; ...&lt;/build&gt; 原因在于如果你的资源文件在java包下面，则maven默认打包是不会认为这些资源文件需要打入包内，所以在启动的时候老是会报Invalid bound statement (not found)，而如果资源文件放在resources文件夹下面就不会有问题，这与maven的资源存放机制有关。如果要求maven打包的时候将java包下面的非*.java文件也打入包中，则需要上面这这个配置项。","categories":[{"name":"原创文章","slug":"原创文章","permalink":"http://www.lpnote.com/categories/原创文章/"}],"tags":[{"name":"Mybatis","slug":"Mybatis","permalink":"http://www.lpnote.com/tags/Mybatis/"},{"name":"ORM","slug":"ORM","permalink":"http://www.lpnote.com/tags/ORM/"}]},{"title":"Fastjson的一个BUG","slug":"fastjon-bug-class-cast-exception","date":"2016-01-17T12:20:00.000Z","updated":"2019-02-13T08:59:06.524Z","comments":true,"path":"2016/01/17/fastjon-bug-class-cast-exception/","link":"","permalink":"http://www.lpnote.com/2016/01/17/fastjon-bug-class-cast-exception/","excerpt":"项目中使用的fastjson版本为：1.1.41，今天突然在重启线上服务器后莫名出现异常，而这个异常以前重来没有出现过，这个异常类似这样：123456789101112131415161718com.alibaba.fastjson.JSONException: write javaBean error at com.alibaba.fastjson.serializer.JavaBeanSerializer.write(JavaBeanSerializer.java:212) ~[fastjson-1.1.41.jar:na] at Serializer_6.write1(Unknown Source) ~[na:na] at Serializer_6.write(Unknown Source) ~[na:na] at com.alibaba.fastjson.serializer.JSONSerializer.write(JSONSerializer.java:369) ~[fastjson-1.1.41.jar:na] at com.alibaba.fastjson.JSON.toJSONString(JSON.java:418) ~[fastjson-1.1.41.jar:na] at com.alibaba.fastjson.JSON.toJSONString(JSON.java:568) ~[fastjson-1.1.41.jar:na] ... ... ...at java.lang.Thread.run(Thread.java:745) [na:1.7.0_79]Caused by: java.lang.ClassCastException: com.google.common.collect.Lists$TransformingSequentialList cannot be cast to com.xxx.common.dto.pager.PagerData at Serializer_9.write1(Unknown Source) ~[na:na] at Serializer_9.write(Unknown Source) ~[na:na] at com.alibaba.fastjson.serializer.ObjectFieldSerializer.writeValue(ObjectFieldSerializer.java:115) ~[fastjson-1.1.41.jar:na] at com.alibaba.fastjson.serializer.ObjectFieldSerializer.writeProperty(ObjectFieldSerializer.java:68) ~[fastjson-1.1.41.jar:na] at com.alibaba.fastjson.serializer.JavaBeanSerializer.write(JavaBeanSerializer.java:194) ~[fastjson-1.1.41.jar:na] ... 66 common frames omitted 百思不得其解，因为我返回的对象中根本就没有com.xxx.common.dto.pager.PagerData 这个对象信息，为什么在序列化的时候会出现这个错误呢，非常怪异。让人十分摸不着头脑的是这个错误是重启服务器后发生。毕竟线上一直在报错，当时情急之下的解决办法试了两个方法： 再次重启服务器重新启动服务器几次，错误依然，仅仅一次代码的小调整（根本和报错的问题风马牛不相及），但是启动服务器就报这个错，给跪了！ 赶紧找其它json类库暂时替代json框架毕竟我还是熟悉几个的，情急下只能仓促使用Gson（google出品的json框架）临时替代了fastjson的json序列化输出，问题解决！！！","text":"项目中使用的fastjson版本为：1.1.41，今天突然在重启线上服务器后莫名出现异常，而这个异常以前重来没有出现过，这个异常类似这样：123456789101112131415161718com.alibaba.fastjson.JSONException: write javaBean error at com.alibaba.fastjson.serializer.JavaBeanSerializer.write(JavaBeanSerializer.java:212) ~[fastjson-1.1.41.jar:na] at Serializer_6.write1(Unknown Source) ~[na:na] at Serializer_6.write(Unknown Source) ~[na:na] at com.alibaba.fastjson.serializer.JSONSerializer.write(JSONSerializer.java:369) ~[fastjson-1.1.41.jar:na] at com.alibaba.fastjson.JSON.toJSONString(JSON.java:418) ~[fastjson-1.1.41.jar:na] at com.alibaba.fastjson.JSON.toJSONString(JSON.java:568) ~[fastjson-1.1.41.jar:na] ... ... ...at java.lang.Thread.run(Thread.java:745) [na:1.7.0_79]Caused by: java.lang.ClassCastException: com.google.common.collect.Lists$TransformingSequentialList cannot be cast to com.xxx.common.dto.pager.PagerData at Serializer_9.write1(Unknown Source) ~[na:na] at Serializer_9.write(Unknown Source) ~[na:na] at com.alibaba.fastjson.serializer.ObjectFieldSerializer.writeValue(ObjectFieldSerializer.java:115) ~[fastjson-1.1.41.jar:na] at com.alibaba.fastjson.serializer.ObjectFieldSerializer.writeProperty(ObjectFieldSerializer.java:68) ~[fastjson-1.1.41.jar:na] at com.alibaba.fastjson.serializer.JavaBeanSerializer.write(JavaBeanSerializer.java:194) ~[fastjson-1.1.41.jar:na] ... 66 common frames omitted 百思不得其解，因为我返回的对象中根本就没有com.xxx.common.dto.pager.PagerData 这个对象信息，为什么在序列化的时候会出现这个错误呢，非常怪异。让人十分摸不着头脑的是这个错误是重启服务器后发生。毕竟线上一直在报错，当时情急之下的解决办法试了两个方法： 再次重启服务器重新启动服务器几次，错误依然，仅仅一次代码的小调整（根本和报错的问题风马牛不相及），但是启动服务器就报这个错，给跪了！ 赶紧找其它json类库暂时替代json框架毕竟我还是熟悉几个的，情急下只能仓促使用Gson（google出品的json框架）临时替代了fastjson的json序列化输出，问题解决！！！ 然后就是走上了寻找问题之路，找到fastjson的github网站，在issue列表中经过一些查找搜索终于找到一个issue和我遇到问题非常相像： issue 60 issue 107情形和我的基本一致，而且我使用的fastjson版本也刚好落在他们描述的bug版本区间。fastjson的开发者wenshao回复在1.1.42修复 我于是赶紧到Maven库查看fastjson版本，不看不知道，一看版本已经演进了好多，最新版本已经是1.2.7，更新代码maven依赖至1.2.7，然后测试发布到线上，多次重启确定没有再出现强制类转换异常。 关于这个错误的造成原因，还没有深入去了解是什么原因造成的，因为这个异常有一定的偶然性，可验证性比较差，线上服务器也是偶发出现。代码提交也没有明确是在哪次提交的时候修复了这个bug，等后面有时间再看看这个问题。","categories":[{"name":"原创文章","slug":"原创文章","permalink":"http://www.lpnote.com/categories/原创文章/"}],"tags":[{"name":"问题解析","slug":"问题解析","permalink":"http://www.lpnote.com/tags/问题解析/"},{"name":"fastjson","slug":"fastjson","permalink":"http://www.lpnote.com/tags/fastjson/"},{"name":"bug","slug":"bug","permalink":"http://www.lpnote.com/tags/bug/"}]},{"title":"关于Guava类库中Lists.transform的问题解析","slug":"note-about-list-transform-using-guava","date":"2015-12-30T12:28:00.000Z","updated":"2019-02-13T08:59:06.536Z","comments":true,"path":"2015/12/30/note-about-list-transform-using-guava/","link":"","permalink":"http://www.lpnote.com/2015/12/30/note-about-list-transform-using-guava/","excerpt":"这里讲述是的google的Guava类库中的一个需要注意的问题，如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111public class GuavaListTest &#123; public static class TestDO &#123; private String name; private int age; private String description; public TestDO(String name, int age, String description) &#123; this.name = name; this.age = age; this.description = description; &#125; public TestDO() &#123; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public String getDescription() &#123; return description; &#125; public void setDescription(String description) &#123; this.description = description; &#125; &#125; public static class TestDTO &#123; private String name; private int age; private String description; public TestDTO(String name, int age, String description) &#123; this.name = name; this.age = age; this.description = description; &#125; public TestDTO() &#123; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public String getDescription() &#123; return description; &#125; public void setDescription(String description) &#123; this.description = description; &#125; &#125; public static void main(String[] args) &#123; List&lt;TestDTO&gt; retList = query(); System.out.println(JSON.toJSONString(retList,true)); &#125; private static List&lt;TestDTO&gt; query() &#123; List&lt;TestDO&gt; queryList = ImmutableList.of( new TestDO(\"test1\", 18, \"test obj1\"), new TestDO(\"test2\", 19, \"test obj2\"), new TestDO(\"test3\", 20, \"test obj3\") ); List&lt;TestDTO&gt; retList = Lists.transform(queryList, new Function&lt;TestDO, TestDTO&gt;() &#123; @Override public TestDTO apply(TestDO input) &#123; return DTOUtils.createAndCopy(TestDTO.class, input); &#125; &#125;); //此处是见证神奇的地方 for(TestDTO dto : retList)&#123; dto.setAge(dto.getAge()+1); &#125; return retList; &#125;&#125; 这段代码的输出是什么？","text":"这里讲述是的google的Guava类库中的一个需要注意的问题，如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111public class GuavaListTest &#123; public static class TestDO &#123; private String name; private int age; private String description; public TestDO(String name, int age, String description) &#123; this.name = name; this.age = age; this.description = description; &#125; public TestDO() &#123; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public String getDescription() &#123; return description; &#125; public void setDescription(String description) &#123; this.description = description; &#125; &#125; public static class TestDTO &#123; private String name; private int age; private String description; public TestDTO(String name, int age, String description) &#123; this.name = name; this.age = age; this.description = description; &#125; public TestDTO() &#123; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public String getDescription() &#123; return description; &#125; public void setDescription(String description) &#123; this.description = description; &#125; &#125; public static void main(String[] args) &#123; List&lt;TestDTO&gt; retList = query(); System.out.println(JSON.toJSONString(retList,true)); &#125; private static List&lt;TestDTO&gt; query() &#123; List&lt;TestDO&gt; queryList = ImmutableList.of( new TestDO(\"test1\", 18, \"test obj1\"), new TestDO(\"test2\", 19, \"test obj2\"), new TestDO(\"test3\", 20, \"test obj3\") ); List&lt;TestDTO&gt; retList = Lists.transform(queryList, new Function&lt;TestDO, TestDTO&gt;() &#123; @Override public TestDTO apply(TestDO input) &#123; return DTOUtils.createAndCopy(TestDTO.class, input); &#125; &#125;); //此处是见证神奇的地方 for(TestDTO dto : retList)&#123; dto.setAge(dto.getAge()+1); &#125; return retList; &#125;&#125; 这段代码的输出是什么？ 这段代码的测试本意是将得到的数据做一些处理（这里简单将age+1），然后返回结果。粗看起来应该打印的结果是这样的：1234567891011121314151617[ &#123; \"age\":19, \"description\":\"test obj1\", \"name\":\"test1\" &#125;, &#123; \"age\":20, \"description\":\"test obj2\", \"name\":\"test2\" &#125;, &#123; \"age\":21, \"description\":\"test obj3\", \"name\":\"test3\" &#125;] 但是实际上的结果是这样的：1234567891011121314151617[ &#123; \"age\":18, \"description\":\"test obj1\", \"name\":\"test1\" &#125;, &#123; \"age\":19, \"description\":\"test obj2\", \"name\":\"test2\" &#125;, &#123; \"age\":20, \"description\":\"test obj3\", \"name\":\"test3\" &#125;] 如果第一次使用Guava类库或者对其不熟悉，也许某一天你就会踩上这个坑，其实也不算是坑，因为它的文档已经说明了： Returns a list that applies function to each element of fromList. The returned list is a transformed view of fromList; changes to fromList will be reflected in the returned list and vice versa.Since functions are not reversible, the transform is one-way and new items cannot be stored in the returned list. The add, addAll and set methods are unsupported in the returned list.The function is applied lazily, invoked when needed. This is necessary for the returned list to be a view, but it means that the function will be applied many times for bulk operations like List.contains and List.hashCode. For this to perform well, function should be fast. To avoid lazy evaluation when the returned list doesn’t need to be a view, copy the returned list into a new list of your choosing.If fromList implements RandomAccess, so will the returned list. The returned list is threadsafe if the supplied list and function are.If only a Collection or Iterable input is available, use Collections2.transform or Iterables.transform.Note: serializing the returned list is implemented by serializing fromList, its contents, and function – not by serializing the transformed values. This can lead to surprising behavior, so serializing the returned list is not recommended. Instead, copy the list using ImmutableList.copyOf(Collection) (for example), then serialize the copy. Other methods similar to this do not implement serialization at all for this reason. 大体翻译意思是在说： 该方法返回一个列表，这个列表中元素是运用方法中传入的功能函数(Function)对原列表中的元素进行处理后的结果，它是原列表的一个功能视图，任何对原列表的改变将会体现到视图列表中。因为Function函数是不可逆的，所以这样的转换是单向的，并且转换的结果不能存储在返回的列表中。所有对视图列表的添加（add/addAll）、设置（set）等都是不被支持的。Function函数的调用是延迟的（只有在需要的时候才进行调用），这对于视图列表来说是非常有必要的，但是这也同时意味着Function函数会因为各种情况而重复调用多次，比如List.contains/List.hashCode等，也正因为这个原因，所以建议Function应该是尽量轻量级而快速。如果你不是将返回的视图作为视图使用，那么你需要将该此视图列表拷贝到新的你需要的各种列表中。如果原列表实现了RandomAccess接口，那么返回的视图列表也会实现该接口。如果原列表是线程安全的，同时Function函数是线程安全的，那么返回的视图列表也是线程安全的。如果原集合是一个更高的抽象类，如：Collection、Iterable，那么使用Collection2.transform或者Iterables.transform也能满足需求。注意：序列化视图列表实际上是对原列表的内容的序列化，并不是序列化转换过后的视图内容。这会产生一些比较奇怪的行为，所以序列化返回的视图对象不建议的。如果确实需要序列化返回视图，请使用ImmutableList.copyOf(Collection),然后序列化这个拷贝，而介于这个原因，其它与该方法相似的方法根本就没有实现序列化。 所以说，返回的对象列表是一个视图，其中对它的任何更改都是无效的，并且也不建议对视图对象产生更改，如果需要更改返回的列表，那么需要自己进一步包装，如Lists.newArrayList(retList); 如果需要对返回的结果视图进行处理：123456List&lt;TestDTO&gt; retList = query();List&lt;TestDTO&gt; copyList = Lists.newArrayList(retList);for(TestDTO dto : copyList)&#123; dto.setAge(dto.getAge()+1);&#125;System.out.println(JSON.toJSONString(copyList,true)); 这时输出了正确的结果：1234567891011121314151617[ &#123; \"age\":19, \"description\":\"test obj1\", \"name\":\"test1\" &#125;, &#123; \"age\":20, \"description\":\"test obj2\", \"name\":\"test2\" &#125;, &#123; \"age\":21, \"description\":\"test obj3\", \"name\":\"test3\" &#125;]","categories":[{"name":"原创文章","slug":"原创文章","permalink":"http://www.lpnote.com/categories/原创文章/"}],"tags":[{"name":"guava","slug":"guava","permalink":"http://www.lpnote.com/tags/guava/"}]},{"title":"KMP字符串搜索算法","slug":"string-search-algorithm-KMP","date":"2015-12-26T16:00:00.000Z","updated":"2019-02-13T08:59:06.544Z","comments":true,"path":"2015/12/26/string-search-algorithm-KMP/","link":"","permalink":"http://www.lpnote.com/2015/12/26/string-search-algorithm-KMP/","excerpt":"","text":"一篇介绍KMP字符串搜索算法的好文章：http://kb.cnblogs.com/page/176818/","categories":[{"name":"转载文章","slug":"转载文章","permalink":"http://www.lpnote.com/categories/转载文章/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://www.lpnote.com/tags/算法/"}]},{"title":"SVN版本控制系统分支的用法及分析","slug":"usage-of-branche-in-svn","date":"2015-10-05T10:22:00.000Z","updated":"2019-02-13T08:59:06.552Z","comments":true,"path":"2015/10/05/usage-of-branche-in-svn/","link":"","permalink":"http://www.lpnote.com/2015/10/05/usage-of-branche-in-svn/","excerpt":"SVN是一种软件开发中非常流行的源代码版本控制工具软件，它能保存你每一次的源代码提交历史，便于我们对源码的历史做追溯，这样的好处是： 可以浏览软件源代码版本的演化历史以及回滚相关历史版本代码 分支系统优秀，可以多人进行协作开发 管理方便，逻辑明确，符合一般人思维习惯。 易于管理，集中式服务器更能保证代码安全性。 代码一致性非常高。 SVN适合开发人数不多的项目开发。大部分软件配置管理的大学教材都是使用SVN系统。 其中SVN的分支功能可以说是SVN源代码控制系统的核心功能。根据trunk分支的功用不同我将分支的使用方式分为两类： 以trunk作为稳定发布分支(Basically stable) 以trunk作为开发分支(Basically unstable) 以trunk作为稳定发布分支 顾名思义这个分支原则主导思想是trunk只包含稳定的随时可发布的代码。branches则用于开发新功能/修正bug/发布前的QA控制/重构/实验性质功能试点。一切未经测试验证的代码都禁止向trunk提交。这个是我从参加工作以来历届公司的开发方式，什么是以trunk作为稳定发布分支？这里我们从第一份的源代码说起，第一份源代码我们提交到SVN上的trunk上，并进行开发，开发完结后我们发布第一个版本，我们称之为第一个稳定版本，在第一个稳定版本完成后，以后所有的开发都不会在trunk上进行提交，什么意思呢？假如这时有一个新需求到来，那么就从主干trunk上提取一份代码（术语叫分支）到另一分支目录（一般分支目录取名branches），建立自己的开发分支，测试也是在分支上完成功能测试。待要发布时才将分支代码合并到主干上。因为主干trunk上的代码从一开始到每次迭代都是非常稳定的，所以这种方式叫以trunk为稳定发布分支的开发方式。每当开发完成某项功能，并达到发布标准且分支源代码已经合并到trunk分支后，我们就会以当前的trunk打出一个标签(tag)，标识一个稳定的版本（里程碑）。 这种开发方式的优点是： trunk代码非常稳定，始终存在一个功能稳定的分支可用 代码开发的并行度非常高 代码之间的抗干扰非常强 缺点是： 可能会存在较多分支（一般以某个功能或某几个功能作为一个功能分支开发） 代码发布前需要合并到trunk，可能会产生代码冲突，需要人工合并 目前大部分的互联网公司的源代码分支开发方式都是采用的这种方式。","text":"SVN是一种软件开发中非常流行的源代码版本控制工具软件，它能保存你每一次的源代码提交历史，便于我们对源码的历史做追溯，这样的好处是： 可以浏览软件源代码版本的演化历史以及回滚相关历史版本代码 分支系统优秀，可以多人进行协作开发 管理方便，逻辑明确，符合一般人思维习惯。 易于管理，集中式服务器更能保证代码安全性。 代码一致性非常高。 SVN适合开发人数不多的项目开发。大部分软件配置管理的大学教材都是使用SVN系统。 其中SVN的分支功能可以说是SVN源代码控制系统的核心功能。根据trunk分支的功用不同我将分支的使用方式分为两类： 以trunk作为稳定发布分支(Basically stable) 以trunk作为开发分支(Basically unstable) 以trunk作为稳定发布分支 顾名思义这个分支原则主导思想是trunk只包含稳定的随时可发布的代码。branches则用于开发新功能/修正bug/发布前的QA控制/重构/实验性质功能试点。一切未经测试验证的代码都禁止向trunk提交。这个是我从参加工作以来历届公司的开发方式，什么是以trunk作为稳定发布分支？这里我们从第一份的源代码说起，第一份源代码我们提交到SVN上的trunk上，并进行开发，开发完结后我们发布第一个版本，我们称之为第一个稳定版本，在第一个稳定版本完成后，以后所有的开发都不会在trunk上进行提交，什么意思呢？假如这时有一个新需求到来，那么就从主干trunk上提取一份代码（术语叫分支）到另一分支目录（一般分支目录取名branches），建立自己的开发分支，测试也是在分支上完成功能测试。待要发布时才将分支代码合并到主干上。因为主干trunk上的代码从一开始到每次迭代都是非常稳定的，所以这种方式叫以trunk为稳定发布分支的开发方式。每当开发完成某项功能，并达到发布标准且分支源代码已经合并到trunk分支后，我们就会以当前的trunk打出一个标签(tag)，标识一个稳定的版本（里程碑）。 这种开发方式的优点是： trunk代码非常稳定，始终存在一个功能稳定的分支可用 代码开发的并行度非常高 代码之间的抗干扰非常强 缺点是： 可能会存在较多分支（一般以某个功能或某几个功能作为一个功能分支开发） 代码发布前需要合并到trunk，可能会产生代码冲突，需要人工合并 目前大部分的互联网公司的源代码分支开发方式都是采用的这种方式。 以trunk作为开发分支 Basically unstable原则下，要求trunk包含最新的代码，不管它的稳定性如何；而用于release candidate的版本则应该开辟一个分支来交付。这种开发方式是我现在这家公司的团队所使用的源代码管理方式（现因为这种方式的缺点已经转向Basically stable模式），与上面的方式不同的是，所有的开发源代码提交都是直接在trunk上提交，即trunk上拥有最新的项目功能代码，也就是说trunk上不断会有人提交代码，也可以说trunk上的代码是非常不稳定的。所有的测试人员都是基于trunk进行代码测试，当测试完毕后，由trunk上打出一个发布tag。当然在我看来这种方式存在的问题很大，这种开发方式的优点我想了想可能只有以下一条： 所有人基于同一分支开发代码，测试代码，发布代码，基本不存在合并冲突的风险 但是这种开发方式的缺点我却可以随便列举两个： trunk代码非常不稳定，源代码中不容易找出当前的稳定版本分支什么意思呢？就是说所有的人都在不断的向trunk分支上提交代码，这个分支的测试是相当困难的，根据测试的相关原理，如果在已经测试OK的代码基础上提交代码，理论上说可能存在污染原已经测试过的代码，增加了引入新BUG的风险程度 代码开发的并行度极低为什么说并行度极低呢？比如说两个开发人员需要完成不同需求，他们都在trunk上开发，这样会导致的一个问题是可能A开发人员已经开发好了一个功能，但是B开发人员还没有完成相关的功能需求，那么此时A是无法进行上线上操作的，或者说可能会遇到代码残缺甚至无法启动的问题。那么此时A开发人员只能是等到B开发人员完成相关功能后才能发布，且B开发人员开发的相关功能更改可能会对A的功能造成影响。 目前许多开源的软件会采用这种开发方式。 结合上面的分析以及开源世界的软件源代码管理方式，我想说说自己的意见： 如果是开发开源软件，那么可以考虑使用以trunk作为开发分支的方式，原因如下：开源软件有着良好的人员素质以及里程碑控制，开源软件的发布毕竟来说不会是非常频繁的，它们有着自己的roadmap（路线图），所有的开发人员都是以这种roadmap作为开发主线，即在某一里程碑完成前所有人都是可以向trunk上提交代码的，但是一旦代码功能完成，进行测试期间时，trunk分支已经被冻结的，所有的人员都不能够向trunk提交代码。这样避免了我上面所说的测试代码污染性的问题。当软件测试完成并需要发布一个版本时，会从trunk上打出一个tag(标签)。作为后续软件特有版本BUG修复的源代码基线。这也是大家浏览开源软件的源代码仓库时看到的情景。开源的软件，或者说是基于版本发布的软件，它们更需要对发布版本的控制，因为它们需要对每个发布版本都维持有相关的修复生命周期，在该软件的某个版本发现的BUG，需要针对该版本（tag）进行修复，并合并回trunk以修复最新发布的版本。 如果是互联网公司的软件开发，并且所开发的软件是以服务方式（网站、API、WS、网络基础服务等）提供给外部用户使用，那么请考虑使用以trunk作为稳定发布分支的方式，原因如下：互联网公司的开发节奏是非常快的，虽然它也可能存在里程碑式的管理，但是这里面存在的发布与上线的次数是远远大于开源软件的版本发布，那么这里存在的问题就是，如何快速的实现功能迭代上线，这要求软件开发过程中的并行度要非常高且相互不受影响。而且由于是以服务的方式提供，所以不存在历史版本的维护问题，即源代码是不断向前演进的，并不需要对某个版本的服务需要进行BUG修复，所有的修复工作都是在服务端的自然演进中完成修复工作。","categories":[{"name":"原创文章","slug":"原创文章","permalink":"http://www.lpnote.com/categories/原创文章/"}],"tags":[{"name":"SVN","slug":"SVN","permalink":"http://www.lpnote.com/tags/SVN/"},{"name":"版本控制","slug":"版本控制","permalink":"http://www.lpnote.com/tags/版本控制/"},{"name":"VCS","slug":"VCS","permalink":"http://www.lpnote.com/tags/VCS/"},{"name":"分支管理","slug":"分支管理","permalink":"http://www.lpnote.com/tags/分支管理/"}]},{"title":"关于apache http转发后后端应用获取前端port问题","slug":"apache-http-proxy-port","date":"2015-05-20T16:00:00.000Z","updated":"2019-02-13T08:59:06.516Z","comments":true,"path":"2015/05/20/apache-http-proxy-port/","link":"","permalink":"http://www.lpnote.com/2015/05/20/apache-http-proxy-port/","excerpt":"apache+jetty转发配置下，jetty下应用获取request.getServerPort()获取到的是jetty的端口，而非apache入口的端口，情形如下： apache通过配置虚拟主机： 123456&lt;VirtualHost *:80&gt; ServerName \"admin.test.com\" ProxyRequests Off ProxyPass / http://localhost:6808/ ProxyPassReverse / http://localhost:6808/&lt;/VirtualHost&gt; 在80端口接受外界访问，然后转发到端口6808上。","text":"apache+jetty转发配置下，jetty下应用获取request.getServerPort()获取到的是jetty的端口，而非apache入口的端口，情形如下： apache通过配置虚拟主机： 123456&lt;VirtualHost *:80&gt; ServerName \"admin.test.com\" ProxyRequests Off ProxyPass / http://localhost:6808/ ProxyPassReverse / http://localhost:6808/&lt;/VirtualHost&gt; 在80端口接受外界访问，然后转发到端口6808上。 但是在6808端口上的应用在获取request.getServerPort()时获取到的是6808，而非80,对于这种情况在构造自引用地址时会出现一些问题： 1String basePath = request.getScheme()+\"://\"+request.getServerName()+\":\"+request.getServerPort(); 这块代码一般会引用在前端的JSP页面或者程序需要进行URL地址引用的时候。这个时候应用端口就不正确了。 一般的解决办法： 1、修改basePath的获取方式，改成配置文件方式，直接读取配置文件，但是这种方式不是特别方便，不推荐使用 2、使用反向代理服务器配置 nginx配置方式1proxy_set_header host $host:$port apache配置方式1234567&lt;VirtualHost *:80&gt; ServerName \"admin.test.com\" ProxyRequests Off ProxyPass / http://localhost:6808/ ProxyPassReverse / http://localhost:6808/ ProxyPreserveHost On&lt;/VirtualHost&gt;","categories":[],"tags":[{"name":"apache","slug":"apache","permalink":"http://www.lpnote.com/tags/apache/"},{"name":"nginx","slug":"nginx","permalink":"http://www.lpnote.com/tags/nginx/"},{"name":"http","slug":"http","permalink":"http://www.lpnote.com/tags/http/"}]},{"title":"Nginx负载均衡重定向问题","slug":"nginx-proxy-port","date":"2015-04-09T16:00:00.000Z","updated":"2019-02-13T08:59:06.536Z","comments":true,"path":"2015/04/09/nginx-proxy-port/","link":"","permalink":"http://www.lpnote.com/2015/04/09/nginx-proxy-port/","excerpt":"","text":"当负载端口不是80时，发现所有 response.sendRedirect() 重定向的页面都返回80端口，后来发现是代理设置Header时没有指定Ngnix监听的负载端口 #设置被代理服务器的端口或套接字，以及URL 1proxy_set_header Host $host:6112;","categories":[{"name":"原创文章","slug":"原创文章","permalink":"http://www.lpnote.com/categories/原创文章/"}],"tags":[{"name":"nginx","slug":"nginx","permalink":"http://www.lpnote.com/tags/nginx/"},{"name":"负载均衡","slug":"负载均衡","permalink":"http://www.lpnote.com/tags/负载均衡/"}]},{"title":"tomcat/jetty容器之间的路径兼容性问题","slug":"tomcat-jetty-container-path-compatibility","date":"2015-04-09T16:00:00.000Z","updated":"2019-02-13T08:59:06.544Z","comments":true,"path":"2015/04/09/tomcat-jetty-container-path-compatibility/","link":"","permalink":"http://www.lpnote.com/2015/04/09/tomcat-jetty-container-path-compatibility/","excerpt":"","text":"在项目中使用springmvc框架时，在controller方法中返回的view路径字符串最后和xml文件配置中的配置路径进行整合，从而形成一个完成的视图文件路径，然后在tomcat和jetty身上两者之间的差异出现问题： 12345&lt;bean id=&quot;internalViewResolver&quot; class=&quot;org.springframework.web.servlet.view.InternalResourceViewResolver&quot; p:viewClass=&quot;org.springframework.web.servlet.view.JstlView&quot; p:prefix=&quot;/WEB-INF/view/jsp&quot; p:suffix=&quot;.jsp&quot; p:order=&quot;1&quot;/&gt; tomcat中view文件/WEB-INF/view/jsp//default/ui/user/update_passwd.jsp是可以找到的 而在jetty中view文件/WEB-INF/view/jsp//default/ui/user/update_passwd.jsp是不能找到的，去掉多余的/后方能找到，jetty对于路径的规范更加严格？","categories":[{"name":"原创文章","slug":"原创文章","permalink":"http://www.lpnote.com/categories/原创文章/"}],"tags":[{"name":"tomcat","slug":"tomcat","permalink":"http://www.lpnote.com/tags/tomcat/"},{"name":"jetty","slug":"jetty","permalink":"http://www.lpnote.com/tags/jetty/"}]},{"title":"no such object in table","slug":"java-no-such-object-in-table","date":"2015-04-06T16:00:00.000Z","updated":"2019-02-13T08:59:06.532Z","comments":true,"path":"2015/04/06/java-no-such-object-in-table/","link":"","permalink":"http://www.lpnote.com/2015/04/06/java-no-such-object-in-table/","excerpt":"","text":"检查一下主机名配置,以及host文件或者DNS解析,可能Context.PROVIDER_URL需要域名而不能使用ip 把etc/hosts恢复成1127.0.0.1 localhost 试试","categories":[{"name":"原创文章","slug":"原创文章","permalink":"http://www.lpnote.com/categories/原创文章/"}],"tags":[{"name":"java","slug":"java","permalink":"http://www.lpnote.com/tags/java/"}]},{"title":"virtualbox 安装ubuntu 14.04后分辨率不正确的问题修复","slug":"virtualbox-ubuntu-14.04-resolution","date":"2015-01-21T16:00:00.000Z","updated":"2019-02-13T08:59:06.552Z","comments":true,"path":"2015/01/21/virtualbox-ubuntu-14.04-resolution/","link":"","permalink":"http://www.lpnote.com/2015/01/21/virtualbox-ubuntu-14.04-resolution/","excerpt":"","text":"Instead of using the Virtualbox Guest Addition ISO file, try using the following commands in the terminal on the guest Ubuntu Virtual Machine : Update apt-get. apt-get update Install dependencies apt-get install build-essential linux-headers-$(uname -r) Install guest additions apt-get install virtualbox-guest-x11 Use sudo before the commands if required.","categories":[{"name":"转载文章","slug":"转载文章","permalink":"http://www.lpnote.com/categories/转载文章/"}],"tags":[{"name":"virtualbox","slug":"virtualbox","permalink":"http://www.lpnote.com/tags/virtualbox/"},{"name":"ubuntu","slug":"ubuntu","permalink":"http://www.lpnote.com/tags/ubuntu/"}]},{"title":"关于tomcat 开启gc日志后每隔1小时full gc的问题","slug":"tomcat-one-hour-fullgc","date":"2014-08-19T16:00:00.000Z","updated":"2019-02-13T08:59:06.544Z","comments":true,"path":"2014/08/19/tomcat-one-hour-fullgc/","link":"","permalink":"http://www.lpnote.com/2014/08/19/tomcat-one-hour-fullgc/","excerpt":"","text":"关于tomcat 开启gc日志后每隔1小时full gc的问题 主要是因为rmi导致的，可以参见以下的博文： http://www.iteye.com/topic/1121073 http://hllvm.group.iteye.com/group/topic/27945 http://docs.oracle.com/javase/6/docs/technotes/guides/rmi/sunrmiproperties.html","categories":[{"name":"转载文章","slug":"转载文章","permalink":"http://www.lpnote.com/categories/转载文章/"}],"tags":[{"name":"tomcat","slug":"tomcat","permalink":"http://www.lpnote.com/tags/tomcat/"},{"name":"gc","slug":"gc","permalink":"http://www.lpnote.com/tags/gc/"}]},{"title":"nginx server节配置","slug":"nginx-server-segment-conf","date":"2014-08-05T16:00:00.000Z","updated":"2019-02-13T08:59:06.536Z","comments":true,"path":"2014/08/05/nginx-server-segment-conf/","link":"","permalink":"http://www.lpnote.com/2014/08/05/nginx-server-segment-conf/","excerpt":"","text":"123456789101112131415server &#123; listen 80; server_name *.test.com default; root /home/a/share/htdocs; index index.html index.htm; location / &#123; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header X-Forwarded-For $remote_addr; proxy_pass http://127.0.0.1:8080/test/; &#125;# rewrite ^/(.*)$ http://www.test.com/abc.htm$1;&#125; server_name节点表示从哪个域名过来，nginx里可以配置多个server节点以支持不同域名的转发需求。 default的意思是如果所有的server节点都没有匹配，那么就使用这个default节点匹配了。 index节点表示如果域名后没有带任何的地址信息，则默认访问的页面，一般应用会以index.html展现。 location节点可以根据正则表达式进行配置，以满足不同路径的转发规则。 proxy_set_header 节点主要是将请求的原始信息附加到nginx的转发上，从而能让后端服务（如tomcat)能够获取到最原始的客户端信息，而不是nginx转发端的信息 rewrite 节点是重定向，当访问匹配的正则表达式路径时会被重定向到相应的地址上。","categories":[],"tags":[{"name":"nginx","slug":"nginx","permalink":"http://www.lpnote.com/tags/nginx/"}]},{"title":"Linux自动同步时间","slug":"linux-time-auto-update","date":"2013-12-09T16:00:00.000Z","updated":"2019-02-13T08:59:06.532Z","comments":true,"path":"2013/12/09/linux-time-auto-update/","link":"","permalink":"http://www.lpnote.com/2013/12/09/linux-time-auto-update/","excerpt":"","text":"1、 一般的Linux发行版都带有ntpdate这个命令，如没有，可以从其它发行版中拷贝一个/usr/local/bin 2、#crontab -e 添加 1*/10 * * * * /usr/local/bin/ntpdate time.nist.gov&gt; /dev/null 2&amp;1 每10秒执行一次ntpdate，当然ntpdate也可以用脚本代替，这样可以更加灵活 time.nist.gov 是一个时间同步服务器 3、1/etc/rc.d/init.d/crond restart","categories":[{"name":"转载文章","slug":"转载文章","permalink":"http://www.lpnote.com/categories/转载文章/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://www.lpnote.com/tags/linux/"}]},{"title":"centos安装VIM","slug":"linux-vim-setup","date":"2013-12-09T16:00:00.000Z","updated":"2019-02-13T08:59:06.532Z","comments":true,"path":"2013/12/09/linux-vim-setup/","link":"","permalink":"http://www.lpnote.com/2013/12/09/linux-vim-setup/","excerpt":"","text":"yum install vim-X11 vim-common vim-enhanced vim-minimal 如果Centos如果精简安装，则不会默认安装VIM，只会安装VI","categories":[{"name":"转载文章","slug":"转载文章","permalink":"http://www.lpnote.com/categories/转载文章/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://www.lpnote.com/tags/linux/"},{"name":"centos","slug":"centos","permalink":"http://www.lpnote.com/tags/centos/"},{"name":"vim","slug":"vim","permalink":"http://www.lpnote.com/tags/vim/"}]},{"title":"linux 如何查找命令的路径","slug":"linux-find-command-path","date":"2013-12-03T16:00:00.000Z","updated":"2019-02-13T08:59:06.532Z","comments":true,"path":"2013/12/03/linux-find-command-path/","link":"","permalink":"http://www.lpnote.com/2013/12/03/linux-find-command-path/","excerpt":"","text":"linux 下，我们常使用 cd ,grep,vi 等命令，有时候我们要查到这些命令所在的位置，如何做呢？ linux下有2个命令可完成该功能：which ,whereis which 用来查看当前要执行的命令所在的路径。whereis 用来查看一个命令或者文件所在的路径。 which命令的原理：在PATH变量指定的路径中，搜索某个系统命令的位置，并且返回第一个搜索结果。也就是说，使用which命令，就可以看到某个系统命令是否存在，以及执行的到底是哪一个位置的命令。 which命令的使用实例： $ which grep whereis命令原理：只能用于程序名的搜索，而且只搜索二进制文件（参数-b）、man说明文件（参数-m）和源代码文件（参数-s）。如果省略参数，则返回所有信息。 whereis命令的使用实例： $ whereis grep 下面举个例子来说明。加入你的linux系统上装了多个版本的java。如果你直接在命令行敲命令 “java -version” ，会得到一个结果。但是，你知道是哪一个路径下的java在执行吗？如果想知道，可以用 which 命令： which java 返回的是 PATH路径中第一个JAVA的位置，也就是JAVA命令默认执行的位置 如果使用命令： whereis java 那么你会得到很多条结果，因为这个命令把所有包含java（不管是文件还是文件夹）的路径都列了出来。","categories":[{"name":"转载文章","slug":"转载文章","permalink":"http://www.lpnote.com/categories/转载文章/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://www.lpnote.com/tags/linux/"}]},{"title":"maven -Dmaven.skip.test=true 和 -DskipTests=true的区别","slug":"maven-skip-tests-two-styles-diff","date":"2013-12-01T16:00:00.000Z","updated":"2019-02-13T08:59:06.536Z","comments":true,"path":"2013/12/01/maven-skip-tests-two-styles-diff/","link":"","permalink":"http://www.lpnote.com/2013/12/01/maven-skip-tests-two-styles-diff/","excerpt":"","text":"在使用mvn package进行编译、打包时，Maven会执行src/test/java中的JUnit测试用例，有时为了跳过测试，会使用参数-DskipTests和-Dmaven.test.skip=true，这两个参数的主要区别是： -DskipTests，不执行测试用例，但编译测试用例类生成相应的class文件至target/test-classes下。 -Dmaven.test.skip=true，不执行测试用例，也不编译测试用例类。","categories":[{"name":"原创文章","slug":"原创文章","permalink":"http://www.lpnote.com/categories/原创文章/"}],"tags":[{"name":"maven","slug":"maven","permalink":"http://www.lpnote.com/tags/maven/"}]},{"title":"IDEA 快捷键","slug":"intellij-idea-quick-key","date":"2013-09-25T16:00:00.000Z","updated":"2019-02-13T08:59:06.532Z","comments":true,"path":"2013/09/25/intellij-idea-quick-key/","link":"","permalink":"http://www.lpnote.com/2013/09/25/intellij-idea-quick-key/","excerpt":"Ctrl + Space完成类、方法、变量名称的自动输入,这个快捷键是我最经常使用的快捷键了，它可以完成类、方法、变量名称的自动录入，很方便 Ctrl + N（Ctrl + Shift + N）跳转到指定的java文件（其它文件）这个功能很方便，至少我不用每回都在一长串的文件列表里找寻我想要编辑的类文件和jsp文件了 Ctrl + B跳转到定义处这个就不用多说了，好象是个IDE就会提供的功能 Ctrl + Alt + T用来围绕选中的代码行（ 包括if、while、try catch等）这个功能也很方便，把我以前要做的：①先写if-else，②然后调整代码的缩进格式，还要注意括号是否匹配了，现在用这个功能来做，省事多了（不过让我变得越来越懒了） Ctrl + Alt + B跳转到方法实现处这个也算是很普遍的功能了，就不多说了。 Ctrl + W按一个word来进行选择操作在IDEA里的这个快捷键功能是先选择光标所在字符处的单词，然后是选择源代码的扩展区域。举例来说，对下边这个语句java.text.SimpleDateFormat formatter = new java.text.SimpleDateFormat(“yyyy-MM-dd HH:mm”);当光标的位置在双引号内的字符串中时，会先选中这个字符串，然后是等号右边的表达式，再是整个句子。我一般都是在对代码进行重新修改的时候使用它来选择出那些长长的复合表达式，很方便：） Shift + F1在浏览器中显示指定的java docs,这个也应该是几乎所有的java ide都提供的功能，就不多说了。 Ctrl + Q在editor window中显示java docs这个功能很方便–因为有时仅仅是忘记了自己编写的方法中的某个参数的含义，此时又不想再起一个浏览器来查看java doc，此时这个功能的好处就体现出来了 Ctrl + /注释/反注释指定的语句,这个功能很象PB中提供的一个功能，它可以注释和反注释你所选择的语句（使用单行注释符号”//“），你也可以用Ctrl + Shift + / 来进行多行语句的注释（即使用多行注释符号”/ … /“） F2/Shift + F2跳转到下/上一个错误语句处IDEA提供了一个在错误语句之间方便的跳转的功能，你使用这个快捷键可以快捷在出错的语句之间进行跳转。","text":"Ctrl + Space完成类、方法、变量名称的自动输入,这个快捷键是我最经常使用的快捷键了，它可以完成类、方法、变量名称的自动录入，很方便 Ctrl + N（Ctrl + Shift + N）跳转到指定的java文件（其它文件）这个功能很方便，至少我不用每回都在一长串的文件列表里找寻我想要编辑的类文件和jsp文件了 Ctrl + B跳转到定义处这个就不用多说了，好象是个IDE就会提供的功能 Ctrl + Alt + T用来围绕选中的代码行（ 包括if、while、try catch等）这个功能也很方便，把我以前要做的：①先写if-else，②然后调整代码的缩进格式，还要注意括号是否匹配了，现在用这个功能来做，省事多了（不过让我变得越来越懒了） Ctrl + Alt + B跳转到方法实现处这个也算是很普遍的功能了，就不多说了。 Ctrl + W按一个word来进行选择操作在IDEA里的这个快捷键功能是先选择光标所在字符处的单词，然后是选择源代码的扩展区域。举例来说，对下边这个语句java.text.SimpleDateFormat formatter = new java.text.SimpleDateFormat(“yyyy-MM-dd HH:mm”);当光标的位置在双引号内的字符串中时，会先选中这个字符串，然后是等号右边的表达式，再是整个句子。我一般都是在对代码进行重新修改的时候使用它来选择出那些长长的复合表达式，很方便：） Shift + F1在浏览器中显示指定的java docs,这个也应该是几乎所有的java ide都提供的功能，就不多说了。 Ctrl + Q在editor window中显示java docs这个功能很方便–因为有时仅仅是忘记了自己编写的方法中的某个参数的含义，此时又不想再起一个浏览器来查看java doc，此时这个功能的好处就体现出来了 Ctrl + /注释/反注释指定的语句,这个功能很象PB中提供的一个功能，它可以注释和反注释你所选择的语句（使用单行注释符号”//“），你也可以用Ctrl + Shift + / 来进行多行语句的注释（即使用多行注释符号”/ … /“） F2/Shift + F2跳转到下/上一个错误语句处IDEA提供了一个在错误语句之间方便的跳转的功能，你使用这个快捷键可以快捷在出错的语句之间进行跳转。 Shift + F6提供对方法、变量的重命名对IDEA提供的Refector功能我用得比较少，相比之下这个功能是我用得最多的了。对于这个功能没什么可说的了，确实很方便，赶快试一试吧。 Ctrl + Alt + L根据模板格式化选择的代码,根据模板中设定的格式来format你的java代码，不过可惜的是只对java文件有效 Ctrl + Alt + I将选中的代码进行自动缩进编排这个功能在编辑jsp文件的时候也可以工作，提供了一个对上边格式化代码功能的补充。 Ctrl + Alt + O优化import自动去除无用的import语句，蛮不错的一个功能。 Ctrl + ]/[跳转到代码块结束/开始处,这个功能vi也有，也是很常用的一个代码编辑功能了。 Ctrl+E可以显示最近编辑的文件列表 Shift+Click可以关闭文件 Ctrl+Shift+Backspace可以跳转到上次编辑的地方 Ctrl+F12可以显示当前文件的结构 Ctrl+F7可以查询当前元素在当前文件中的引用，然后按F3可以选择 Ctrl+Shift+N可以快速打开文件 Alt+Q可以看到当前方法的声明 Ctrl+P可以显示参数信息 Alt+Insert可以生成构造器/Getter/Setter等 Ctrl+Alt+V可以引入变量。例如把括号内的SQL赋成一个变量 Alt+Up and Alt+Down可在方法间快速移动 Alt+Enter可以得到一些Intention Action，例如将”==”改为”equals()” Ctrl+Shift+Alt+N可以快速打开符号 Ctrl+Shift+Space在很多时候都能够给出Smart提示 Alt+F3可以快速寻找 Ctrl+O可以选择父类的方法进行重写 Ctrl+Alt+Space是类名自动完成 Ctrl+JLive Templates! Ctrl+Shift+F7可以高亮当前元素在当前文件中的使用 Ctrl+Alt+Up /Ctrl+Alt+Down可以快速跳转搜索结果 Ctrl+Shift+J可以整合两行 Alt+F8是计算变量值 Ctrl+D 复制上一行或复制选定Ctrl+Alt+L 格式化代码Alt+Shift+Insert 列编辑 装上UpperLowerCapitalize后Alt+P // to uppercaseAlt+L // to lowercaseAlt+C // 首字母大写","categories":[{"name":"转载文章","slug":"转载文章","permalink":"http://www.lpnote.com/categories/转载文章/"}],"tags":[{"name":"IDE","slug":"IDE","permalink":"http://www.lpnote.com/tags/IDE/"},{"name":"IDEA","slug":"IDEA","permalink":"http://www.lpnote.com/tags/IDEA/"}]},{"title":"nginx error_page配置","slug":"nginx-error-page-configuration","date":"2013-07-31T16:00:00.000Z","updated":"2019-02-13T08:59:06.536Z","comments":true,"path":"2013/07/31/nginx-error-page-configuration/","link":"","permalink":"http://www.lpnote.com/2013/07/31/nginx-error-page-configuration/","excerpt":"","text":"今天偶然访问了一个线上应用不存在的url，应用报错，出现了乱码。 乱码是从nginx转发的tomcat报出来的。tomcat默认处理HTML是以ISO-8859-1处理的，所以就产生了乱码。 解决这个error_page的途径我尝试了两种方法： 1、让tomcat返回正常的非乱码的error_pagetomcat的错误页是在项目的web.xml中配置的，但是除了这个之外，别无其它编码配置。在网上搜索了有人提现将.html这种页面也交由jsp servlet处理就好，我认为这种方式不好，所以直接没尝试。我配置的web.xml如下： 1234&lt;error-page&gt; &lt;error-code&gt;500&lt;/error-code&gt; &lt;location&gt;/error.html&lt;/location&gt;&lt;/error-page&gt; 那么首先想到的就是把error.html页的返回头改掉：1&lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=UTF-8&quot;/&gt; 但是改后，不幸的是还是不行！tomcat还是把它处理成ISO-8859-1了。杯具！ 2、第二种途径是不管tomcat返回的错误页，直接使用nginx的错误页这里要注意一件事就是一定要配置nginx这个选项：proxy_intercept_errors on; 这个选项默认在nginx是off的。所以这时候你配置的所有error_page错误页都不会生效。为此我查了好久才知道是这个原因。 我的配置：12345678location / &#123; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header X-Forwarded-For $remote_addr; proxy_pass http://127.0.0.1:8080; proxy_intercept_errors on;&#125;","categories":[{"name":"原创文章","slug":"原创文章","permalink":"http://www.lpnote.com/categories/原创文章/"}],"tags":[{"name":"nginx","slug":"nginx","permalink":"http://www.lpnote.com/tags/nginx/"}]}]}